import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import google.generativeai as genai
import time
import asyncio
import re

# X·ª≠ l√Ω event loop ƒë·ªÉ tr√°nh xung ƒë·ªôt gi·ªØa Streamlit v√† PyTorch
try:
    asyncio.get_event_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ƒê·∫£m b·∫£o API key ƒë∆∞·ª£c c·∫•u h√¨nh
if not GEMINI_API_KEY:
    st.error("GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ki·ªÉm tra file .env")
    st.stop()

try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh Gemini API: {e}")
    st.stop()

# Cache d·ªØ li·ªáu ƒë·ªÉ tr√°nh t·∫£i l·∫°i
@st.cache_resource
def load_resources():
    """T·∫£i FAISS index, d·ªØ li·ªáu chunks v√† c√°c m√¥ h√¨nh c·∫ßn thi·∫øt."""
    
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file
    faiss_index_path = "Faiss/all_syllabi_faiss.index"
    chunks_json_path = "Embedded/all_embeddings.json"
    
    # T·∫£i FAISS index
    try:
        index = faiss.read_index(faiss_index_path)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i FAISS index v·ªõi {index.ntotal} vectors")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i FAISS index: {e}")
        return None, None, None
    
    # T·∫£i d·ªØ li·ªáu chunks
    try:
        with open(chunks_json_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu chunks: {len(chunks_data)} chunks")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu chunks: {e}")
        return index, None, None
    
    # T·∫£i m√¥ h√¨nh embedding
    embedding_model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    try:
        embedding_model = SentenceTransformer(embedding_model_name)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh embedding")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh embedding: {e}")
        return index, chunks_data, None
    
    # T·∫°o b·∫£ng tra c·ª©u subject_code -> syllabus_name v√† c√°c metadata kh√°c
    subject_metadata = {}
    for chunk in chunks_data:
        if 'metadata' in chunk and 'subject_code' in chunk['metadata']:
            subject_code = chunk['metadata']['subject_code']
            if subject_code not in subject_metadata and chunk['type'] == 'general_info':
                # T√¨m t√™n m√¥n h·ªçc t·ª´ n·ªôi dung chunk
                content = chunk.get('content', '')
                name_match = re.search(r"T√™n m√¥n h·ªçc: ([^(]+)", content)
                if name_match:
                    subject_name = name_match.group(1).strip()
                    subject_metadata[subject_code] = {
                        'name': subject_name,
                        'keywords': [subject_code, subject_name]
                    }
                    
                    # Extract number of credits
                    credit_match = re.search(r"S·ªë t√≠n ch·ªâ: ([0-9]+)", content)
                    if credit_match:
                        subject_metadata[subject_code]['credits'] = credit_match.group(1)
                        
    # L·∫•y danh s√°ch c√°c m√£ m√¥n h·ªçc
    subject_codes = sorted(list(subject_metadata.keys()))
    
    return index, chunks_data, embedding_model, subject_codes, subject_metadata

def identify_subject_from_query(query, subject_metadata, embedding_model, faiss_index, all_chunks_data, top_k=3):
    """X√°c ƒë·ªãnh m√¥n h·ªçc m√† c√¢u h·ªèi c√≥ th·ªÉ ƒëang ƒë·ªÅ c·∫≠p ƒë·∫øn."""
    # T·∫°o embedding cho c√¢u h·ªèi
    query_embedding = embedding_model.encode([query])
    
    # T√¨m ki·∫øm top_k chunks g·∫ßn nh·∫•t
    distances, indices = faiss_index.search(query_embedding.astype(np.float32), top_k*5)
    
    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói m√£ m√¥n h·ªçc
    subject_counts = {}
    subject_scores = {}  # ƒêi·ªÉm s·ªë cho m·ªói m√¥n h·ªçc d·ª±a tr√™n kho·∫£ng c√°ch embedding
    
    for idx, i in enumerate(indices[0]):
        if 0 <= i < len(all_chunks_data):
            chunk = all_chunks_data[i]
            if 'metadata' in chunk and 'subject_code' in chunk['metadata']:
                subject_code = chunk['metadata']['subject_code']
                
                # TƒÉng s·ªë l·∫ßn xu·∫•t hi·ªán
                if subject_code not in subject_counts:
                    subject_counts[subject_code] = 0
                    subject_scores[subject_code] = 0
                
                subject_counts[subject_code] += 1
                # T√≠nh ƒëi·ªÉm: 1/kho·∫£ng c√°ch (c√†ng g·∫ßn c√†ng cao ƒëi·ªÉm)
                # C·ªông th√™m ƒëi·ªÉm n·∫øu chunk l√† general_info
                score_boost = 2 if chunk['type'] == 'general_info' else 1
                subject_scores[subject_code] += (1 / (1 + distances[0][idx])) * score_boost
    
    # Ki·ªÉm tra xem c√≥ tr·ª±c ti·∫øp ƒë·ªÅ c·∫≠p ƒë·∫øn m√£ m√¥n h·ªçc trong c√¢u h·ªèi kh√¥ng
    direct_mention = None
    for subject_code in subject_metadata:
        # Ki·ªÉm tra m√£ m√¥n h·ªçc ƒë·∫ßy ƒë·ªß
        if subject_code.lower() in query.lower():
            direct_mention = subject_code
            break
        # Ki·ªÉm tra prefix c·ªßa m√£ m√¥n h·ªçc (e.g., "DPL" cho "DPL302m")
        code_prefix = ''.join([c for c in subject_code if c.isalpha()])
        if code_prefix.lower() in query.lower() and len(code_prefix) >= 2:  # Ch·ªâ x√©t prefix c√≥ √≠t nh·∫•t 2 k√Ω t·ª±
            # T√¨m t·∫•t c·∫£ m√¥n c√≥ prefix n√†y
            possible_subjects = [code for code in subject_metadata if code.startswith(code_prefix)]
            if len(possible_subjects) == 1:
                direct_mention = possible_subjects[0]
                break
            elif len(possible_subjects) > 1:
                # N·∫øu c√≥ nhi·ªÅu m√¥n c√πng prefix, ch·ªçn m√¥n c√≥ ƒëi·ªÉm cao nh·∫•t
                highest_score = 0
                for subj in possible_subjects:
                    if subj in subject_scores and subject_scores[subj] > highest_score:
                        highest_score = subject_scores[subj]
                        direct_mention = subj
                break
    
    if direct_mention:
        return direct_mention, subject_metadata.get(direct_mention, {}).get('name', '')
    
    # N·∫øu kh√¥ng c√≥ ƒë·ªÅ c·∫≠p tr·ª±c ti·∫øp, t√¨m m√¥n c√≥ ƒëi·ªÉm cao nh·∫•t
    if subject_scores:
        best_subject = max(subject_scores, key=subject_scores.get)
        return best_subject, subject_metadata.get(best_subject, {}).get('name', '')
    
    return None, None

def get_answer_from_rag(query_text, faiss_index, all_chunks_data, embedding_model, gemini_model, 
                       subject_metadata, subject_filter=None, top_k=5, temperature=0.2,
                       auto_detect_subject=True):
    """Th·ª±c hi·ªán pipeline RAG v·ªõi Gemini ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi."""
    start_time = time.time()  # Move this to the beginning of the function
    
    if not query_text:
        return "Vui l√≤ng cung c·∫•p c√¢u h·ªèi.", [], None, None, 0  # Added 0 as processing_time
    
    detected_subject_code = None
    detected_subject_name = None
    
    # 1. X·ª≠ l√Ω C√¢u h·ªèi v√† x√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan (n·∫øu ƒë∆∞·ª£c b·∫≠t)
    with st.status("ƒêang x·ª≠ l√Ω c√¢u h·ªèi...") as status:
        if auto_detect_subject and not subject_filter:
            status.update(label="ƒêang x√°c ƒë·ªãnh m√¥n h·ªçc t·ª´ c√¢u h·ªèi...")
            detected_subject_code, detected_subject_name = identify_subject_from_query(
                query_text, subject_metadata, embedding_model, faiss_index, all_chunks_data
            )
            
            if detected_subject_code:
                status.update(label=f"ƒê√£ x√°c ƒë·ªãnh c√¢u h·ªèi c√≥ th·ªÉ li√™n quan ƒë·∫øn m√¥n {detected_subject_code} ({detected_subject_name})")
                # S·ª≠ d·ª•ng m√¥n h·ªçc ƒë∆∞·ª£c ph√°t hi·ªán l√†m b·ªô l·ªçc
                subject_filter = detected_subject_code
            else:
                status.update(label="Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c m√¥n h·ªçc c·ª• th·ªÉ, t√¨m ki·∫øm trong t·∫•t c·∫£ c√°c m√¥n")
        
        try:
            status.update(label="ƒêang t·∫°o embedding cho c√¢u h·ªèi...")
            query_embedding = embedding_model.encode([query_text])
            status.write("‚úÖ ƒê√£ t·∫°o embedding cho c√¢u h·ªèi")
        except Exception as e:
            status.error(f"‚ùå L·ªói khi t·∫°o embedding cho c√¢u h·ªèi: {e}")
            processing_time = time.time() - start_time  # Calculate time even for errors
            return "L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi.", [], None, None, processing_time

        # 2. Truy xu·∫•t Th√¥ng tin (Information Retrieval)
        try:
            status.update(label="ƒêang t√¨m ki·∫øm th√¥ng tin li√™n quan...")
            distances, indices = faiss_index.search(query_embedding.astype(np.float32), top_k*3)  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ l·ªçc
            status.write("‚úÖ ƒê√£ t√¨m ki·∫øm trong FAISS index")
        except Exception as e:
            status.error(f"‚ùå L·ªói khi t√¨m ki·∫øm trong FAISS: {e}")
            processing_time = time.time() - start_time
            return "L·ªói khi truy xu·∫•t th√¥ng tin.", [], None, None, processing_time

        # 3. Chu·∫©n b·ªã Ng·ªØ c·∫£nh (Context Preparation)
        retrieved_chunks = []
        retrieved_chunks_content = []
        retrieved_indices = []

        for idx, i in enumerate(indices[0]):
            if 0 <= i < len(all_chunks_data):
                chunk = all_chunks_data[i]
                # L·ªçc theo m√¥n h·ªçc n·∫øu c√≥ y√™u c·∫ßu
                if subject_filter and 'metadata' in chunk and 'subject_code' in chunk['metadata']:
                    chunk_subject = chunk['metadata']['subject_code']
                    # Ki·ªÉm tra xem m√£ m√¥n h·ªçc c√≥ kh·ªõp v·ªõi b·ªô l·ªçc kh√¥ng (kh·ªõp ƒë·∫ßy ƒë·ªß ho·∫∑c l√† ti·ªÅn t·ªë)
                    if not (chunk_subject == subject_filter or 
                           (subject_filter in chunk_subject and 
                            chunk_subject.startswith(subject_filter))):
                        continue
                
                # Th√™m v√†o danh s√°ch k·∫øt qu·∫£
                retrieved_chunks.append(chunk)
                retrieved_chunks_content.append(chunk.get("content", ""))
                retrieved_indices.append(i)
                
                # D·ª´ng khi ƒë·ªß top_k chunks
                if len(retrieved_chunks) >= top_k:
                    break
            else:
                status.warning(f"C·∫£nh b√°o: Index {i} n·∫±m ngo√†i ph·∫°m vi c·ªßa all_chunks_data.")
        
        if not retrieved_chunks_content:
            processing_time = time.time() - start_time
            if subject_filter:
                return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong syllabus m√¥n {subject_filter} ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", [], detected_subject_code, detected_subject_name, processing_time
            else:
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c√°c syllabus ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", [], None, None, processing_time

        status.write(f"‚úÖ ƒê√£ truy xu·∫•t {len(retrieved_chunks_content)} chunks li√™n quan")
        
        # N·∫øu ph√°t hi·ªán m√¥n h·ªçc, th√™m th√¥ng tin v√†o ng·ªØ c·∫£nh
        if detected_subject_code or subject_filter:
            subject_code_to_use = detected_subject_code if detected_subject_code else subject_filter
            subject_info = subject_metadata.get(subject_code_to_use, {})
            subject_context = f"Th√¥ng tin v·ªÅ m√¥n h·ªçc {subject_code_to_use}"
            if 'name' in subject_info:
                subject_context += f" ({subject_info['name']})"
            if 'credits' in subject_info:
                subject_context += f", s·ªë t√≠n ch·ªâ: {subject_info['credits']}"
            
            context_for_llm = subject_context + "\n\n" + "\n\n".join(retrieved_chunks_content)
        else:
            context_for_llm = "\n\n".join(retrieved_chunks_content)
        
        # T·∫°o prompt cho Gemini
        prompt = f"""D·ª±a v√†o c√°c th√¥ng tin sau ƒë√¢y t·ª´ syllabus c·ªßa c√°c m√¥n h·ªçc t·∫°i tr∆∞·ªùng ƒê·∫°i h·ªçc FPT:

-- B·∫ÆT ƒê·∫¶U NG·ªÆ C·∫¢NH SYLLABUS --
{context_for_llm}
-- K·∫æT TH√öC NG·ªÆ C·∫¢NH SYLLABUS --

H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c, CH·ªà d·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ng·ªØ c·∫£nh syllabus ·ªü tr√™n. 
N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

C√¢u h·ªèi: {query_text}

N·∫øu c√¢u h·ªèi l√† v·ªÅ m·ªôt m√¥n h·ªçc c·ª• th·ªÉ, lu√¥n ƒë·ªÅ c·∫≠p ƒë·∫øn m√£ m√¥n h·ªçc (nh∆∞ DPL302m) trong c√¢u tr·∫£ l·ªùi ƒë·ªÉ l√†m r√µ b·∫°n ƒëang cung c·∫•p th√¥ng tin v·ªÅ m√¥n h·ªçc n√†o.

Tr·∫£ l·ªùi:"""

        # 4. Sinh C√¢u tr·∫£ l·ªùi (Answer Generation) v·ªõi Gemini
        status.update(label="ƒêang sinh c√¢u tr·∫£ l·ªùi b·∫±ng Gemini...")
        try:
            model_options = {
                "temperature": temperature,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 1024,
            }
            
            gemini_model_name = "gemini-1.5-flash-latest"
            model = genai.GenerativeModel(gemini_model_name, generation_config=model_options)
            response = model.generate_content(prompt)
            answer = response.text
            status.write("‚úÖ ƒê√£ nh·∫≠n ph·∫£n h·ªìi t·ª´ Gemini")
        except Exception as e:
            status.error(f"‚ùå L·ªói khi Gemini sinh c√¢u tr·∫£ l·ªùi: {e}")
            if hasattr(e, 'response') and e.response:
                status.error(f"Gemini API Response Error: {e.response}")
            if hasattr(e, 'message'):
                status.error(f"Error message: {e.message}")
            processing_time = time.time() - start_time
            return "L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng Gemini.", [], detected_subject_code, detected_subject_name, processing_time
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    return answer, retrieved_chunks, detected_subject_code, detected_subject_name, processing_time

# UI c·ªßa ·ª©ng d·ª•ng Streamlit
def main():
    st.set_page_config(
        page_title="RAG Syllabus Query - FPTU",
        page_icon="üìö",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üìö H·ªá th·ªëng Truy v·∫•n Syllabus FPT University")
    st.markdown("H·ªèi ƒë√°p th√¥ng tin v·ªÅ c√°c m√¥n h·ªçc d·ª±a tr√™n syllabus c·ªßa tr∆∞·ªùng ƒê·∫°i h·ªçc FPT.")
    
    # T·∫£i resources
    with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
        faiss_index, chunks_data, embedding_model, subject_codes, subject_metadata = load_resources()
    
    if not faiss_index or not chunks_data or not embedding_model:
        st.error("Kh√¥ng th·ªÉ t·∫£i c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt. Vui l√≤ng ki·ªÉm tra l·ªói.")
        return
    
    # Sidebar cho c·∫•u h√¨nh
    st.sidebar.title("C·∫•u h√¨nh")
    
    selected_subject = st.sidebar.selectbox(
        "Ch·ªçn m√¥n h·ªçc c·ª• th·ªÉ (ƒë·ªÉ tr·ªëng ƒë·ªÉ t√¨m trong t·∫•t c·∫£ c√°c m√¥n):",
        ["T·∫•t c·∫£ c√°c m√¥n"] + subject_codes
    )
    
    subject_filter = None if selected_subject == "T·∫•t c·∫£ c√°c m√¥n" else selected_subject
    
    auto_detect_subject = st.sidebar.checkbox("T·ª± ƒë·ªông ph√°t hi·ªán m√¥n h·ªçc t·ª´ c√¢u h·ªèi", value=True)
    
    top_k = st.sidebar.slider("S·ªë l∆∞·ª£ng chunks ƒë·ªÉ truy xu·∫•t:", 1, 100, 20)
    
    temperature = st.sidebar.slider(
        "Nhi·ªát ƒë·ªô (ƒë·ªô s√°ng t·∫°o) c·ªßa Gemini:", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.2, 
        step=0.1
    )
    
    show_sources = st.sidebar.checkbox("Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o", value=True)
    
    # Kh·ªüi t·∫°o Gemini model 
    gemini_model = genai.GenerativeModel("gemini-1.5-flash-latest")
    
    # √î input cho c√¢u h·ªèi
    query_text = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ syllabus:", height=100)
    
    # G·ª£i √Ω c√¢u h·ªèi
    st.caption("""
    **G·ª£i √Ω c√¢u h·ªèi:** 
    - "DPL302m l√† m√¥n h·ªçc g√¨?"
    - "M√¥n Deep Learning c√≥ bao nhi√™u t√≠n ch·ªâ?"
    - "Chu·∫©n ƒë·∫ßu ra c·ªßa m√¥n DPL l√† g√¨?"
    - "Cho t√¥i bi·∫øt t√†i li·ªáu h·ªçc t·∫≠p c·ªßa m√¥n deep learning"
    """)
    
    # N√∫t t√¨m ki·∫øm
    col1, col2 = st.columns([1, 5])
    with col1:
        search_button = st.button("üîç T√¨m ki·∫øm", type="primary", use_container_width=True)
    
    # X·ª≠ l√Ω t√¨m ki·∫øm khi nh·∫•n n√∫t
    if search_button and query_text:
        # Th·ª±c hi·ªán RAG v√† hi·ªÉn th·ªã k·∫øt qu·∫£
        answer, retrieved_chunks, detected_subject, detected_subject_name, processing_time = get_answer_from_rag(
            query_text, faiss_index, chunks_data, embedding_model, gemini_model,
            subject_metadata, subject_filter, top_k, temperature, auto_detect_subject
        )
        
        # Hi·ªÉn th·ªã m√¥n h·ªçc ƒë∆∞·ª£c ph√°t hi·ªán (n·∫øu c√≥)
        if detected_subject and auto_detect_subject:
            st.info(f"üìò H·ªá th·ªëng ph√°t hi·ªán c√¢u h·ªèi li√™n quan ƒë·∫øn m√¥n: **{detected_subject}** ({detected_subject_name})")
        
        # Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω
        st.caption(f"Th·ªùi gian x·ª≠ l√Ω: {processing_time:.2f} gi√¢y")
        
        # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        st.markdown("### C√¢u tr·∫£ l·ªùi:")
        st.markdown(answer)
        
        # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o n·∫øu ƒë∆∞·ª£c ch·ªçn
        if show_sources and retrieved_chunks:
            st.markdown("### Ngu·ªìn tham kh·∫£o:")
            
            for i, chunk in enumerate(retrieved_chunks):
                with st.expander(f"Ngu·ªìn {i+1}: {chunk.get('metadata', {}).get('title', f'Chunk {i+1}')}"):
                    st.markdown(f"**M√¥n h·ªçc:** {chunk.get('metadata', {}).get('subject_code', 'N/A')}")
                    st.markdown(f"**Lo·∫°i n·ªôi dung:** {chunk.get('type', 'N/A')}")
                    st.markdown(f"**N·ªôi dung:**\n{chunk.get('content', 'N/A')}")

if __name__ == "__main__":
    main()