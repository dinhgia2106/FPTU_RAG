import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import google.generativeai as genai
import time
import asyncio

# X·ª≠ l√Ω event loop ƒë·ªÉ tr√°nh xung ƒë·ªôt gi·ªØa Streamlit v√† PyTorch
try:
    asyncio.get_event_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ƒê·∫£m b·∫£o API key ƒë∆∞·ª£c c·∫•u h√¨nh
if not GEMINI_API_KEY:
    st.error("GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ki·ªÉm tra file .env")
    st.stop()

try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh Gemini API: {e}")
    st.stop()

# Cache d·ªØ li·ªáu ƒë·ªÉ tr√°nh t·∫£i l·∫°i
@st.cache_resource
def load_resources():
    """T·∫£i FAISS index, d·ªØ li·ªáu chunks v√† c√°c m√¥ h√¨nh c·∫ßn thi·∫øt."""
    
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file
    faiss_index_path = "Faiss/all_syllabi_faiss.index"
    chunks_json_path = "Embedded/all_embeddings.json"
    
    # T·∫£i FAISS index
    try:
        index = faiss.read_index(faiss_index_path)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i FAISS index v·ªõi {index.ntotal} vectors")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i FAISS index: {e}")
        return None, None, None
    
    # T·∫£i d·ªØ li·ªáu chunks
    try:
        with open(chunks_json_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu chunks: {len(chunks_data)} chunks")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu chunks: {e}")
        return index, None, None
    
    # T·∫£i m√¥ h√¨nh embedding
    embedding_model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    try:
        embedding_model = SentenceTransformer(embedding_model_name)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh embedding")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh embedding: {e}")
        return index, chunks_data, None
    
    # L·∫•y danh s√°ch c√°c m√£ m√¥n h·ªçc
    subject_codes = set()
    for chunk in chunks_data:
        if 'metadata' in chunk and 'subject_code' in chunk['metadata']:
            subject_codes.add(chunk['metadata']['subject_code'])
    
    return index, chunks_data, embedding_model, sorted(list(subject_codes))

def get_answer_from_rag(query_text, faiss_index, all_chunks_data, embedding_model, gemini_model, 
                       subject_filter=None, top_k=5, temperature=0.2):
    """Th·ª±c hi·ªán pipeline RAG v·ªõi Gemini ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi."""
    if not query_text:
        return "Vui l√≤ng cung c·∫•p c√¢u h·ªèi.", []
    
    start_time = time.time()
    
    # 1. X·ª≠ l√Ω C√¢u h·ªèi (Query Processing)
    with st.status("ƒêang x·ª≠ l√Ω c√¢u h·ªèi..."):
        try:
            query_embedding = embedding_model.encode([query_text])
            st.write("‚úÖ ƒê√£ t·∫°o embedding cho c√¢u h·ªèi")
        except Exception as e:
            st.error(f"‚ùå L·ªói khi t·∫°o embedding cho c√¢u h·ªèi: {e}")
            return "L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi.", []

        # 2. Truy xu·∫•t Th√¥ng tin (Information Retrieval)
        try:
            distances, indices = faiss_index.search(query_embedding.astype(np.float32), top_k*3)  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ l·ªçc theo m√¥n h·ªçc
            st.write("‚úÖ ƒê√£ t√¨m ki·∫øm trong FAISS index")
        except Exception as e:
            st.error(f"‚ùå L·ªói khi t√¨m ki·∫øm trong FAISS: {e}")
            return "L·ªói khi truy xu·∫•t th√¥ng tin.", []

        # 3. Chu·∫©n b·ªã Ng·ªØ c·∫£nh (Context Preparation)
        retrieved_chunks = []
        retrieved_chunks_content = []
        retrieved_indices = []

        for idx, i in enumerate(indices[0]):
            if 0 <= i < len(all_chunks_data):
                chunk = all_chunks_data[i]
                # L·ªçc theo m√¥n h·ªçc n·∫øu c√≥ y√™u c·∫ßu
                if subject_filter and 'metadata' in chunk and 'subject_code' in chunk['metadata']:
                    if chunk['metadata']['subject_code'] != subject_filter:
                        continue
                
                # Th√™m v√†o danh s√°ch k·∫øt qu·∫£
                retrieved_chunks.append(chunk)
                retrieved_chunks_content.append(chunk.get("content", ""))
                retrieved_indices.append(i)
                
                # D·ª´ng khi ƒë·ªß top_k chunks
                if len(retrieved_chunks) >= top_k:
                    break
            else:
                st.warning(f"C·∫£nh b√°o: Index {i} n·∫±m ngo√†i ph·∫°m vi c·ªßa all_chunks_data.")
        
        if not retrieved_chunks_content:
            if subject_filter:
                return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong syllabus m√¥n {subject_filter} ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", []
            else:
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c√°c syllabus ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", []

        st.write(f"‚úÖ ƒê√£ truy xu·∫•t {len(retrieved_chunks_content)} chunks li√™n quan")
        context_for_llm = "\n\n".join(retrieved_chunks_content)
        
        # T·∫°o prompt cho Gemini
        prompt = f"""D·ª±a v√†o c√°c th√¥ng tin sau ƒë√¢y t·ª´ syllabus c·ªßa c√°c m√¥n h·ªçc t·∫°i tr∆∞·ªùng ƒê·∫°i h·ªçc FPT:

-- B·∫ÆT ƒê·∫¶U NG·ªÆ C·∫¢NH SYLLABUS --
{context_for_llm}
-- K·∫æT TH√öC NG·ªÆ C·∫¢NH SYLLABUS --

H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c, CH·ªà d·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ng·ªØ c·∫£nh syllabus ·ªü tr√™n. 
N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

C√¢u h·ªèi: {query_text}

Tr·∫£ l·ªùi:"""

        # 4. Sinh C√¢u tr·∫£ l·ªùi (Answer Generation) v·ªõi Gemini
        st.write("ƒêang sinh c√¢u tr·∫£ l·ªùi b·∫±ng Gemini...")
        try:
            model_options = {
                "temperature": temperature,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 1024,
            }
            
            gemini_model_name = "gemini-1.5-flash-latest"
            model = genai.GenerativeModel(gemini_model_name, generation_config=model_options)
            response = model.generate_content(prompt)
            answer = response.text
            st.write("‚úÖ ƒê√£ nh·∫≠n ph·∫£n h·ªìi t·ª´ Gemini")
        except Exception as e:
            st.error(f"‚ùå L·ªói khi Gemini sinh c√¢u tr·∫£ l·ªùi: {e}")
            if hasattr(e, 'response') and e.response:
                st.error(f"Gemini API Response Error: {e.response}")
            if hasattr(e, 'message'):
                st.error(f"Error message: {e.message}")
            return "L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng Gemini.", []
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    return answer, retrieved_chunks, processing_time

# UI c·ªßa ·ª©ng d·ª•ng Streamlit
def main():
    st.set_page_config(
        page_title="RAG Syllabus Query - FPTU",
        page_icon="üìö",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üìö H·ªá th·ªëng Truy v·∫•n Syllabus FPT University")
    st.markdown("H·ªèi ƒë√°p th√¥ng tin v·ªÅ c√°c m√¥n h·ªçc d·ª±a tr√™n syllabus c·ªßa tr∆∞·ªùng ƒê·∫°i h·ªçc FPT.")
    
    # T·∫£i resources
    with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
        faiss_index, chunks_data, embedding_model, subject_codes = load_resources()
    
    if not faiss_index or not chunks_data or not embedding_model:
        st.error("Kh√¥ng th·ªÉ t·∫£i c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt. Vui l√≤ng ki·ªÉm tra l·ªói.")
        return
    
    # Sidebar cho c·∫•u h√¨nh
    st.sidebar.title("C·∫•u h√¨nh")
    
    selected_subject = st.sidebar.selectbox(
        "Ch·ªçn m√¥n h·ªçc c·ª• th·ªÉ (ƒë·ªÉ tr·ªëng ƒë·ªÉ t√¨m trong t·∫•t c·∫£ c√°c m√¥n):",
        ["T·∫•t c·∫£ c√°c m√¥n"] + subject_codes
    )
    
    subject_filter = None if selected_subject == "T·∫•t c·∫£ c√°c m√¥n" else selected_subject
    
    top_k = st.sidebar.slider("S·ªë l∆∞·ª£ng chunks ƒë·ªÉ truy xu·∫•t:", 1, 10, 5)
    
    temperature = st.sidebar.slider(
        "Nhi·ªát ƒë·ªô (ƒë·ªô s√°ng t·∫°o) c·ªßa Gemini:", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.2, 
        step=0.1
    )
    
    show_sources = st.sidebar.checkbox("Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o", value=True)
    
    # Kh·ªüi t·∫°o Gemini model 
    gemini_model = genai.GenerativeModel("gemini-1.5-flash-latest")
    
    # √î input cho c√¢u h·ªèi
    query_text = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ syllabus:", height=100)
    
    # N√∫t t√¨m ki·∫øm
    col1, col2 = st.columns([1, 5])
    with col1:
        search_button = st.button("üîç T√¨m ki·∫øm", type="primary", use_container_width=True)
    
    # X·ª≠ l√Ω t√¨m ki·∫øm khi nh·∫•n n√∫t
    if search_button and query_text:
        # Th·ª±c hi·ªán RAG v√† hi·ªÉn th·ªã k·∫øt qu·∫£
        answer, retrieved_chunks, processing_time = get_answer_from_rag(
            query_text, faiss_index, chunks_data, embedding_model, gemini_model,
            subject_filter, top_k, temperature
        )
        
        # Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω
        st.caption(f"Th·ªùi gian x·ª≠ l√Ω: {processing_time:.2f} gi√¢y")
        
        # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        st.markdown("### C√¢u tr·∫£ l·ªùi:")
        st.markdown(answer)
        
        # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o n·∫øu ƒë∆∞·ª£c ch·ªçn
        if show_sources and retrieved_chunks:
            st.markdown("### Ngu·ªìn tham kh·∫£o:")
            
            for i, chunk in enumerate(retrieved_chunks):
                with st.expander(f"Ngu·ªìn {i+1}: {chunk.get('metadata', {}).get('title', f'Chunk {i+1}')}"):
                    st.markdown(f"**M√¥n h·ªçc:** {chunk.get('metadata', {}).get('subject_code', 'N/A')}")
                    st.markdown(f"**Lo·∫°i n·ªôi dung:** {chunk.get('type', 'N/A')}")
                    st.markdown(f"**N·ªôi dung:**\n{chunk.get('content', 'N/A')}")

if __name__ == "__main__":
    main()