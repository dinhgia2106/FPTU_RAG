"""
Improved Syllabus Search Engine App - C·∫£i ti·∫øn ·ª©ng d·ª•ng t√¨m ki·∫øm Syllabus FPT

·ª®ng d·ª•ng Streamlit t√≠ch h·ª£p c√°c t√≠nh nƒÉng t√¨m ki·∫øm n√¢ng cao, hi·ªÉu ng·ªØ c·∫£nh,
v√† truy v·∫•n ph·ª©c t·∫°p cho d·ªØ li·ªáu syllabus.
"""

import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import google.generativeai as genai
import time
import asyncio
import re
import sys
from typing import Dict, List, Any, Optional, Union

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import c√°c module t√πy ch·ªânh
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import c√°c module t√πy ch·ªânh
from contextual_query_processor import (
    ContextualQueryProcessor, 
    EntityLinker, 
    QueryExecutor, 
    ContextualSearchEngine,
    build_entity_data_from_chunks,
    save_entity_data,
    load_entity_data,
    EntityType,
    QueryType
)

# X·ª≠ l√Ω event loop ƒë·ªÉ tr√°nh xung ƒë·ªôt gi·ªØa Streamlit v√† PyTorch
try:
    asyncio.get_event_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ƒê·∫£m b·∫£o API key ƒë∆∞·ª£c c·∫•u h√¨nh
if not GEMINI_API_KEY:
    st.error("GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ki·ªÉm tra file .env")
    st.stop()

try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh Gemini API: {e}")
    st.stop()

# Cache d·ªØ li·ªáu ƒë·ªÉ tr√°nh t·∫£i l·∫°i
@st.cache_resource
def load_resources():
    """T·∫£i FAISS index, d·ªØ li·ªáu chunks v√† c√°c m√¥ h√¨nh c·∫ßn thi·∫øt."""
    
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file
    faiss_index_path = "Faiss/all_syllabi_faiss.index"
    chunks_json_path = "Embedded/all_embeddings.json"
    entity_data_path = "Entity/entity_data.json"
    
    # T·∫£i FAISS index
    try:
        index = faiss.read_index(faiss_index_path)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i FAISS index v·ªõi {index.ntotal} vectors")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i FAISS index: {e}")
        return None, None, None, None, None, None
    
    # T·∫£i d·ªØ li·ªáu chunks
    try:
        with open(chunks_json_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu chunks: {len(chunks_data)} chunks")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu chunks: {e}")
        return index, None, None, None, None, None
    
    # T·∫£i m√¥ h√¨nh embedding
    embedding_model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    try:
        embedding_model = SentenceTransformer(embedding_model_name)
        st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh embedding")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh embedding: {e}")
        return index, chunks_data, None, None, None, None
    
    # T·∫£i ho·∫∑c x√¢y d·ª±ng d·ªØ li·ªáu th·ª±c th·ªÉ
    entity_data = {}
    try:
        if os.path.exists(entity_data_path):
            entity_data = load_entity_data(entity_data_path)
            st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu th·ª±c th·ªÉ: {len(entity_data)} th·ª±c th·ªÉ")
        else:
            st.sidebar.info("ƒêang x√¢y d·ª±ng d·ªØ li·ªáu th·ª±c th·ªÉ t·ª´ chunks...")
            entity_data = build_entity_data_from_chunks(chunks_data, embedding_model)
            os.makedirs(os.path.dirname(entity_data_path), exist_ok=True)
            save_entity_data(entity_data, entity_data_path)
            st.sidebar.success(f"‚úÖ ƒê√£ x√¢y d·ª±ng v√† l∆∞u d·ªØ li·ªáu th·ª±c th·ªÉ: {len(entity_data)} th·ª±c th·ªÉ")
    except Exception as e:
        st.sidebar.warning(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu th·ª±c th·ªÉ: {e}. S·∫Ω ti·∫øp t·ª•c v·ªõi d·ªØ li·ªáu tr·ªëng.")
    
    # T·∫°o b·∫£ng tra c·ª©u subject_code -> syllabus_name v√† c√°c metadata kh√°c
    subject_metadata = {}
    for chunk in chunks_data:
        if 'metadata' in chunk and 'subject_code' in chunk['metadata']:
            subject_code = chunk['metadata']['subject_code']
            if subject_code not in subject_metadata and chunk['type'] == 'general_info':
                # T√¨m t√™n m√¥n h·ªçc t·ª´ n·ªôi dung chunk
                content = chunk.get('content', '')
                name_match = re.search(r"T√™n m√¥n h·ªçc: ([^(]+)", content)
                if name_match:
                    subject_name = name_match.group(1).strip()
                    subject_metadata[subject_code] = {
                        'name': subject_name,
                        'keywords': [subject_code, subject_name]
                    }
                    
                    # Extract number of credits
                    credit_match = re.search(r"S·ªë t√≠n ch·ªâ: ([0-9]+)", content)
                    if credit_match:
                        subject_metadata[subject_code]['credits'] = credit_match.group(1)
                        
    # L·∫•y danh s√°ch c√°c m√£ m√¥n h·ªçc
    subject_codes = sorted(list(subject_metadata.keys()))
    
    # T·∫°o contextual search engine
    try:
        search_engine = ContextualSearchEngine(
            embedding_model=embedding_model,
            faiss_index=index,
            chunks_data=chunks_data,
            entity_data=entity_data
        )
        st.sidebar.success(f"‚úÖ ƒê√£ kh·ªüi t·∫°o Contextual Search Engine")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o Contextual Search Engine: {e}")
        search_engine = None
    
    return index, chunks_data, embedding_model, subject_codes, subject_metadata, search_engine

def identify_subject_from_query(query, subject_metadata, embedding_model, faiss_index, all_chunks_data, top_k=3):
    """X√°c ƒë·ªãnh m√¥n h·ªçc m√† c√¢u h·ªèi c√≥ th·ªÉ ƒëang ƒë·ªÅ c·∫≠p ƒë·∫øn."""
    # T·∫°o embedding cho c√¢u h·ªèi
    query_embedding = embedding_model.encode([query])
    
    # T√¨m ki·∫øm top_k chunks g·∫ßn nh·∫•t
    distances, indices = faiss_index.search(query_embedding.astype(np.float32), top_k*5)
    
    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói m√£ m√¥n h·ªçc
    subject_counts = {}
    subject_scores = {}  # ƒêi·ªÉm s·ªë cho m·ªói m√¥n h·ªçc d·ª±a tr√™n kho·∫£ng c√°ch embedding
    
    for idx, i in enumerate(indices[0]):
        if 0 <= i < len(all_chunks_data):
            chunk = all_chunks_data[i]
            if 'metadata' in chunk and 'subject_code' in chunk['metadata']:
                subject_code = chunk['metadata']['subject_code']
                
                # TƒÉng s·ªë l·∫ßn xu·∫•t hi·ªán
                if subject_code not in subject_counts:
                    subject_counts[subject_code] = 0
                    subject_scores[subject_code] = 0
                
                subject_counts[subject_code] += 1
                # T√≠nh ƒëi·ªÉm: 1/kho·∫£ng c√°ch (c√†ng g·∫ßn c√†ng cao ƒëi·ªÉm)
                # C·ªông th√™m ƒëi·ªÉm n·∫øu chunk l√† general_info
                score_boost = 2 if chunk['type'] == 'general_info' else 1
                subject_scores[subject_code] += (1 / (1 + distances[0][idx])) * score_boost
    
    # Ki·ªÉm tra xem c√≥ tr·ª±c ti·∫øp ƒë·ªÅ c·∫≠p ƒë·∫øn m√£ m√¥n h·ªçc trong c√¢u h·ªèi kh√¥ng
    direct_mention = None
    for subject_code in subject_metadata:
        # Ki·ªÉm tra m√£ m√¥n h·ªçc ƒë·∫ßy ƒë·ªß
        if subject_code.lower() in query.lower():
            direct_mention = subject_code
            break
        # Ki·ªÉm tra prefix c·ªßa m√£ m√¥n h·ªçc (e.g., "DPL" cho "DPL302m")
        code_prefix = ''.join([c for c in subject_code if c.isalpha()])
        if code_prefix.lower() in query.lower() and len(code_prefix) >= 2:  # Ch·ªâ x√©t prefix c√≥ √≠t nh·∫•t 2 k√Ω t·ª±
            # T√¨m t·∫•t c·∫£ m√¥n c√≥ prefix n√†y
            possible_subjects = [code for code in subject_metadata if code.startswith(code_prefix)]
            if len(possible_subjects) == 1:
                direct_mention = possible_subjects[0]
                break
            elif len(possible_subjects) > 1:
                # N·∫øu c√≥ nhi·ªÅu m√¥n c√πng prefix, ch·ªçn m√¥n c√≥ ƒëi·ªÉm cao nh·∫•t
                highest_score = 0
                for subj in possible_subjects:
                    if subj in subject_scores and subject_scores[subj] > highest_score:
                        highest_score = subject_scores[subj]
                        direct_mention = subj
                break
    
    if direct_mention:
        return direct_mention, subject_metadata.get(direct_mention, {}).get('name', '')
    
    # N·∫øu kh√¥ng c√≥ ƒë·ªÅ c·∫≠p tr·ª±c ti·∫øp, t√¨m m√¥n c√≥ ƒëi·ªÉm cao nh·∫•t
    if subject_scores:
        best_subject = max(subject_scores, key=subject_scores.get)
        return best_subject, subject_metadata.get(best_subject, {}).get('name', '')
    
    return None, None

def get_answer_from_contextual_rag(query_text, search_engine, gemini_model, 
                                  subject_metadata, subject_filter=None, top_k=5, temperature=0.2,
                                  auto_detect_subject=True):
    """Th·ª±c hi·ªán pipeline RAG v·ªõi Contextual Search Engine v√† Gemini ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi."""
    start_time = time.time()
    
    if not query_text:
        return "Vui l√≤ng cung c·∫•p c√¢u h·ªèi.", [], None, None, 0
    
    detected_subject_code = None
    detected_subject_name = None
    
    with st.status("ƒêang x·ª≠ l√Ω c√¢u h·ªèi...") as status:
        # 1. X√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan (n·∫øu ƒë∆∞·ª£c b·∫≠t)
        if auto_detect_subject and not subject_filter:
            status.update(label="ƒêang ph√¢n t√≠ch truy v·∫•n v√† x√°c ƒë·ªãnh m√¥n h·ªçc...")
            
            # S·ª≠ d·ª•ng ph√¢n t√≠ch truy v·∫•n t·ª´ ContextualQueryProcessor
            query_analysis = search_engine.query_processor.analyze_query(query_text)
            
            # Ki·ªÉm tra xem c√≥ th·ª±c th·ªÉ Course trong ph√¢n t√≠ch kh√¥ng
            course_entities = query_analysis.get("entities", {}).get(EntityType.COURSE, [])
            if course_entities:
                subject_code = course_entities[0].get("value")
                detected_subject_code = subject_code
                detected_subject_name = subject_metadata.get(subject_code, {}).get('name', '')
                status.update(label=f"ƒê√£ x√°c ƒë·ªãnh c√¢u h·ªèi li√™n quan ƒë·∫øn m√¥n {detected_subject_code} ({detected_subject_name})")
                subject_filter = detected_subject_code
            else:
                # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng d·ª±a tr√™n vector similarity
                detected_subject_code, detected_subject_name = identify_subject_from_query(
                    query_text, subject_metadata, search_engine.embedding_model, 
                    search_engine.faiss_index, search_engine.chunks_data
                )
                
                if detected_subject_code:
                    status.update(label=f"ƒê√£ x√°c ƒë·ªãnh c√¢u h·ªèi c√≥ th·ªÉ li√™n quan ƒë·∫øn m√¥n {detected_subject_code} ({detected_subject_name})")
                    subject_filter = detected_subject_code
                else:
                    status.update(label="Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c m√¥n h·ªçc c·ª• th·ªÉ, t√¨m ki·∫øm trong t·∫•t c·∫£ c√°c m√¥n")
        
        # 2. Th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ c·∫£nh
        try:
            status.update(label="ƒêang th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ c·∫£nh...")
            
            # √Åp d·ª•ng b·ªô l·ªçc m√¥n h·ªçc n·∫øu c√≥
            if subject_filter:
                # T·∫°o truy v·∫•n c√≥ c·∫•u tr√∫c v·ªõi b·ªô l·ªçc m√¥n h·ªçc
                structured_query = {
                    "type": "filter",
                    "target": {
                        "entity_type": EntityType.COURSE,
                        "entity_value": subject_filter
                    }
                }
                search_results = search_engine.search(query_text, top_k, structured_query)
            else:
                search_results = search_engine.search(query_text, top_k)
            
            status.write("‚úÖ ƒê√£ th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ c·∫£nh")
            
            # Ki·ªÉm tra k·∫øt qu·∫£ t√¨m ki·∫øm
            if not search_results.get("success") or not search_results.get("results"):
                status.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p, s·∫Ω th·ª≠ t√¨m ki·∫øm vector")
                
                # Th·ª±c hi·ªán t√¨m ki·∫øm vector nh∆∞ ph∆∞∆°ng ph√°p d·ª± ph√≤ng
                query_embedding = search_engine.embedding_model.encode([query_text])
                distances, indices = search_engine.faiss_index.search(query_embedding.astype(np.float32), top_k)
                
                retrieved_chunks = []
                for i, idx in enumerate(indices[0]):
                    if 0 <= idx < len(search_engine.chunks_data):
                        chunk = search_engine.chunks_data[idx]
                        
                        # L·ªçc theo m√¥n h·ªçc n·∫øu c√≥ y√™u c·∫ßu
                        if subject_filter and 'metadata' in chunk and 'subject_code' in chunk['metadata']:
                            chunk_subject = chunk['metadata']['subject_code']
                            if chunk_subject != subject_filter:
                                continue
                        
                        retrieved_chunks.append({
                            "chunk": chunk,
                            "distance": float(distances[0][i]),
                            "score": float(1 / (1 + distances[0][i]))
                        })
                
                if not retrieved_chunks:
                    processing_time = time.time() - start_time
                    if subject_filter:
                        return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong syllabus m√¥n {subject_filter} ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", [], detected_subject_code, detected_subject_name, processing_time
                    else:
                        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c√°c syllabus ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", [], None, None, processing_time
                
                search_results = {
                    "success": True,
                    "query_type": "vector",
                    "results": retrieved_chunks
                }
            
        except Exception as e:
            status.error(f"‚ùå L·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm: {e}")
            processing_time = time.time() - start_time
            return f"L·ªói khi t√¨m ki·∫øm th√¥ng tin: {e}", [], detected_subject_code, detected_subject_name, processing_time
        
        # 3. Chu·∫©n b·ªã ng·ªØ c·∫£nh cho LLM
        status.update(label="ƒêang chu·∫©n b·ªã ng·ªØ c·∫£nh cho Gemini...")
        
        context_chunks = []
        if search_results.get("query_type") == "vector":
            # K·∫øt qu·∫£ t·ª´ t√¨m ki·∫øm vector
            for result in search_results.get("results", []):
                chunk = result.get("chunk", {})
                if "content" in chunk:
                    context_chunks.append(chunk.get("content", ""))
        else:
            # K·∫øt qu·∫£ t·ª´ t√¨m ki·∫øm ng·ªØ c·∫£nh
            for result in search_results.get("results", []):
                if isinstance(result, dict):
                    if "chunk" in result and "content" in result["chunk"]:
                        context_chunks.append(result["chunk"].get("content", ""))
                    elif "content" in result:
                        context_chunks.append(result.get("content", ""))
                    elif "entity_data" in result and "content" in result["entity_data"]:
                        context_chunks.append(result["entity_data"].get("content", ""))
        
        if not context_chunks:
            processing_time = time.time() - start_time
            if subject_filter:
                return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong syllabus m√¥n {subject_filter} ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", [], detected_subject_code, detected_subject_name, processing_time
            else:
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c√°c syllabus ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.", [], None, None, processing_time
        
        status.write(f"‚úÖ ƒê√£ chu·∫©n b·ªã {len(context_chunks)} ƒëo·∫°n ng·ªØ c·∫£nh")
        
        # N·∫øu ph√°t hi·ªán m√¥n h·ªçc, th√™m th√¥ng tin v√†o ng·ªØ c·∫£nh
        if detected_subject_code or subject_filter:
            subject_code_to_use = detected_subject_code if detected_subject_code else subject_filter
            subject_info = subject_metadata.get(subject_code_to_use, {})
            subject_context = f"Th√¥ng tin v·ªÅ m√¥n h·ªçc {subject_code_to_use}"
            if 'name' in subject_info:
                subject_context += f" ({subject_info['name']})"
            if 'credits' in subject_info:
                subject_context += f", s·ªë t√≠n ch·ªâ: {subject_info['credits']}"
            
            context_for_llm = subject_context + "\n\n" + "\n\n".join(context_chunks)
        else:
            context_for_llm = "\n\n".join(context_chunks)
        
        # Th√™m th√¥ng tin ph√¢n t√≠ch truy v·∫•n v√†o prompt n·∫øu c√≥
        query_analysis_info = ""
        if "query_analysis" in search_results:
            analysis = search_results["query_analysis"]
            query_type = analysis.get("query_type", "")
            
            if query_type == QueryType.SIMPLE_INFO:
                query_analysis_info = "ƒê√¢y l√† truy v·∫•n th√¥ng tin ƒë∆°n gi·∫£n v·ªÅ m·ªôt m√¥n h·ªçc."
            elif query_type == QueryType.ATTRIBUTE:
                attributes = analysis.get("attributes", [])
                if attributes:
                    attr_names = [attr.get("name") for attr in attributes]
                    query_analysis_info = f"ƒê√¢y l√† truy v·∫•n v·ªÅ thu·ªôc t√≠nh {', '.join(attr_names)} c·ªßa m·ªôt m√¥n h·ªçc."
            elif query_type == QueryType.RELATIONSHIP:
                query_analysis_info = "ƒê√¢y l√† truy v·∫•n v·ªÅ m·ªëi quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ (v√≠ d·ª•: session, CLO)."
            elif query_type == QueryType.AGGREGATION:
                aggregations = analysis.get("aggregations", [])
                if aggregations:
                    agg_names = [agg.get("name") for agg in aggregations]
                    query_analysis_info = f"ƒê√¢y l√† truy v·∫•n t·ªïng h·ª£p {', '.join(agg_names)}."
            elif query_type == QueryType.CLASSIFICATION:
                classifications = analysis.get("classifications", [])
                if classifications:
                    class_names = [cls.get("name") for cls in classifications]
                    query_analysis_info = f"ƒê√¢y l√† truy v·∫•n ph√¢n lo·∫°i theo {', '.join(class_names)}."
            elif query_type == QueryType.LINKING:
                query_analysis_info = "ƒê√¢y l√† truy v·∫•n li√™n k·∫øt gi·ªØa c√°c th·ª±c th·ªÉ."
        
        # T·∫°o prompt cho Gemini
        prompt = f"""D·ª±a v√†o c√°c th√¥ng tin sau ƒë√¢y t·ª´ syllabus c·ªßa c√°c m√¥n h·ªçc t·∫°i tr∆∞·ªùng ƒê·∫°i h·ªçc FPT:

-- B·∫ÆT ƒê·∫¶U NG·ªÆ C·∫¢NH SYLLABUS --
{context_for_llm}
-- K·∫æT TH√öC NG·ªÆ C·∫¢NH SYLLABUS --

{query_analysis_info}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c, CH·ªà d·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ng·ªØ c·∫£nh syllabus ·ªü tr√™n. 
N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

C√¢u h·ªèi: {query_text}

N·∫øu c√¢u h·ªèi l√† v·ªÅ m·ªôt m√¥n h·ªçc c·ª• th·ªÉ, lu√¥n ƒë·ªÅ c·∫≠p ƒë·∫øn m√£ m√¥n h·ªçc (nh∆∞ DPL302m) trong c√¢u tr·∫£ l·ªùi ƒë·ªÉ l√†m r√µ b·∫°n ƒëang cung c·∫•p th√¥ng tin v·ªÅ m√¥n h·ªçc n√†o.

Tr·∫£ l·ªùi:"""

        # 4. Sinh C√¢u tr·∫£ l·ªùi (Answer Generation) v·ªõi Gemini
        status.update(label="ƒêang sinh c√¢u tr·∫£ l·ªùi b·∫±ng Gemini...")
        try:
            model_options = {
                "temperature": temperature,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 1024,
            }
            
            gemini_model_name = "gemini-1.5-flash"
            model = genai.GenerativeModel(gemini_model_name, generation_config=model_options)
            response = model.generate_content(prompt)
            answer = response.text
            status.write("‚úÖ ƒê√£ nh·∫≠n ph·∫£n h·ªìi t·ª´ Gemini")
        except Exception as e:
            status.error(f"‚ùå L·ªói khi Gemini sinh c√¢u tr·∫£ l·ªùi: {e}")
            if hasattr(e, 'response') and e.response:
                status.error(f"Gemini API Response Error: {e.response}")
            if hasattr(e, 'message'):
                status.error(f"Error message: {e.message}")
            processing_time = time.time() - start_time
            return "L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng Gemini.", [], detected_subject_code, detected_subject_name, processing_time
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    return answer, search_results.get("results", []), detected_subject_code, detected_subject_name, processing_time

# UI c·ªßa ·ª©ng d·ª•ng Streamlit
def main():
    st.set_page_config(
        page_title="Enhanced Syllabus Query - FPTU",
        page_icon="üìö",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üìö H·ªá th·ªëng Truy v·∫•n Syllabus FPT University (C·∫£i ti·∫øn)")
    st.markdown("H·ªèi ƒë√°p th√¥ng tin v·ªÅ c√°c m√¥n h·ªçc d·ª±a tr√™n syllabus c·ªßa tr∆∞·ªùng ƒê·∫°i h·ªçc FPT v·ªõi kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh n√¢ng cao.")
    
    # T·∫£i resources
    with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
        faiss_index, chunks_data, embedding_model, subject_codes, subject_metadata, search_engine = load_resources()
    
    if not faiss_index or not chunks_data or not embedding_model or not search_engine:
        st.error("Kh√¥ng th·ªÉ t·∫£i c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt. Vui l√≤ng ki·ªÉm tra l·ªói.")
        return
    
    # Sidebar cho c·∫•u h√¨nh
    st.sidebar.title("C·∫•u h√¨nh")
    
    selected_subject = st.sidebar.selectbox(
        "Ch·ªçn m√¥n h·ªçc c·ª• th·ªÉ (ƒë·ªÉ tr·ªëng ƒë·ªÉ t√¨m trong t·∫•t c·∫£ c√°c m√¥n):",
        ["T·∫•t c·∫£ c√°c m√¥n"] + subject_codes
    )
    
    subject_filter = None if selected_subject == "T·∫•t c·∫£ c√°c m√¥n" else selected_subject
    
    auto_detect_subject = st.sidebar.checkbox("T·ª± ƒë·ªông ph√°t hi·ªán m√¥n h·ªçc t·ª´ c√¢u h·ªèi", value=True)
    
    top_k = st.sidebar.slider("S·ªë l∆∞·ª£ng chunks ƒë·ªÉ truy xu·∫•t:", 1, 5000, 100)
    
    temperature = st.sidebar.slider(
        "Nhi·ªát ƒë·ªô (ƒë·ªô s√°ng t·∫°o) c·ªßa Gemini:", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.2, 
        step=0.1
    )
    
    show_sources = st.sidebar.checkbox("Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o", value=True)
    show_analysis = st.sidebar.checkbox("Hi·ªÉn th·ªã ph√¢n t√≠ch truy v·∫•n", value=False)
    
    # Kh·ªüi t·∫°o Gemini model 
    gemini_model = genai.GenerativeModel("gemini-1.5-flash")
    
    # √î input cho c√¢u h·ªèi
    query_text = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ syllabus:", height=100)
    
    # G·ª£i √Ω c√¢u h·ªèi
    st.caption("""
    **G·ª£i √Ω c√¢u h·ªèi n√¢ng cao:** 
    - "DPL302m l√† m√¥n h·ªçc g√¨?"
    - "M√¥n Deep Learning c√≥ bao nhi√™u t√≠n ch·ªâ?"
    - "Chu·∫©n ƒë·∫ßu ra c·ªßa m√¥n DPL l√† g√¨?"
    - "M√¥n DPL302m c√≥ bao nhi√™u CLO?"
    - "C√≥ bao nhi√™u m√¥n to√°n?"
    - "M√¥n DPL302m c√≥ bao nhi√™u b√†i ki·ªÉm tra, c√°ch t√≠nh ƒëi·ªÉm th·∫ø n√†o?"
    """)
    
    # N√∫t t√¨m ki·∫øm
    col1, col2 = st.columns([1, 5])
    with col1:
        search_button = st.button("üîç T√¨m ki·∫øm", type="primary", use_container_width=True)
    
    # X·ª≠ l√Ω t√¨m ki·∫øm khi nh·∫•n n√∫t
    if search_button and query_text:
        # Th·ª±c hi·ªán RAG v√† hi·ªÉn th·ªã k·∫øt qu·∫£
        answer, retrieved_results, detected_subject, detected_subject_name, processing_time = get_answer_from_contextual_rag(
            query_text, search_engine, gemini_model,
            subject_metadata, subject_filter, top_k, temperature, auto_detect_subject
        )
        
        # Hi·ªÉn th·ªã th√¥ng tin m√¥n h·ªçc ƒë∆∞·ª£c ph√°t hi·ªán
        if detected_subject and auto_detect_subject and not subject_filter:
            st.info(f"üìå C√¢u h·ªèi ƒë∆∞·ª£c x√°c ƒë·ªãnh li√™n quan ƒë·∫øn m√¥n: **{detected_subject}** ({detected_subject_name})")
        
        # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        st.markdown("### C√¢u tr·∫£ l·ªùi:")
        st.markdown(answer)
        
        # Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω
        st.caption(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {processing_time:.2f} gi√¢y")
        
        # Hi·ªÉn th·ªã ph√¢n t√≠ch truy v·∫•n n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if show_analysis:
            st.markdown("### Ph√¢n t√≠ch truy v·∫•n:")
            
            # Ph√¢n t√≠ch truy v·∫•n
            query_analysis = search_engine.query_processor.analyze_query(query_text)
            
            # Hi·ªÉn th·ªã lo·∫°i truy v·∫•n
            query_type = query_analysis.get("query_type", "")
            query_type_names = {
                QueryType.SIMPLE_INFO: "Truy v·∫•n th√¥ng tin ƒë∆°n gi·∫£n",
                QueryType.ATTRIBUTE: "Truy v·∫•n thu·ªôc t√≠nh",
                QueryType.RELATIONSHIP: "Truy v·∫•n quan h·ªá",
                QueryType.AGGREGATION: "Truy v·∫•n t·ªïng h·ª£p",
                QueryType.CLASSIFICATION: "Truy v·∫•n ph√¢n lo·∫°i",
                QueryType.LINKING: "Truy v·∫•n li√™n k·∫øt"
            }
            st.markdown(f"**Lo·∫°i truy v·∫•n:** {query_type_names.get(query_type, query_type)}")
            
            # Hi·ªÉn th·ªã c√°c th·ª±c th·ªÉ ƒë∆∞·ª£c nh·∫≠n di·ªán
            entities = query_analysis.get("entities", {})
            if entities:
                st.markdown("**Th·ª±c th·ªÉ ƒë∆∞·ª£c nh·∫≠n di·ªán:**")
                for entity_type, entity_list in entities.items():
                    entity_type_names = {
                        EntityType.COURSE: "M√¥n h·ªçc",
                        EntityType.SESSION: "Bu·ªïi h·ªçc",
                        EntityType.CLO: "Chu·∫©n ƒë·∫ßu ra",
                        EntityType.ASSESSMENT: "ƒê√°nh gi√°",
                        EntityType.MATERIAL: "T√†i li·ªáu"
                    }
                    st.markdown(f"- {entity_type_names.get(entity_type, entity_type)}: {', '.join([e.get('value', '') for e in entity_list])}")
            
            # Hi·ªÉn th·ªã c√°c thu·ªôc t√≠nh ƒë∆∞·ª£c nh·∫≠n di·ªán
            attributes = query_analysis.get("attributes", [])
            if attributes:
                st.markdown("**Thu·ªôc t√≠nh ƒë∆∞·ª£c nh·∫≠n di·ªán:**")
                for attr in attributes:
                    st.markdown(f"- {attr.get('name', '')}")
            
            # Hi·ªÉn th·ªã truy v·∫•n c√≥ c·∫•u tr√∫c
            structured_query = query_analysis.get("structured_query", {})
            if structured_query:
                st.markdown("**Truy v·∫•n c√≥ c·∫•u tr√∫c:**")
                st.json(structured_query)
        
        # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if show_sources and retrieved_results:
            st.markdown("### Ngu·ªìn tham kh·∫£o:")
            
            for i, result in enumerate(retrieved_results[:5]):  # Gi·ªõi h·∫°n hi·ªÉn th·ªã 5 ngu·ªìn
                with st.expander(f"Ngu·ªìn #{i+1}"):
                    if isinstance(result, dict):
                        if "chunk" in result and "content" in result["chunk"]:
                            st.markdown(result["chunk"].get("content", ""))
                            if "metadata" in result["chunk"]:
                                metadata = result["chunk"]["metadata"]
                                st.caption(f"Lo·∫°i: {result['chunk'].get('type', 'N/A')} | M√¥n h·ªçc: {metadata.get('subject_code', 'N/A')} | Ti√™u ƒë·ªÅ: {metadata.get('title', 'N/A')}")
                        elif "content" in result:
                            st.markdown(result.get("content", ""))
                            if "metadata" in result:
                                metadata = result["metadata"]
                                st.caption(f"Lo·∫°i: {result.get('type', 'N/A')} | M√¥n h·ªçc: {metadata.get('subject_code', 'N/A')} | Ti√™u ƒë·ªÅ: {metadata.get('title', 'N/A')}")
                        elif "entity_data" in result and "content" in result["entity_data"]:
                            st.markdown(result["entity_data"].get("content", ""))
                            entity_data = result["entity_data"]
                            st.caption(f"Lo·∫°i th·ª±c th·ªÉ: {entity_data.get('entity_type', 'N/A')} | ID: {entity_data.get('entity_id', 'N/A')}")

if __name__ == "__main__":
    main()
