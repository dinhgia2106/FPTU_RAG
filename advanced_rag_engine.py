"""
Advanced RAG Engine - H·ªá th·ªëng RAG ti√™n ti·∫øn cho FPTU
T√≠ch h·ª£p c√°c k·ªπ thu·∫≠t ti√™n ti·∫øn: Hierarchical Indexing, Multi-stage Retrieval, Query Routing, Document Summarization, Multi-hop Query
ENHANCED v·ªõi GraphRAG: Vector Search + Knowledge Graph Traversal
"""

import json
import numpy as np
import faiss
from typing import Dict, List, Any, Optional, Tuple, Union
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from dataclasses import dataclass
import re
import os
from collections import defaultdict
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
from enum import Enum
import time
import hashlib
import pickle

# Import GraphRAG components
try:
    from graph_database import GraphDatabase, GraphPath, GraphNode, GraphRelationship
    GRAPH_AVAILABLE = True
except ImportError:
    GRAPH_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("‚ö† GraphDatabase module kh√¥ng kh·∫£ d·ª•ng - ch·∫°y trong vector-only mode")

# C·∫•u h√¨nh logging chi ti·∫øt
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Console output
    ]
)
logger = logging.getLogger(__name__)

class APIKeyManager:
    """Qu·∫£n l√Ω v√† xoay v√≤ng API keys ƒë·ªÉ tr√°nh quota exceeded"""
    
    def __init__(self, api_keys: List[str]):
        self.api_keys = [key for key in api_keys if key]  # Filter out empty keys
        self.current_index = 0
        self.failed_keys = set()  # Track keys that are temporarily failed
        self.last_error_time = {}  # Track when each key last failed
        self.retry_delay = 60  # Wait 60 seconds before retrying a failed key
        
        if not self.api_keys:
            raise ValueError("C·∫ßn √≠t nh·∫•t m·ªôt API key h·ª£p l·ªá")
        
        logger.info(f"‚úì APIKeyManager kh·ªüi t·∫°o v·ªõi {len(self.api_keys)} API keys")
        
    def get_current_key(self) -> str:
        """L·∫•y API key hi·ªán t·∫°i"""
        return self.api_keys[self.current_index]
        
    def get_current_model(self):
        """L·∫•y Gemini model v·ªõi API key hi·ªán t·∫°i"""
        try:
            current_key = self.get_current_key()
            genai.configure(api_key=current_key)
            return genai.GenerativeModel('gemini-1.5-flash')
        except Exception as e:
            logger.error(f"L·ªói t·∫°o model v·ªõi key {self.current_index + 1}: {e}")
            return None
    
    def rotate_key(self, error_message: str = None) -> bool:
        """
        Xoay sang API key ti·∫øp theo
        Returns: True n·∫øu c√≤n key kh·∫£ d·ª•ng, False n·∫øu h·∫øt key
        """
        # Mark current key as failed
        current_key_index = self.current_index
        self.failed_keys.add(current_key_index)
        self.last_error_time[current_key_index] = time.time()
        
        logger.warning(f"API Key {current_key_index + 1} failed: {error_message}")
        
        # Try to find next available key
        attempts = 0
        while attempts < len(self.api_keys):
            self.current_index = (self.current_index + 1) % len(self.api_keys)
            
            # Check if this key is available
            if self._is_key_available(self.current_index):
                logger.info(f"Chuy·ªÉn sang API Key {self.current_index + 1}")
                return True
                
            attempts += 1
        
        # No available keys
        logger.error("T·∫•t c·∫£ API keys ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")
        return False
    
    def _is_key_available(self, key_index: int) -> bool:
        """Ki·ªÉm tra xem API key c√≥ kh·∫£ d·ª•ng kh√¥ng"""
        if key_index not in self.failed_keys:
            return True
            
        # Check if enough time has passed since last failure
        if key_index in self.last_error_time:
            time_since_failure = time.time() - self.last_error_time[key_index]
            if time_since_failure > self.retry_delay:
                # Remove from failed set to retry
                self.failed_keys.discard(key_index)
                logger.info(f"API Key {key_index + 1} s·∫µn s√†ng th·ª≠ l·∫°i sau {time_since_failure:.1f}s")
                return True
        
        return False
    
    def reset_failed_keys(self):
        """Reset t·∫•t c·∫£ failed keys - d√πng ƒë·ªÉ force retry"""
        self.failed_keys.clear()
        self.last_error_time.clear()
        logger.info("ƒê√£ reset t·∫•t c·∫£ failed API keys")
    
    def get_status(self) -> Dict[str, Any]:
        """L·∫•y tr·∫°ng th√°i c·ªßa API key manager"""
        return {
            'total_keys': len(self.api_keys),
            'current_index': self.current_index,
            'current_key_suffix': self.get_current_key()[-10:] if self.api_keys else 'N/A',
            'failed_keys': list(self.failed_keys),
            'available_keys': [i for i in range(len(self.api_keys)) if self._is_key_available(i)]
        }
    
    def call_with_rotation(self, func, *args, **kwargs):
        """
        G·ªçi function v·ªõi auto rotation khi g·∫∑p quota error
        """
        max_attempts = len(self.api_keys)
        attempt = 0
        
        while attempt < max_attempts:
            try:
                # Get current model and try the function
                model = self.get_current_model()
                if not model:
                    if not self.rotate_key("Failed to create model"):
                        break
                    attempt += 1
                    continue
                
                # Call function with current model
                if hasattr(func, '__self__'):
                    # Method call - update the model reference
                    func.__self__.gemini_model = model
                
                return func(*args, **kwargs)
                
            except Exception as e:
                error_str = str(e)
                
                # Check if it's a quota error
                if '429' in error_str or 'quota' in error_str.lower() or 'exceeded' in error_str.lower():
                    logger.warning(f"Quota exceeded with API Key {self.current_index + 1}, attempting rotation...")
                    
                    if not self.rotate_key(error_str):
                        # No more keys available
                        logger.error("T·∫•t c·∫£ API keys ƒë√£ h·∫øt quota")
                        raise e
                    
                    attempt += 1
                    continue
                else:
                    # Non-quota error, don't rotate
                    raise e
        
        # If we get here, all attempts failed
        raise Exception("T·∫•t c·∫£ API keys ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng")

@dataclass
class SearchResult:
    """K·∫øt qu·∫£ t√¨m ki·∫øm v·ªõi metadata phong ph√∫"""
    content: str
    score: float
    subject_code: str
    document_type: str
    level: str  # 'summary', 'chunk', 'detail'
    metadata: Dict[str, Any]

@dataclass
class QueryIntent:
    """Ph√¢n t√≠ch √Ω ƒë·ªãnh truy v·∫•n"""
    query_type: str  # 'factual', 'analytical', 'comparative', 'listing'
    subject_scope: str  # 'single', 'multiple', 'all'
    complexity: str  # 'simple', 'medium', 'complex'
    requires_summarization: bool
    target_subjects: List[str]

@dataclass
class FollowupQuery:
    """Truy v·∫•n ti·∫øp theo ƒë∆∞·ª£c ph√°t hi·ªán t·ª´ c√¢u tr·∫£ l·ªùi"""
    query: str
    confidence: float
    query_type: str  # 'prerequisite', 'related_subject', 'detail_expansion'
    source_info: str  # Th√¥ng tin ngu·ªìn g·ªëc t·ª´ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc

@dataclass
class QueryChainResult:
    """K·∫øt qu·∫£ c·ªßa chu·ªói truy v·∫•n ƒëa c·∫•p"""
    original_query: str
    original_answer: str
    followup_queries: List[FollowupQuery]
    followup_results: List[Dict[str, Any]]
    final_integrated_answer: str
    execution_path: List[str]

@dataclass
class ProcessedQuery:
    """Query ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi LLM"""
    original_query: str
    processed_query: str
    intent_description: str
    suggested_keywords: List[str]
    confidence: float
    needs_data_search: bool

class QueryPreprocessor:
    """X·ª≠ l√Ω query b·∫±ng LLM tr∆∞·ªõc khi ƒë∆∞a v√†o h·ªá th·ªëng ch√≠nh"""
    
    def __init__(self, gemini_model, rag_engine=None):
        self.gemini_model = gemini_model
        self.rag_engine = rag_engine  # Reference to parent engine for rotation
        
        # Domain-specific knowledge v·ªÅ FPTU
        self.fptu_domain_knowledge = {
            'combo_terms': [
                'combo', 'combo chuy√™n ng√†nh', 'chuy√™n ng√†nh h·∫πp', 'specialization', 
                'track', 'specialization track', 'major track'
            ],
            'semester_terms': [
                'k√¨', 'ky', 'k·ª≥', 'ki', 'semester', 'h·ªçc k√¨', 'hoc ky', 'term'
            ],
            'quantity_terms': [
                'bao nhi√™u', 'c√≥ m·∫•y', 's·ªë l∆∞·ª£ng', 't·ªïng c·ªông', 'how many', 'count'
            ],
            'definition_terms': [
                'l√† g√¨', 'la gi', 'what is', 'what are', 'ƒë·ªãnh nghƒ©a', 'dinh nghia', 'nghƒ©a l√†'
            ]
        }
        
    def preprocess_query(self, query: str, conversation_context: str = "") -> ProcessedQuery:
        """X·ª≠ l√Ω query b·∫±ng LLM ƒë·ªÉ c·∫£i thi·ªán hi·ªÉu √Ω ƒë·ªãnh"""
        
        try:
            # T·∫°o prompt cho LLM
            prompt = self._build_preprocessing_prompt(query, conversation_context)
            
            # G·ªçi LLM v·ªõi rotation n·∫øu c√≥ reference t·ªõi engine
            if self.rag_engine and hasattr(self.rag_engine, '_call_gemini_with_rotation'):
                result_text = self.rag_engine._call_gemini_with_rotation(prompt)
            else:
                # Fallback to direct call
                response = self.gemini_model.generate_content(prompt)
                result_text = response.text.strip()
            
            # Parse k·∫øt qu·∫£
            parsed_result = self._parse_llm_response(result_text, query)
            
            logger.info(f"QUERY PREPROCESSING:")
            logger.info(f"  Original: '{query}'")
            logger.info(f"  Processed: '{parsed_result.processed_query}'")
            logger.info(f"  Intent: {parsed_result.intent_description}")
            logger.info(f"  Keywords: {parsed_result.suggested_keywords}")
            logger.info(f"  Needs data search: {parsed_result.needs_data_search}")
            
            return parsed_result
            
        except Exception as e:
            logger.error(f"L·ªói preprocessing query: {e}")
            # Fallback - tr·∫£ v·ªÅ query g·ªëc
            return ProcessedQuery(
                original_query=query,
                processed_query=query,
                intent_description="unknown",
                suggested_keywords=[],
                confidence=0.5,
                needs_data_search=True
            )
    
    def _build_preprocessing_prompt(self, query: str, conversation_context: str) -> str:
        """T·∫°o prompt cho LLM preprocessing"""
        
        context_part = ""
        if conversation_context:
            context_part = f"\nL·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán:\n{conversation_context}\n"
        
        return f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch query cho h·ªá th·ªëng th√¥ng tin h·ªçc t·∫≠p FPT University. 
Nhi·ªám v·ª•: Ph√¢n t√≠ch v√† c·∫£i thi·ªán query c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ t√¨m ki·∫øm hi·ªáu qu·∫£ h∆°n.

DOMAIN KNOWLEDGE - FPT UNIVERSITY:
- Ng√†nh AI c√≥ 45 m√¥n h·ªçc, ph√¢n b·ªë theo 8 k√¨
- C√≥ 3 combo chuy√™n ng√†nh h·∫πp: AI17_COM1 (Data Science), AI17_COM2.1 (Text Mining), AI17_COM3 (AI Healthcare)
- M·ªói m√¥n c√≥ m√£ (vd: CSI106, AIG202c), t√™n ti·∫øng Vi·ªát/Anh, s·ªë t√≠n ch·ªâ, k√¨ h·ªçc
- Thu·∫≠t ng·ªØ "combo chuy√™n ng√†nh h·∫πp" = "specialization track"

{context_part}

QUERY C·∫¶N PH√ÇN T√çCH: "{query}"

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng JSON:
{{
  "processed_query": "[Query ƒë∆∞·ª£c c·∫£i thi·ªán ƒë·ªÉ t√¨m ki·∫øm t·ªët h∆°n]",
  "intent_description": "[M√¥ t·∫£ √Ω ƒë·ªãnh: factual/listing/definition/counting/etc.]",
  "suggested_keywords": ["keyword1", "keyword2", "keyword3"],
  "confidence": [0.0-1.0],
  "needs_data_search": [true/false],
  "reasoning": "[Gi·∫£i th√≠ch logic ph√¢n t√≠ch]"
}}

H∆Ø·ªöNG D·∫™N C·ª§TH·ªÇ:
1. N·∫øu h·ªèi "bao nhi√™u combo/chuy√™n ng√†nh" ‚Üí processed_query: "li·ªát k√™ t·∫•t c·∫£ combo chuy√™n ng√†nh h·∫πp AI"
2. N·∫øu h·ªèi "combo/chuy√™n ng√†nh l√† g√¨" ‚Üí processed_query: "th√¥ng tin v·ªÅ combo chuy√™n ng√†nh h·∫πp ng√†nh AI"
3. N·∫øu h·ªèi v·ªÅ k√¨ h·ªçc c·ª• th·ªÉ ‚Üí th√™m keywords v·ªÅ semester
4. N·∫øu ch·ªâ ch√†o h·ªèi/c·∫£m ∆°n ‚Üí needs_data_search: false

Tr·∫£ v·ªÅ JSON h·ª£p l·ªá:"""

    def _parse_llm_response(self, response_text: str, original_query: str) -> ProcessedQuery:
        """Parse ph·∫£n h·ªìi t·ª´ LLM"""
        
        try:
            # T√¨m JSON trong response
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed = json.loads(json_str)
                
                return ProcessedQuery(
                    original_query=original_query,
                    processed_query=parsed.get('processed_query', original_query),
                    intent_description=parsed.get('intent_description', 'unknown'),
                    suggested_keywords=parsed.get('suggested_keywords', []),
                    confidence=float(parsed.get('confidence', 0.7)),
                    needs_data_search=bool(parsed.get('needs_data_search', True))
                )
            else:
                # Kh√¥ng parse ƒë∆∞·ª£c JSON, fallback
                logger.warning("Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ LLM response")
                return self._fallback_processing(original_query)
                
        except Exception as e:
            logger.error(f"L·ªói parse LLM response: {e}")
            return self._fallback_processing(original_query)
    
    def _fallback_processing(self, query: str) -> ProcessedQuery:
        """Fallback processing khi LLM th·∫•t b·∫°i - Enhanced version"""
        
        query_lower = query.lower()
        
        # ENHANCED RULE-BASED PROCESSING
        
        # 1. SEMESTER RELATIONSHIP QUERIES
        if ('k√¨' in query_lower or 'ky' in query_lower or 'k·ª≥' in query_lower) and ('li√™n quan' in query_lower or 'lien quan' in query_lower):
            return ProcessedQuery(
                original_query=query,
                processed_query=f"m√¥n h·ªçc k√¨ 4 k√¨ 5 m·ªëi li√™n h·ªá ti√™n quy·∫øt ph·ª• thu·ªôc {query}",
                intent_description="semester_relationship_analysis",
                suggested_keywords=['k√¨ 4', 'k√¨ 5', 'm√¥n h·ªçc', 'li√™n quan', 'ti√™n quy·∫øt', 'prerequisite'],
                confidence=0.85,
                needs_data_search=True
            )
        
        # 2. COMBO COUNTING QUERIES
        if any(term in query_lower for term in ['bao nhi√™u', 'c√≥ m·∫•y', 's·ªë l∆∞·ª£ng']):
            if any(term in query_lower for term in ['combo', 'chuy√™n ng√†nh']):
                return ProcessedQuery(
                    original_query=query,
                    processed_query="li·ªát k√™ t·∫•t c·∫£ combo chuy√™n ng√†nh h·∫πp AI specialization track",
                    intent_description="counting_combo",
                    suggested_keywords=['combo', 'chuy√™n ng√†nh h·∫πp', 'AI', 'specialization', 'track'],
                    confidence=0.9,
                    needs_data_search=True
                )
        
        # 3. COMBO DEFINITION QUERIES  
        if any(term in query_lower for term in ['l√† g√¨', 'what is', 'ƒë·ªãnh nghƒ©a']):
            if any(term in query_lower for term in ['combo', 'chuy√™n ng√†nh']):
                return ProcessedQuery(
                    original_query=query,
                    processed_query="th√¥ng tin ƒë·ªãnh nghƒ©a combo chuy√™n ng√†nh h·∫πp ng√†nh AI specialization track",
                    intent_description="definition_combo",
                    suggested_keywords=['combo', 'chuy√™n ng√†nh h·∫πp', 'th√¥ng tin', 'specialization'],
                    confidence=0.9,
                    needs_data_search=True
                )
        
        # 4. SEMESTER LISTING QUERIES
        semester_patterns = ['k√¨ 1', 'k√¨ 2', 'k√¨ 3', 'k√¨ 4', 'k√¨ 5', 'k√¨ 6', 'k√¨ 7', 'k√¨ 8',
                           'ky 1', 'ky 2', 'ky 3', 'ky 4', 'ky 5', 'ky 6', 'ky 7', 'ky 8']
        if any(pattern in query_lower for pattern in semester_patterns):
            # Extract semester numbers
            semesters = []
            for i in range(1, 9):
                if f'k√¨ {i}' in query_lower or f'ky {i}' in query_lower or f'k·ª≥ {i}' in query_lower:
                    semesters.append(str(i))
            
            if semesters:
                semester_text = ' '.join([f'k√¨ {s}' for s in semesters])
                return ProcessedQuery(
                    original_query=query,
                    processed_query=f"m√¥n h·ªçc {semester_text} semester {' '.join(semesters)} curriculum subjects",
                    intent_description="semester_listing",
                    suggested_keywords=['m√¥n h·ªçc'] + [f'k√¨ {s}' for s in semesters] + ['semester', 'curriculum'],
                    confidence=0.85,
                    needs_data_search=True
                )
        
        # 5. GREETINGS AND THANKS - SHOULD BE DIRECT CHAT
        greeting_patterns = [
            'xin ch√†o', 'hello', 'hi', 'ch√†o b·∫°n', 'chao ban',
            'c·∫£m ∆°n', 'cam on', 'thank you', 'thanks', 'c√°m ∆°n'
        ]
        if any(pattern in query_lower for pattern in greeting_patterns):
            return ProcessedQuery(
                original_query=query,
                processed_query=query,
                intent_description="greeting_or_thanks",
                suggested_keywords=[],
                confidence=0.95,
                needs_data_search=False  # IMPORTANT: Direct chat
            )
        
        # 6. SUBJECT CODE QUERIES
        subject_codes = re.findall(r'[A-Za-z]{2,4}\d{3}[a-zA-Z]*', query)
        if subject_codes:
            return ProcessedQuery(
                original_query=query,
                processed_query=f"{query} subject code course information",
                intent_description="subject_specific",
                suggested_keywords=subject_codes + ['m√¥n h·ªçc', 'course', 'subject'],
                confidence=0.8,
                needs_data_search=True
            )
        
        # 7. GENERAL ACADEMIC QUERIES
        academic_terms = ['m√¥n h·ªçc', 'mon hoc', 'subject', 'course', 'curriculum', 'ch∆∞∆°ng tr√¨nh', 'chuong trinh']
        if any(term in query_lower for term in academic_terms):
            return ProcessedQuery(
                original_query=query,
                processed_query=f"{query} academic curriculum course information",
                intent_description="general_academic",
                suggested_keywords=['m√¥n h·ªçc', 'curriculum', 'academic'],
                confidence=0.7,
                needs_data_search=True
            )
        
        # 8. DEFAULT FALLBACK
        return ProcessedQuery(
            original_query=query,
            processed_query=query,
            intent_description="unknown",
            suggested_keywords=[],
            confidence=0.5,
            needs_data_search=True  # Default to data search unless clearly conversational
        )

class QueryRouter:
    """Router th√¥ng minh ƒë·ªÉ ƒë·ªãnh tuy·∫øn query"""
    
    def __init__(self):
        # Quick response patterns for basic queries
        self.quick_response_patterns = {
            'greeting': {
                'patterns': [
                    r'xin ch√†o', r'hello', r'hi', r'ch√†o b·∫°n', r'ch√†o',
                    r'good morning', r'good afternoon', r'good evening'
                ],
                'response': "Xin ch√†o! T√¥i l√† AI Assistant c·ªßa FPTU. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m ki·∫øm th√¥ng tin v·ªÅ c√°c m√¥n h·ªçc, syllabus v√† ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o. H√£y ƒë·∫∑t c√¢u h·ªèi cho t√¥i!"
            },
            'identity': {
                'patterns': [
                    r'b·∫°n l√† ai', r'who are you', r'b·∫°n l√† g√¨', r'what are you',
                    r'gi·ªõi thi·ªáu v·ªÅ b·∫°n', r'tell me about yourself'
                ],
                'response': "T√¥i l√† AI Assistant c·ªßa FPTU - h·ªá th·ªëng h·ªó tr·ª£ t√¨m ki·∫øm th√¥ng tin v·ªÅ ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o v√† m√¥n h·ªçc t·∫°i FPT University. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n:\n\n‚Ä¢ T√¨m ki·∫øm th√¥ng tin m√¥n h·ªçc theo m√£ (VD: CSI106, SEG301)\n‚Ä¢ Li·ªát k√™ c√°c m√¥n h·ªçc theo ng√†nh\n‚Ä¢ T√¨m hi·ªÉu v·ªÅ syllabus, CLO, t√†i li·ªáu h·ªçc t·∫≠p\n‚Ä¢ Tra c·ª©u th√¥ng tin v·ªÅ m√¥n ti√™n quy·∫øt\n\nH√£y th·ª≠ h·ªèi t√¥i v·ªÅ b·∫•t k·ª≥ m√¥n h·ªçc n√†o b·∫°n quan t√¢m!"
            },
            'help': {
                'patterns': [
                    r'help', r'gi√∫p ƒë·ª°', r'h∆∞·ªõng d·∫´n', r'l√†m g√¨', r'what can you do',
                    r'b·∫°n c√≥ th·ªÉ l√†m g√¨', r't√¥i c√≥ th·ªÉ h·ªèi g√¨'
                ],
                'response': "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n:\n\n**T√¨m ki·∫øm m√¥n h·ªçc:**\n‚Ä¢ Theo m√£ m√¥n: 'CSI106 l√† m√¥n g√¨?'\n‚Ä¢ Theo t√™n m√¥n: 'Machine Learning l√† m√¥n g√¨?'\n‚Ä¢ Theo ng√†nh: 'Li·ªát k√™ c√°c m√¥n h·ªçc ng√†nh AI'\n\n**Th√¥ng tin chi ti·∫øt:**\n‚Ä¢ Syllabus v√† CLO\n‚Ä¢ T√†i li·ªáu h·ªçc t·∫≠p\n‚Ä¢ Ph∆∞∆°ng th·ª©c ƒë√°nh gi√°\n‚Ä¢ M√¥n ti√™n quy·∫øt\n\n**C√¢u h·ªèi m·∫´u:**\n‚Ä¢ 'C√°c m√¥n c√≥ 3 t√≠n ch·ªâ'\n‚Ä¢ 'Danh s√°ch m√¥n h·ªçc k·ª≥ 1'\n‚Ä¢ 'SEG301 v√† c√°c m√¥n ti√™n quy·∫øt'"
            },
            'thanks': {
                'patterns': [
                    r'c·∫£m ∆°n', r'thank you', r'thanks', r'c√°m ∆°n', r'c·∫£m ∆°n b·∫°n'
                ],
                'response': "R·∫•t vui ƒë∆∞·ª£c gi√∫p ƒë·ª° b·∫°n! N·∫øu b·∫°n c√≥ th√™m c√¢u h·ªèi n√†o v·ªÅ ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ho·∫∑c m√¥n h·ªçc t·∫°i FPTU, ƒë·ª´ng ng·∫°i h·ªèi t√¥i nh√©!"
            }
        }
        
        self.factual_patterns = [
            r'(.*?) l√† g√¨',
            r'ƒë·ªãnh nghƒ©a (.*?)',
            r'(.*?) c√≥ (bao nhi√™u|m·∫•y) t√≠n ch·ªâ',
            r'ai l√† gi·∫£ng vi√™n (.*?)',
            r'(.*?) thu·ªôc k·ª≥ (.*?)'
        ]
        
        self.listing_patterns = [
            r'li·ªát k√™ (.*?)',
            r't·∫•t c·∫£ (.*?) m√¥n',
            r'c√°c m√¥n (.*?)',
            r'danh s√°ch (.*?)',
            r'cho t√¥i bi·∫øt (.*?) m√¥n'
        ]
        
        self.comparative_patterns = [
            r'so s√°nh (.*?) v√† (.*?)',
            r'kh√°c nhau gi·ªØa (.*?) v√† (.*?)',
            r'(.*?) hay (.*?) t·ªët h∆°n'
        ]
        
        self.analytical_patterns = [
            r't·∫°i sao (.*?)',
            r'ph√¢n t√≠ch (.*?)',
            r'ƒë√°nh gi√° (.*?)',
            r'∆∞u nh∆∞·ª£c ƒëi·ªÉm (.*?)'
        ]

    def check_quick_response(self, query: str) -> Optional[str]:
        """Check if query matches quick response patterns"""
        query_lower = query.lower().strip()
        
        # Skip if query looks like subject search (contains technical terms)
        # But be more careful with short terms that might be in normal conversation
        technical_indicators = [
            'machine learning', 'artificial intelligence', 'data science',
            'programming', 'algorithm', 'database', 'network', 'security',
            'software', 'hardware', 'computer science', 'engineering',
            'mathematics', 'statistics', 'physics', 'chemistry'
        ]
        
        # Check for longer technical terms first
        for indicator in technical_indicators:
            if indicator in query_lower:
                return None
        
        # For short terms, be more restrictive - only block if they appear as isolated words
        # BUT exclude common conversational contexts
        short_technical_terms = ['ml', 'cs', 'se', 'it']  # Remove 'ai' as it's too common in Vietnamese
        words = query_lower.split()
        
        # Special handling for 'ai' - only block if it's clearly technical context
        if 'ai' in words:
            # Don't block if it's in conversational context
            conversational_ai_contexts = [
                'b·∫°n l√† ai', 'ai l√†', 'ai ƒë√≥', 'ai ƒëang', 'ai s·∫Ω', 
                'cho ai', 'v·ªõi ai', 'c·ªßa ai', 'ai c√≥', 'ai c·∫ßn'
            ]
            is_conversational = any(ctx in query_lower for ctx in conversational_ai_contexts)
            
            if not is_conversational:
                # Check if it's technical AI context
                if any(tech in query_lower for tech in ['artificial', 'intelligence', 'machine', 'learning', 'deep']):
                    return None
        
        for term in short_technical_terms:
            if term in words:  # Only if it's a separate word, not part of another word
                return None
        
        # Skip if query contains subject codes (full or partial)
        if re.search(r'[A-Z]{2,4}\d{3}[a-z]*', query):
            return None
            
        # Also skip if query contains likely subject code patterns
        # Look for common subject code prefixes that appear in academic context
        academic_context_indicators = [
            'm√¥n', 'mon', 'h·ªçc', 'hoc', 't√≠n ch·ªâ', 'tin chi', 'ti√™n quy·∫øt', 'tien quyet',
            'syllabus', 'course', 'subject', 'credit', 'prerequisite', 'th√¥ng tin', 'thong tin',
            'chi ti·∫øt', 'chi tiet', 'n·ªôi dung', 'noi dung',
            # COMBO/SPECIALIZATION INDICATORS
            'combo', 'chuy√™n ng√†nh', 'chuyen nganh', 'specialization', 'track',
            'chuy√™n ng√†nh h·∫πp', 'chuyen nganh hep', 'combo chuy√™n ng√†nh', 'combo chuyen nganh',
            # COUNTING/QUANTITY INDICATORS
            'bao nhi√™u', 'bao nhieu', 'c√≥ m·∫•y', 'co may', 's·ªë l∆∞·ª£ng', 'so luong',
            't·ªïng c·ªông', 'tong cong', 'how many', 'count', 'list', 'li·ªát k√™', 'liet ke'
        ]
        
        # Only skip if query contains both academic context AND potential subject codes
        has_academic_context = any(indicator in query_lower for indicator in academic_context_indicators)
        
        if has_academic_context:
            words = query.upper().split()
            excluded_words = {
                'M√îN', 'MON', 'H·ªåC', 'HOC', 'C√ì', 'CO', 'KH√îNG', 'KHONG', 
                'G√å', 'GI', 'L√Ä', 'LA', 'N√ÄO', 'NAO', 'TI√äN', 'TIEN', 
                'QUY·∫æT', 'QUYET', 'ƒêI·ªÄU', 'DIEU', 'KI·ªÜN', 'KIEN',
                'COURSE', 'SUBJECT', 'WHAT', 'IS', 'ARE', 'THE', 'HAVE', 'HAS',
                'V√Ä', 'VA', 'C·ª¶A', 'CUA', 'TH√îNG', 'THONG', 'TIN', 'CHI', 'TI·∫æT', 'TIET',
                'C√ÅC', 'CAC', 'N·ªòI', 'NOI', 'DUNG', 'D·ª§NG', 'B·∫∞NG', 'BANG'
            }
            
            for word in words:
                if (len(word) >= 3 and len(word) <= 6 and word.isalpha() 
                    and word not in excluded_words):
                    # This looks like a subject code in academic context
                    return None
        
        for category, data in self.quick_response_patterns.items():
            for pattern in data['patterns']:
                if re.search(pattern, query_lower, re.IGNORECASE):
                    return data['response']
        
        return None

    def analyze_query(self, query: str) -> QueryIntent:
        """Ph√¢n t√≠ch √Ω ƒë·ªãnh v√† ph·∫°m vi c·ªßa query"""
        query_lower = query.lower()
        
        # Extract subject codes and combo codes for later use (CASE INSENSITIVE)
        subject_codes = re.findall(r'[A-Za-z]{2,4}\d{3}[a-zA-Z]*', query, re.IGNORECASE)
        # Normalize to uppercase
        subject_codes = [code.upper() for code in subject_codes]
        
        # ENHANCED: Extract combo codes (like AI17_COM1, AI17_COM2.1)
        combo_codes = re.findall(r'[A-Za-z]{2,4}\d{2}_[A-Za-z]{3}\d+(?:\.\d+)?', query, re.IGNORECASE)
        # Normalize to uppercase
        combo_codes = [code.upper() for code in combo_codes]
        
        # Combine subject and combo codes
        all_codes = subject_codes + combo_codes
        
        # ENHANCED: Extract partial subject codes (like "DPL" -> "DPL302m")
        if not all_codes:
            # Look for partial subject codes (3+ characters)
            words = query.upper().split()
            for word in words:
                # Only consider words that could be subject codes
                # - 3+ characters, alphabetic
                # - Not common Vietnamese/English words
                excluded_words = {
                    'M√îN', 'MON', 'H·ªåC', 'HOC', 'C√ì', 'CO', 'KH√îNG', 'KHONG', 
                    'G√å', 'GI', 'L√Ä', 'LA', 'N√ÄO', 'NAO', 'TI√äN', 'TIEN', 
                    'QUY·∫æT', 'QUYET', 'ƒêI·ªÄU', 'DIEU', 'KI·ªÜN', 'KIEN',
                    'COURSE', 'SUBJECT', 'WHAT', 'IS', 'ARE', 'THE', 'HAVE', 'HAS',
                    'V√Ä', 'VA', 'C·ª¶A', 'CUA', 'TH√îNG', 'THONG', 'TIN', 'CHI', 'TI·∫æT', 'TIET',
                    'C√ÅC', 'CAC', 'N·ªòI', 'NOI', 'DUNG', 'D·ª§NG', 'B·∫∞NG', 'BANG'
                }
                
                # Only add words that look like actual subject codes (3-4 chars, often with consonants)
                if (len(word) >= 3 and len(word) <= 4 and word.isalpha() 
                    and word not in excluded_words
                    and not word.endswith('NG')):  # Avoid words like "THONG", "DUNG"
                    # This could be a partial subject code
                    all_codes.append(word)
        
        # IMPROVEMENT: Also extract partial codes from full subject codes to enable broader search
        # This helps when specific subject code search fails but partial search succeeds
        partial_codes_from_full = []
        for full_code in subject_codes:
            # Extract the prefix (e.g., "DPL" from "DPL302m")
            prefix_match = re.match(r'([A-Z]{2,4})', full_code)
            if prefix_match:
                prefix = prefix_match.group(1)
                if prefix not in all_codes and len(prefix) >= 3:
                    partial_codes_from_full.append(prefix)
        
        # Add partial codes as fallback option
        all_codes.extend(partial_codes_from_full)
        
        # PRIORITY 1: SEMESTER/TERM QUERIES - Always treat as LISTING
        if any(term in query_lower for term in ['k·ª≥', 'ky', 'k√¨', 'ki', 'semester', 'h·ªçc k·ª≥', 'hoc ky']):
            # Check if asking for subjects in a semester
            if any(term in query_lower for term in ['m√¥n', 'mon', 'subject', 'c√≥ g√¨', 'co gi', 'g·ªìm c√≥', 'gom co']):
                return QueryIntent(
                    query_type='listing',
                    subject_scope='multiple',
                    complexity='medium',
                    requires_summarization=True,
                    target_subjects=all_codes
                )
        
        # PRIORITY 2: STUDENT QUERIES (Higher priority than general listing)
        student_indicators = [
            'sinh vi√™n', 'sinh vien', 'student', 'h·ªçc sinh', 'hoc sinh', 
            'danh s√°ch sinh vi√™n', 'm√£ sinh vi√™n', 'ma sinh vien',
            'danh s√°ch sinh vien', 'ma sinh vien',
            'hoc sinh', 'sv ', ' sv', 'students'
        ]
        
        # Also check for combined patterns like "sinh vi√™n ng√†nh", "sinh vi√™n AI"
        if (any(term in query_lower for term in student_indicators) or 
            ('sinh' in query_lower and 'vi√™n' in query_lower) or
            ('sinh' in query_lower and 'vien' in query_lower)):
            # Determine if listing all students or specific student
            if any(pattern in query_lower for pattern in ['danh s√°ch', 'danh sach', 'list', 't·∫•t c·∫£', 'tat ca', 'c√°c sinh vi√™n', 'cac sinh vien']):
                return QueryIntent(
                    query_type='listing',
                    subject_scope='multiple',
                    complexity='medium',
                    requires_summarization=True,
                    target_subjects=[]  # No specific subjects for student listing
                )
            else:
                # Specific student query (usually contains roll number)
                # Extract roll numbers if present
                roll_numbers = re.findall(r'[A-Z]{2}\d{6}', query.upper())
                return QueryIntent(
                    query_type='factual',
                    subject_scope='single',
                    complexity='simple',
                    requires_summarization=False,
                    target_subjects=roll_numbers  # Use roll numbers instead of partial words
                )

        # PRIORITY 3: EXPLICIT LISTING QUERIES (for non-student content)
        if any(pattern in query_lower for pattern in ['li·ªát k√™', 'liet ke', 'danh s√°ch', 'danh sach', 'list', 't·∫•t c·∫£', 'tat ca', 'c√°c m√¥n', 'cac mon']):
            complexity = 'complex' if any(term in query_lower for term in ['ph√¢n t√≠ch', 'so s√°nh', 'compare']) else 'medium'
            return QueryIntent(
                query_type='listing',
                subject_scope='multiple' if not all_codes else 'single',
                complexity=complexity,
                requires_summarization=True,
                target_subjects=all_codes
            )
        
        # PRIORITY 4: COMPARATIVE QUERIES
        if any(pattern in query_lower for pattern in ['so s√°nh', 'so sanh', 'compare', 'kh√°c nhau', 'khac nhau', 'gi·ªëng', 'giong']):
            return QueryIntent(
                query_type='comparative',
                subject_scope='multiple',
                complexity='complex',
                requires_summarization=True,
                target_subjects=all_codes
            )
        
        # PRIORITY 5: ANALYTICAL QUERIES
        if any(pattern in query_lower for pattern in ['ph√¢n t√≠ch', 'phan tich', 'analyze', 'ƒë√°nh gi√°', 'danh gia', 'evaluate', 'l·ªô tr√¨nh', 'lo trinh', 'roadmap']):
            return QueryIntent(
                query_type='analytical',
                subject_scope='multiple',
                complexity='complex',
                requires_summarization=True,
                target_subjects=all_codes
            )
        
        # DEFAULT: FACTUAL QUERIES
        if all_codes:
            scope = 'single'
            complexity = 'simple'
        elif any(term in query_lower for term in ['ng√†nh', 'nganh', 'major', 'ch∆∞∆°ng tr√¨nh', 'chuong trinh']):
            scope = 'multiple'
            complexity = 'medium'
        else:
            scope = 'single'
            complexity = 'simple'
        
        return QueryIntent(
            query_type='factual',
            subject_scope=scope,
            complexity=complexity,
            requires_summarization=False,
            target_subjects=all_codes
        )

class HierarchicalIndex:
    """H·ªá th·ªëng indexing ph√¢n c·∫•p"""
    
    def __init__(self, embedding_model: SentenceTransformer):
        self.embedding_model = embedding_model
        self.summary_index: Optional[faiss.Index] = None
        self.chunk_index: Optional[faiss.Index] = None
        self.detail_index: Optional[faiss.Index] = None
        
        self.summary_data: List[Dict] = []
        self.chunk_data: List[Dict] = []
        self.detail_data: List[Dict] = []
        
        # Metadata mapping
        self.subject_summary: Dict[str, Dict] = {}
        self.document_hierarchy: Dict[str, List[str]] = {}

    def build_hierarchy(self, data: List[Dict], gemini_model):
        """X√¢y d·ª±ng c·∫•u tr√∫c ph√¢n c·∫•p"""
        logger.info("B·∫Øt ƒë·∫ßu x√¢y d·ª±ng hierarchical index...")
        
        # Level 1: Subject Summaries
        self._build_subject_summaries(data, gemini_model)
        
        # Level 2: Document Chunks
        self._build_document_chunks(data)
        
        # Level 3: Detail Chunks
        self._build_detail_chunks(data)
        
        # Build FAISS indices
        self._build_faiss_indices()
        
        logger.info("Ho√†n th√†nh x√¢y d·ª±ng hierarchical index")

    def _build_subject_summaries(self, data: List[Dict], gemini_model):
        """X√¢y d·ª±ng t√≥m t·∫Øt c·∫•p m√¥n h·ªçc"""
        subjects = defaultdict(list)
        
        # Nh√≥m theo m√¥n h·ªçc
        for item in data:
            subject_code = item.get('metadata', {}).get('subject_code', 'UNKNOWN')
            subjects[subject_code].append(item)
        
        for subject_code, items in subjects.items():
            # T·∫°o t√≥m t·∫Øt to√†n di·ªán cho m√¥n h·ªçc
            combined_content = self._combine_subject_content(items)
            summary = self._generate_summary(combined_content, gemini_model, 'subject')
            
            subject_data = {
                'content': summary,
                'subject_code': subject_code,
                'level': 'summary',
                'item_count': len(items),
                'metadata': self._extract_subject_metadata(items)
            }
            
            self.summary_data.append(subject_data)
            self.subject_summary[subject_code] = subject_data

    def _build_document_chunks(self, data: List[Dict]):
        """X√¢y d·ª±ng chunks c·∫•p t√†i li·ªáu"""
        documents = defaultdict(list)
        
        # Nh√≥m theo lo·∫°i t√†i li·ªáu
        for item in data:
            doc_type = item.get('type', 'general')
            subject_code = item.get('metadata', {}).get('subject_code', 'UNKNOWN')
            key = f"{subject_code}_{doc_type}"
            documents[key].append(item)
        
        for doc_key, items in documents.items():
            if len(items) > 1:  # Ch·ªâ t·∫°o chunk n·∫øu c√≥ nhi·ªÅu items
                combined_content = self._combine_document_content(items)
                
                chunk_data = {
                    'content': combined_content[:2000],  # Limit length
                    'document_key': doc_key,
                    'level': 'chunk',
                    'item_count': len(items),
                    'metadata': items[0].get('metadata', {})
                }
                
                self.chunk_data.append(chunk_data)

    def _build_detail_chunks(self, data: List[Dict]):
        """X√¢y d·ª±ng chunks chi ti·∫øt"""
        for item in data:
            detail_data = {
                'content': item.get('content', ''),
                'level': 'detail',
                'original_data': item,
                'metadata': item.get('metadata', {})
            }
            self.detail_data.append(detail_data)

    def _combine_subject_content(self, items: List[Dict]) -> str:
        """K·∫øt h·ª£p n·ªôi dung c·ªßa m·ªôt m√¥n h·ªçc"""
        sections = []
        
        # Th√¥ng tin chung
        general_info = [item for item in items if item.get('type') == 'general_info']
        if general_info:
            sections.append(f"Th√¥ng tin chung: {general_info[0].get('content', '')}")
        
        # CLOs
        clos = [item for item in items if item.get('type') == 'clo']
        if clos:
            clo_content = ' '.join([item.get('content', '') for item in clos])
            sections.append(f"Chu·∫©n ƒë·∫ßu ra: {clo_content}")
        
        # Sessions
        sessions = [item for item in items if item.get('type') == 'session']
        if sessions:
            session_content = ' '.join([item.get('content', '') for item in sessions[:5]])  # Limit
            sections.append(f"N·ªôi dung bu·ªïi h·ªçc: {session_content}")
        
        return ' '.join(sections)

    def _combine_document_content(self, items: List[Dict]) -> str:
        """K·∫øt h·ª£p n·ªôi dung c·ªßa m·ªôt lo·∫°i t√†i li·ªáu"""
        return ' '.join([item.get('content', '') for item in items])

    def _generate_summary(self, content: str, gemini_model, level: str) -> str:
        """T·∫°o t√≥m t·∫Øt b·∫±ng Gemini"""
        if not content.strip():
            return "Kh√¥ng c√≥ th√¥ng tin"
        
        if level == 'subject':
            prompt = f"""
            T·∫°o m·ªôt t√≥m t·∫Øt ng·∫Øn g·ªçn v√† to√†n di·ªán v·ªÅ m√¥n h·ªçc d·ª±a tr√™n th√¥ng tin sau:
            
            {content[:4000]}
            
            T√≥m t·∫Øt c·∫ßn bao g·ªìm:
            - T√™n m√¥n h·ªçc v√† m√£ m√¥n
            - S·ªë t√≠n ch·ªâ
            - M√¥ t·∫£ ng·∫Øn g·ªçn
            - C√°c chu·∫©n ƒë·∫ßu ra ch√≠nh
            - N·ªôi dung h·ªçc ch√≠nh
            
            Gi·ªõi h·∫°n trong 200 t·ª´.
            """
        else:
            prompt = f"T√≥m t·∫Øt ng·∫Øn g·ªçn n·ªôi dung sau trong 100 t·ª´:\n\n{content[:2000]}"
        
        try:
            response = gemini_model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.error(f"L·ªói t·∫°o t√≥m t·∫Øt: {e}")
            return content[:500]  # Fallback

    def _extract_subject_metadata(self, items: List[Dict]) -> Dict:
        """Tr√≠ch xu·∫•t metadata cho m√¥n h·ªçc"""
        metadata = {}
        
        for item in items:
            item_meta = item.get('metadata', {})
            if not metadata.get('syllabus_name') and item_meta.get('syllabus_name'):
                metadata['syllabus_name'] = item_meta['syllabus_name']
            if not metadata.get('credits') and item_meta.get('credits'):
                metadata['credits'] = item_meta['credits']
            if not metadata.get('total_sessions') and item_meta.get('total_sessions'):
                metadata['total_sessions'] = item_meta['total_sessions']
        
        return metadata

    def _build_faiss_indices(self):
        """X√¢y d·ª±ng FAISS indices cho c√°c c·∫•p"""
        
        # Summary index
        if self.summary_data:
            summary_embeddings = []
            for item in self.summary_data:
                embedding = self.embedding_model.encode(item['content'])
                summary_embeddings.append(embedding)
            
            summary_embeddings = np.array(summary_embeddings).astype('float32')
            self.summary_index = faiss.IndexFlatIP(summary_embeddings.shape[1])
            self.summary_index.add(summary_embeddings)
        
        # Chunk index
        if self.chunk_data:
            chunk_embeddings = []
            for item in self.chunk_data:
                embedding = self.embedding_model.encode(item['content'])
                chunk_embeddings.append(embedding)
            
            chunk_embeddings = np.array(chunk_embeddings).astype('float32')
            self.chunk_index = faiss.IndexFlatIP(chunk_embeddings.shape[1])
            self.chunk_index.add(chunk_embeddings)
        
        # Detail index
        if self.detail_data:
            detail_embeddings = []
            for item in self.detail_data:
                embedding = self.embedding_model.encode(item['content'])
                detail_embeddings.append(embedding)
            
            detail_embeddings = np.array(detail_embeddings).astype('float32')
            self.detail_index = faiss.IndexFlatIP(detail_embeddings.shape[1])
            self.detail_index.add(detail_embeddings)

    def search_hierarchical(self, query: str, intent: QueryIntent, top_k: int = 10) -> List[SearchResult]:
        """T√¨m ki·∫øm ph√¢n c·∫•p th√¥ng minh"""
        query_embedding = self.embedding_model.encode([query]).astype('float32')
        results = []
        
        if intent.query_type == 'listing' or intent.subject_scope == 'all':
            # T√¨m ki·∫øm ·ªü level summary tr∆∞·ªõc
            if self.summary_index:
                distances, indices = self.summary_index.search(query_embedding, top_k)
                for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                    if idx < len(self.summary_data):
                        item = self.summary_data[idx]
                        results.append(SearchResult(
                            content=item['content'],
                            score=float(distance),
                            subject_code=item['subject_code'],
                            document_type='summary',
                            level='summary',
                            metadata=item['metadata']
                        ))
        
        elif intent.query_type == 'factual' and intent.complexity == 'simple':
            # T√¨m ki·∫øm chi ti·∫øt cho c√¢u h·ªèi ƒë∆°n gi·∫£n
            if self.detail_index:
                distances, indices = self.detail_index.search(query_embedding, top_k)
                for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                    if idx < len(self.detail_data):
                        item = self.detail_data[idx]
                        results.append(SearchResult(
                            content=item['content'],
                            score=float(distance),
                            subject_code=item['metadata'].get('subject_code', 'UNKNOWN'),
                            document_type=item['metadata'].get('type', 'unknown'),
                            level='detail',
                            metadata=item['metadata']
                        ))
        
        else:
            # T√¨m ki·∫øm multi-level cho c√°c tr∆∞·ªùng h·ª£p ph·ª©c t·∫°p
            results.extend(self._multi_level_search(query_embedding, top_k // 2))
        
        return results[:top_k]

    def _multi_level_search(self, query_embedding: np.ndarray, top_k: int) -> List[SearchResult]:
        """T√¨m ki·∫øm ƒëa c·∫•p"""
        results = []
        
        # T√¨m ·ªü level chunk
        if self.chunk_index and len(self.chunk_data) > 0:
            distances, indices = self.chunk_index.search(query_embedding, top_k)
            for distance, idx in zip(distances[0], indices[0]):
                if idx < len(self.chunk_data):
                    item = self.chunk_data[idx]
                    results.append(SearchResult(
                        content=item['content'],
                        score=float(distance),
                        subject_code=item['metadata'].get('subject_code', 'UNKNOWN'),
                        document_type=item['document_key'],
                        level='chunk',
                        metadata=item['metadata']
                    ))
        
        # T√¨m ·ªü level detail
        if self.detail_index and len(self.detail_data) > 0:
            distances, indices = self.detail_index.search(query_embedding, top_k)
            for distance, idx in zip(distances[0], indices[0]):
                if idx < len(self.detail_data):
                    item = self.detail_data[idx]
                    results.append(SearchResult(
                        content=item['content'],
                        score=float(distance),
                        subject_code=item['metadata'].get('subject_code', 'UNKNOWN'),
                        document_type=item['metadata'].get('type', 'unknown'),
                        level='detail',
                        metadata=item['metadata']
                    ))
        
        return results

class AdvancedRAGEngine:
    """
    Engine RAG ti√™n ti·∫øn v·ªõi ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng  
    ENHANCED v·ªõi GraphRAG: Hybrid Vector + Graph Database Architecture
    """
    
    def __init__(self, api_keys: Union[str, List[str]], enable_graph: bool = True):
        """Kh·ªüi t·∫°o RAG Engine v·ªõi API key rotation v√† GraphRAG capabilities"""
        
        # X·ª≠ l√Ω API keys input
        if isinstance(api_keys, str):
            api_keys = [api_keys]  # Convert single key to list
        
        # Initialize API Key Manager
        self.api_key_manager = APIKeyManager(api_keys)
        
        # Configure Gemini v·ªõi key ƒë·∫ßu ti√™n
        self.model = self.api_key_manager.get_current_model()
        
        # Initialize core components
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.embeddings = None
        self.data = None
        self.index = None
        
        # Enhanced components
        self.hierarchical_index = None
        self.subject_mapping = {}
        self.query_router = QueryRouter()
        self.multihop_engine = None
        
        # NEW: Query Preprocessor v·ªõi API key rotation support
        self.query_preprocessor = QueryPreprocessor(self.model, self)
        
        # GraphRAG components
        self.graph_db = None
        self.graph_enabled = enable_graph and GRAPH_AVAILABLE
        
        if self.graph_enabled:
            logger.info("üîÑ Kh·ªüi t·∫°o GraphRAG components...")
            try:
                self.graph_db = GraphDatabase()
                graph_connected = self.graph_db.connect()
                if graph_connected:
                    logger.info("‚úÖ GraphRAG mode: Vector + Graph Hybrid")
                else:
                    logger.warning("‚ö† Graph DB kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c - fallback to Vector-only")
                    self.graph_enabled = False
            except Exception as e:
                logger.warning(f"‚ö† L·ªói kh·ªüi t·∫°o GraphDatabase: {e} - fallback to Vector-only")
                self.graph_enabled = False
        else:
            logger.info("üìã Vector-only mode enabled")
        
        # Conversation memory for chatbot functionality
        self.conversation_memory = {}  # session_id -> [{'user': query, 'bot': response}]
        
        logger.info(f"‚úì AdvancedRAGEngine ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi {len(self.api_key_manager.api_keys)} API keys")
        logger.info(f"‚úì GraphRAG enabled: {self.graph_enabled}")

    def _call_gemini_with_rotation(self, prompt: str, max_retries: int = None) -> str:
        """
        G·ªçi Gemini v·ªõi auto rotation khi g·∫∑p quota error
        """
        if max_retries is None:
            max_retries = len(self.api_key_manager.api_keys)
        
        for attempt in range(max_retries):
            try:
                # L·∫•y model hi·ªán t·∫°i
                current_model = self.api_key_manager.get_current_model()
                if not current_model:
                    raise Exception("Kh√¥ng th·ªÉ t·∫°o Gemini model")
                
                # Update model reference
                self.model = current_model
                self.query_preprocessor.gemini_model = current_model
                
                # G·ªçi API
                response = current_model.generate_content(prompt)
                return response.text.strip()
                
            except Exception as e:
                error_str = str(e)
                
                # Ki·ªÉm tra quota error
                if ('429' in error_str or 
                    'quota' in error_str.lower() or 
                    'exceeded' in error_str.lower() or
                    'resource exhausted' in error_str.lower()):
                    
                    logger.warning(f"API Key {self.api_key_manager.current_index + 1} quota exceeded, rotating...")
                    
                    # Th·ª≠ rotate key
                    if not self.api_key_manager.rotate_key(error_str):
                        logger.error("T·∫•t c·∫£ API keys ƒë√£ h·∫øt quota")
                        break
                    
                    continue  # Th·ª≠ v·ªõi key m·ªõi
                else:
                    # L·ªói kh√°c (kh√¥ng ph·∫£i quota) -> throw ngay
                    raise e
        
        # N·∫øu t·ªõi ƒë√¢y = h·∫øt quota t·∫•t c·∫£ keys
        raise Exception("T·∫•t c·∫£ API keys ƒë√£ h·∫øt quota, vui l√≤ng th·ª≠ l·∫°i sau")

    def initialize(self, data_path: str):
        """Kh·ªüi t·∫°o engine v·ªõi d·ªØ li·ªáu t·ª´ file JSON v√† populate graph database"""
        logger.info(f"ƒêang kh·ªüi t·∫°o Advanced RAG Engine v·ªõi d·ªØ li·ªáu t·ª´ {data_path}")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        self.data = self._process_data(raw_data)
        self._create_embeddings()
        self._build_index()
        
        # Initialize hierarchical index
        self.hierarchical_index = HierarchicalIndex(self.embedding_model)
        
        # Initialize query chain for multi-hop queries
        self.query_chain = QueryChain(self)
        
        # GraphRAG: Populate graph database
        if self.graph_enabled and self.graph_db:
            logger.info("üîÑ Populating Graph Database...")
            try:
                # Extract curriculum data for graph
                curriculum_data = []
                if isinstance(raw_data, dict) and 'syllabuses' in raw_data:
                    curriculum_data = raw_data['syllabuses']
                elif isinstance(raw_data, list):
                    curriculum_data = raw_data
                else:
                    logger.warning("Unknown data format for graph extraction")
                
                if curriculum_data:
                    nodes, relationships = self.graph_db.extract_entities_from_curriculum_data(curriculum_data)
                    
                    # Create schema and populate (simplified approach)
                    if nodes and relationships:
                        logger.info(f"üìä Graph entities: {len(nodes)} nodes, {len(relationships)} relationships")
                        # Note: In production, you'd want to populate the actual Neo4j database here
                        # For now, we store the extracted entities for later use
                        self.graph_entities = {'nodes': nodes, 'relationships': relationships}
                        logger.info("‚úÖ Graph entities extracted successfully")
                    else:
                        logger.warning("‚ö† No graph entities extracted")
                else:
                    logger.warning("‚ö† No curriculum data found for graph extraction")
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói populate graph database: {e}")
                self.graph_enabled = False
        
        self.is_initialized = True
        logger.info("‚úÖ Kh·ªüi t·∫°o ho√†n t·∫•t")
    
    def add_to_conversation(self, session_id: str, user_message: str, bot_response: str):
        """Th√™m tin nh·∫Øn v√†o l·ªãch s·ª≠ conversation"""
        if session_id not in self.conversation_memory:
            self.conversation_memory[session_id] = []
        
        self.conversation_memory[session_id].append({
            'user': user_message,
            'bot': bot_response,
            'timestamp': time.time()
        })
        
        # Gi·ªõi h·∫°n l·ªãch s·ª≠ conversation (ch·ªâ gi·ªØ 10 tin nh·∫Øn g·∫ßn nh·∫•t)
        if len(self.conversation_memory[session_id]) > 10:
            self.conversation_memory[session_id] = self.conversation_memory[session_id][-10:]
    
    def get_conversation_context(self, session_id: str) -> str:
        """L·∫•y context t·ª´ l·ªãch s·ª≠ conversation"""
        if session_id not in self.conversation_memory:
            return ""
        
        context = "L·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán:\n"
        for msg in self.conversation_memory[session_id][-5:]:  # Ch·ªâ l·∫•y 5 tin nh·∫Øn g·∫ßn nh·∫•t
            context += f"Ng∆∞·ªùi d√πng: {msg['user']}\n"
            context += f"AI: {msg['bot']}\n\n"
        
        return context
    
    def should_use_data_search(self, query: str, session_id: str = None) -> bool:
        """Ph√¢n bi·ªát c√¢u h·ªèi c·∫ßn t√¨m ki·∫øm data hay ch·ªâ c·∫ßn LLM v·ªõi conversation context"""
        query_lower = query.lower().strip()
        
        # ENHANCED: Ki·ªÉm tra context conversation history ƒë·ªÉ x√°c ƒë·ªãnh c√¢u h·ªèi follow-up v·ªÅ data
        has_conversation_history = (session_id and 
                                   session_id in self.conversation_memory and 
                                   len(self.conversation_memory[session_id]) > 0)
        
        if has_conversation_history:
            # L·∫•y c√¢u tr·∫£ l·ªùi bot g·∫ßn nh·∫•t ƒë·ªÉ hi·ªÉu context
            last_bot_response = self.conversation_memory[session_id][-1].get('bot', '')
            
            # SMART CONTEXT DETECTION: N·∫øu c√¢u tr·∫£ l·ªùi tr∆∞·ªõc c√≥ ch·ª©a th√¥ng tin v·ªÅ FPTU/m√¥n h·ªçc
            # v√† c√¢u h·ªèi hi·ªán t·∫°i l√† follow-up v·ªÅ h·ªçc k·ª≥ kh√°c -> c·∫ßn data search
            fptu_context_indicators = [
                'm√¥n h·ªçc', 'mon hoc', 'subject', 'course', 't√≠n ch·ªâ', 'tin chi', 'credit',
                'k·ª≥', 'ky', 'semester', 'ng√†nh ai', 'nganh ai', 'fptu', 'fpt university',
                'csi', 'mad', 'csd', 'dbi', 'aig', 'cea', 'jpd', 'ady', 'ite'  # Common subject prefixes
            ]
            
            has_fptu_context = any(indicator in last_bot_response.lower() for indicator in fptu_context_indicators)
            
            # ENHANCED: Ph√°t hi·ªán c√¢u h·ªèi follow-up v·ªÅ k√¨ h·ªçc kh√°c
            semester_followup_patterns = [
                r'k√¨\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)\s*(th√¨\s*sao|nh∆∞\s*th·∫ø\s*n√†o|ra\s*sao)',
                r'ky\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)\s*(th√¨\s*sao|nh∆∞\s*th·∫ø\s*n√†o|ra\s*sao)',
                r'semester\s*(\d+|two|three|four|five|six|seven|eight)\s*(how|what)',
                r'(c√≤n|con)\s*k√¨\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
                r'(c√≤n|con)\s*ky\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
                r'(k√¨|ky)\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)\s*(c√≥|co)\s*(g√¨|gi|nh·ªØng\s*g√¨)',
                r'(h·ªçc|hoc)\s*(k√¨|ky)\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
                r'(th√¨|thi)\s*(k√¨|ky)\s*(\d+|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
                # Th√™m c√°c pattern kh√°c
                r'k√¨\s*ti·∫øp\s*theo', r'ky\s*tiep\s*theo',
                r'k√¨\s*sau', r'ky\s*sau',
                r'k√¨\s*kh√°c', r'ky\s*khac'
            ]
            
            is_semester_followup = any(re.search(pattern, query_lower) for pattern in semester_followup_patterns)
            
            if has_fptu_context and is_semester_followup:
                logger.info("SMART DETECTION: Semester follow-up question with FPTU context - using DATA SEARCH")
                return True
        
        # Ki·ªÉm tra c√°c pattern ch·ªâ c·∫ßn conversation context (KH√îNG c·∫ßn data search)
        conversation_only_patterns = [
            # Tham chi·∫øu ƒë·∫øn c√¢u tr·∫£ l·ªùi tr∆∞·ªõc (nh∆∞ng KH√îNG ph·∫£i v·ªÅ k√¨ h·ªçc)
            r'(trong|·ªü|t·ª´)?\s*(danh s√°ch|b·∫£ng|k·∫øt qu·∫£|th√¥ng tin)\s*(tr√™n|n√†y|ƒë√≥|v·ª´a|·ªü tr√™n)',
            r'(m√¥n|item|m·ª•c)\s*(ƒë·∫ßu ti√™n|cu·ªëi c√πng|th·ª© \d+|s·ªë \d+)',
            r'(theo|d·ª±a v√†o|t·ª´)\s*(th√¥ng tin|d·ªØ li·ªáu|k·∫øt qu·∫£)\s*(tr√™n|v·ª´a|ƒë√£)',
            r'(c√°i|th·ª©|m√≥n)\s*(ƒë·∫ßu|cu·ªëi|n√†o)\s*(trong|·ªü)',
            r'(first|last|which)\s*(one|item)',
            
            # C√¢u h·ªèi v·ªÅ conversation tr∆∞·ªõc
            r'(b·∫°n|em|m√¨nh)\s*(v·ª´a|ƒë√£|v·ª≠a)\s*(n√≥i|tr·∫£ l·ªùi|ƒë∆∞a ra)',
            r'(t·ª´|theo)\s*(c√¢u tr·∫£ l·ªùi|th√¥ng tin)\s*(tr∆∞·ªõc|ph√≠a tr√™n)',
            r'(√Ω nghƒ©a|hi·ªÉu|gi·∫£i th√≠ch).*(?!m√¥n|ng√†nh)',  # Tr·ª´ khi h·ªèi v·ªÅ m√¥n/ng√†nh
            
            # Pure greetings/chit-chat (KH√îNG li√™n quan FPTU)
            r'^(xin ch√†o|ch√†o|hi|hello)$',
            r'^(c·∫£m ∆°n|thank|thanks).*',
            r'^(t·∫°m bi·ªát|bye|goodbye).*',
            r'(b·∫°n|em)\s*(c√≥ th·ªÉ|c√≥ th·ªÉ)\s*(gi√∫p|h·ªó tr·ª£|l√†m)\s*(g√¨|ƒë∆∞·ª£c g√¨)',
        ]
        
        # Ki·ªÉm tra conversation context patterns tr∆∞·ªõc (NH∆ØNG B·ªé QUA pattern follow-up v·ªÅ h·ªçc k√¨)
        for pattern in conversation_only_patterns:
            if re.search(pattern, query_lower):
                # ƒê·∫∂C BI·ªÜT: N·∫øu pattern match nh∆∞ng c√≥ ch·ª©a t·ª´ kh√≥a v·ªÅ k√¨ h·ªçc -> v·∫´n c·∫ßn data search
                if any(semester_word in query_lower for semester_word in ['k√¨', 'ky', 'semester']):
                    logger.info(f"CONVERSATION PATTERN '{pattern}' matched but contains semester keyword - still using DATA SEARCH")
                    return True
                logger.info(f"CONVERSATION CONTEXT DETECTED: Pattern '{pattern}' matched")
                return False
        
        # ENHANCED: DOMAIN-SPECIFIC PATTERNS - FPTU AI context m·∫°nh m·∫Ω h∆°n
        fptu_domain_patterns = [
            # Ng√†nh AI context (ng·∫ßm ƒë·ªãnh v·ªÅ FPTU)
            r'(m√¥n|mon).*ng√†nh.*ai(?!.*n√†o)',  # "m√¥n ng√†nh AI" 
            r'ng√†nh.*ai(?!.*n√†o).*m√¥n',
            r'to√†n b·ªô.*m√¥n.*ai',
            r'danh s√°ch.*m√¥n.*ai(?!.*n√†o)',
            
            # K√¨ h·ªçc patterns (ENHANCED)
            r'k√¨\s*(\d+|m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
            r'ky\s*(\d+|m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)', 
            r'semester\s*(\d+|one|two|three|four|five|six|seven|eight)',
            r'h·ªçc\s*k√¨\s*(\d+|m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
            r'hoc\s*ky\s*(\d+|m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m)',
            
            # FPTU-specific terms
            r'combo.*chuy√™n ng√†nh',
            r'chuy√™n ng√†nh.*h·∫πp',
            r'combo.*h·∫πp',
            r'combo.*ai',  # "combo AI"
            r'fptu?.*ai',
            r'fpt.*university.*ai',
            
            # Course-related v·ªõi context AI
            r'(m√¥n|mon).*(ai|artificial intelligence)',
            r'curriculum.*ai',
            r'ch∆∞∆°ng tr√¨nh.*ai',
            
            # Khi h·ªèi chung v·ªÅ FPTU domain
            r'm√¥n.*(?:n√†o|g√¨).*(?:thu·ªôc|trong).*ai',
            r'ai.*c√≥.*m√¥n.*n√†o',
        ]
        
        # Ki·ªÉm tra FPTU domain patterns
        for pattern in fptu_domain_patterns:
            if re.search(pattern, query_lower):
                logger.info(f"FPTU DOMAIN CONTEXT DETECTED: Pattern '{pattern}' matched")
                return True
        
        # C√°c pattern C·∫¶N t√¨m ki·∫øm data (traditional patterns)
        data_search_patterns = [
            # T√¨m ki·∫øm m√¥n h·ªçc c·ª• th·ªÉ (case insensitive)
            r'[a-zA-Z]{2,4}\d{3}[a-z]*\s+(?:l√†|g√¨|th√¥ng tin|chi ti·∫øt|m√¥n)',  # M√£ m√¥n h·ªçc v·ªõi context c√¢u h·ªèi
            r'[a-zA-Z]{2,4}\d{3}[a-z]*$',  # M√£ m√¥n h·ªçc ƒë∆°n thu·∫ßn
            r'[a-zA-Z]{2,4}\d{3}[a-z]*',  # M√£ m√¥n h·ªçc general
            r'm√¥n h·ªçc.*(?:n√†o|g√¨|l√†)', r'mon hoc', r'subject', r'course',
            r't√≠n ch·ªâ', r'tin chi', r'credit',
            r'syllabus', r'curriculum', r'ch∆∞∆°ng tr√¨nh',
            r'ng√†nh.*(?:n√†o|g√¨|c√≥)', r'nganh', r'major',
            r'ti√™n quy·∫øt', r'tien quyet', r'prerequisite',
            r'CLO', r'learning outcome',
            
            # T·ª´ kh√≥a t√¨m ki·∫øm TO√ÄN B·ªò data (kh√¥ng ph·∫£i follow-up)
            r'^(li·ªát k√™|liet ke)', r'^(danh s√°ch|danh sach)(?!.*tr√™n)',
            r'^(t·∫•t c·∫£|tat ca)', r'^(c√°c m√¥n|cac mon)',
            r'th√¥ng tin.*(?:v·ªÅ|c·ªßa|m√¥n)', r'chi ti·∫øt.*(?:v·ªÅ|c·ªßa|m√¥n)',
            r'bao nhi√™u.*(?:m√¥n|t√≠n ch·ªâ|credit)',
            
            # T√™n m√¥n h·ªçc ph·ªï bi·∫øn
            r'machine learning', r'artificial intelligence', r'data science',
            r'programming', r'algorithm', r'database', r'network',
            r'mathematics', r'to√°n(?:.*h·ªçc)?', r'toan', r'physics', r'v·∫≠t l√Ω',
        ]
        
        # Ki·ªÉm tra xem c√≥ pattern data search n√†o match kh√¥ng
        for pattern in data_search_patterns:
            if re.search(pattern, query_lower):
                logger.info(f"DATA SEARCH DETECTED: Pattern '{pattern}' matched")
                return True
        
        logger.info("NO DATA SEARCH PATTERNS - using conversation/direct chat")
        return False
    
    def chat_direct(self, question: str, session_id: str = None) -> str:
        """Chat tr·ª±c ti·∫øp v·ªõi LLM kh√¥ng c·∫ßn t√¨m ki·∫øm data"""
        try:
            # L·∫•y context t·ª´ conversation history n·∫øu c√≥ session_id
            conversation_context = ""
            if session_id:
                conversation_context = self.get_conversation_context(session_id)
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi follow-up v·ªÅ d·ªØ li·ªáu v·ª´a tr·∫£ l·ªùi kh√¥ng
            is_followup_about_data = (
                conversation_context and 
                any(indicator in question.lower() for indicator in [
                    'danh s√°ch tr√™n', 'b·∫£ng tr√™n', 'th√¥ng tin tr√™n',
                    'ƒë·∫ßu ti√™n', 'cu·ªëi c√πng', 'th·ª©', 'trong danh s√°ch'
                ])
            )
            
            # Ki·ªÉm tra c√≥ ph·∫£i c√¢u h·ªèi v·ªÅ FPTU domain m√† ƒë∆∞·ª£c misclassified kh√¥ng
            fptu_related_but_misclassified = any(indicator in question.lower() for indicator in [
                'm√¥n ng√†nh ai', 'ng√†nh ai', 'combo chuy√™n ng√†nh', 'chuy√™n ng√†nh h·∫πp',
                'fptu', 'fpt university', 'to√†n b·ªô m√¥n ai', 'danh s√°ch m√¥n ai'
            ])
            
            if is_followup_about_data:
                prompt = f"""B·∫°n l√† AI Assistant c·ªßa FPTU. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n CH√çNH X√ÅC th√¥ng tin ƒë√£ cung c·∫•p trong cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc.

{conversation_context}

QUAN TR·ªåNG: 
- D·ª±a v√†o CH√çNH X√ÅC th√¥ng tin trong l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán
- N·∫øu h·ªèi v·ªÅ "ƒë·∫ßu ti√™n", h√£y t√¨m item ƒë·∫ßu ti√™n trong danh s√°ch/b·∫£ng ƒë√£ tr·∫£ l·ªùi
- N·∫øu h·ªèi v·ªÅ "cu·ªëi c√πng", h√£y t√¨m item cu·ªëi c√πng trong danh s√°ch/b·∫£ng ƒë√£ tr·∫£ l·ªùi  
- Kh√¥ng t·ª± t·∫°o ra th√¥ng tin m·ªõi
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}

Tr·∫£ l·ªùi:"""
            elif fptu_related_but_misclassified:
                # G·ª£i √Ω ng∆∞·ªùi d√πng h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ FPTU
                prompt = f"""B·∫°n l√† AI Assistant c·ªßa FPTU - chuy√™n h·ªó tr·ª£ th√¥ng tin v·ªÅ tr∆∞·ªùng FPT University.

{conversation_context}

T√¥i hi·ªÉu b·∫°n ƒëang h·ªèi v·ªÅ th√¥ng tin li√™n quan ƒë·∫øn FPT University v√† ng√†nh AI. ƒê·ªÉ t√¥i c√≥ th·ªÉ cung c·∫•p th√¥ng tin ch√≠nh x√°c nh·∫•t, b·∫°n c√≥ th·ªÉ h·ªèi c·ª• th·ªÉ h∆°n nh∆∞:

- "Li·ªát k√™ c√°c m√¥n h·ªçc ng√†nh AI theo k·ª≥"
- "Danh s√°ch combo chuy√™n ng√†nh AI" 
- "C√°c m√¥n h·ªçc trong ch∆∞∆°ng tr√¨nh AI FPTU"
- "Th√¥ng tin chi ti·∫øt v·ªÅ m√¥n [t√™n m√¥n]"

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi th√¢n thi·ªán v√† g·ª£i √Ω c√°ch h·ªèi hi·ªáu qu·∫£
- Nh·∫•n m·∫°nh ƒë√¢y l√† h·ªá th·ªëng h·ªó tr·ª£ th√¥ng tin FPTU
- Khuy·∫øn kh√≠ch h·ªèi c·ª• th·ªÉ ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t
- Kh√¥ng s·ª≠ d·ª•ng bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c hay icon

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}

Tr·∫£ l·ªùi:"""
            else:
                prompt = f"""B·∫°n l√† AI Assistant c·ªßa FPTU - tr·ª£ l√Ω th√¥ng minh h·ªó tr·ª£ sinh vi√™n.

{conversation_context}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch t·ª± nhi√™n v√† th√¢n thi·ªán
- N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn m√¥n h·ªçc c·ª• th·ªÉ, g·ª£i √Ω ng∆∞·ªùi d√πng h·ªèi c·ª• th·ªÉ h∆°n
- Gi·ªØ phong c√°ch tr√≤ chuy·ªán t·ª± nhi√™n v√† h·ªØu √≠ch
- Kh√¥ng s·ª≠ d·ª•ng bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c hay icon

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}

Tr·∫£ l·ªùi:"""
            
            answer = self._call_gemini_with_rotation(prompt)
            
            # L∆∞u v√†o conversation memory n·∫øu c√≥ session_id
            if session_id:
                self.add_to_conversation(session_id, question, answer)
            
            return answer
            
        except Exception as e:
            logger.error(f"L·ªói chat_direct: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?"
    
    def chatbot_query(self, question: str, session_id: str, enable_multihop: bool = False) -> Dict[str, Any]:
        """
        Main chatbot method - k·∫øt h·ª£p Query Preprocessing, RAG search v√† conversation management
        
        Args:
            question: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
            session_id: ID phi√™n ƒë·ªÉ qu·∫£n l√Ω conversation
            enable_multihop: C√≥ cho ph√©p multi-hop query kh√¥ng
            
        Returns:
            Dict ch·ª©a answer, search_results, metadata v√† conversation info
        """
        logger.info(f"========== CHATBOT QUERY ==========")
        logger.info(f"SESSION: {session_id}")
        logger.info(f"USER QUERY: '{question}'")
        logger.info(f"MULTIHOP ENABLED: {enable_multihop}")
        
        start_time = time.time()
        
        try:
            # B∆Ø·ªöC 1: Query Preprocessing v·ªõi LLM
            conversation_context = self.get_conversation_context(session_id)
            conversation_summary = ""
            if conversation_context:
                # T·∫°o summary ng·∫Øn g·ªçn cho context
                last_exchanges = conversation_context.split('\n')[-4:]  # 2 l∆∞·ª£t cu·ªëi
                conversation_summary = '\n'.join(last_exchanges)
            
            preprocessed = self.query_preprocessor.preprocess_query(question, conversation_summary)
            
            # B∆Ø·ªöC 2: Quy·∫øt ƒë·ªãnh strategy d·ª±a tr√™n k·∫øt qu·∫£ preprocessing
            use_data_search = preprocessed.needs_data_search
            final_query = preprocessed.processed_query if preprocessed.confidence > 0.6 else question
            
            logger.info(f"PREPROCESSING DECISION:")
            logger.info(f"  Use preprocessed query: {preprocessed.confidence > 0.6}")
            logger.info(f"  Final query: '{final_query}'")
            logger.info(f"  Strategy: {'DATA SEARCH' if use_data_search else 'DIRECT CHAT'}")
            
            if use_data_search:
                # S·ª≠ d·ª•ng RAG v·ªõi query ƒë√£ ƒë∆∞·ª£c preprocessing
                logger.info("Executing RAG search with preprocessed query...")
                
                if enable_multihop:
                    result = self.query_with_multihop(final_query, enable_multihop)
                    query_type = 'data_search_multihop'
                    # For multihop, the answer is in 'final_answer' key
                    answer = result.get('final_answer', result.get('answer', ''))
                else:
                    result = self.query(final_query)
                    query_type = 'data_search'
                    answer = result.get('answer', '')
                
                # L∆∞u v√†o conversation memory (d√πng question g·ªëc)
                self.add_to_conversation(session_id, question, answer)
                
                # Build response structure for multihop vs regular
                if enable_multihop:
                    # For multihop, build consistent response structure
                    response_result = {
                        'answer': answer,
                        'search_results': result.get('followup_results', []),  # Use followup results as search results
                        'metadata': {
                            'query_type': query_type,
                            'is_data_search': True,
                            'is_direct_chat': False,
                            'conversation_enhanced': bool(conversation_context),
                            'subjects_covered': 0,  # Will be calculated below
                            'preprocessing_info': {
                                'original_query': preprocessed.original_query,
                                'processed_query': preprocessed.processed_query,
                                'intent': preprocessed.intent_description,
                                'confidence': preprocessed.confidence,
                                'keywords': preprocessed.suggested_keywords,
                                'query_improved': preprocessed.confidence > 0.6
                            }
                        },
                        'multihop_info': {
                            'has_followup': result.get('has_followup', False),
                            'followup_queries': result.get('followup_queries', []),
                            'execution_path': result.get('execution_path', []),
                            'original_answer': result.get('original_answer', ''),
                            'final_answer': result.get('final_answer', answer)
                        }
                    }
                    result = response_result
                else:
                    # Set metadata v·ªõi preprocessing info
                    if 'metadata' not in result:
                        result['metadata'] = {}
                    
                    result['metadata']['query_type'] = query_type
                    result['metadata']['is_data_search'] = True
                    result['metadata']['is_direct_chat'] = False
                    result['metadata']['conversation_enhanced'] = bool(conversation_context)
                    result['metadata']['preprocessing_info'] = {
                        'original_query': preprocessed.original_query,
                        'processed_query': preprocessed.processed_query,
                        'intent': preprocessed.intent_description,
                        'confidence': preprocessed.confidence,
                        'keywords': preprocessed.suggested_keywords,
                        'query_improved': preprocessed.confidence > 0.6
                    }
                    
                    # Add multihop info if not present
                    if 'multihop_info' not in result:
                        result['multihop_info'] = {'has_followup': False}
                
                logger.info(f"CHATBOT RESPONSE:")
                logger.info(f"  - Query type: {result['metadata']['query_type']}")
                logger.info(f"  - Preprocessing confidence: {preprocessed.confidence:.2f}")
                logger.info(f"  - Used improved query: {preprocessed.confidence > 0.6}")
                logger.info(f"  - Response length: {len(answer)} chars")
                logger.info(f"  - Search results: {len(result.get('search_results', []))}")
                logger.info(f"  - Processing time: {time.time() - start_time:.2f}s")
                
                return result
                
            else:
                # S·ª≠ d·ª•ng direct chat
                logger.info("Executing direct chat...")
                answer = self.chat_direct(question, session_id)
                
                # Build result structure consistent v·ªõi RAG response
                result = {
                    'answer': answer,
                    'search_results': [],
                    'metadata': {
                        'query_type': 'direct_chat',
                        'is_data_search': False,
                        'is_direct_chat': True,
                        'conversation_enhanced': True,
                        'subjects_covered': 0,
                        'preprocessing_info': {
                            'original_query': preprocessed.original_query,
                            'processed_query': preprocessed.processed_query,
                            'intent': preprocessed.intent_description,
                            'confidence': preprocessed.confidence,
                            'keywords': preprocessed.suggested_keywords,
                            'query_improved': False
                        }
                    },
                    'multihop_info': {
                        'has_followup': False
                    }
                }
                
                logger.info(f"CHATBOT RESPONSE:")
                logger.info(f"  - Query type: {result['metadata']['query_type']}")
                logger.info(f"  - Preprocessing confidence: {preprocessed.confidence:.2f}")
                logger.info(f"  - Response length: {len(answer)} chars")
                logger.info(f"  - Search results: {len(result.get('search_results', []))}")
                logger.info(f"  - Processing time: {time.time() - start_time:.2f}s")
                
                return result
                
        except Exception as e:
            logger.error(f"L·ªói trong chatbot_query: {e}")
            
            # Fallback response
            fallback_answer = "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?"
            
            result = {
                'answer': fallback_answer,
                'search_results': [],
                'metadata': {
                    'query_type': 'error',
                    'is_data_search': False,
                    'is_direct_chat': False,
                    'conversation_enhanced': False,
                    'subjects_covered': 0,
                    'error': str(e),
                    'preprocessing_info': None
                },
                'multihop_info': {
                    'has_followup': False
                }
            }
            
            return result
            
        finally:
            total_time = time.time() - start_time
            logger.info(f"========== CHATBOT QUERY COMPLETE ==========")

    def _process_data(self, raw_data):
        processed_data = []
        
        if isinstance(raw_data, dict) and 'syllabuses' in raw_data:
            # Extract major information
            major_code = raw_data.get('major_code_input', 'UNKNOWN')
            curriculum_title = raw_data.get('curriculum_title_on_page', 'N/A')
            
            # 0. MAJOR/NG√ÄNH OVERVIEW ENTRY
            major_overview = f"""
Nganh hoc: {major_code}
Ten chuong trinh: {curriculum_title}
Tong so mon hoc: {len(raw_data['syllabuses'])} mon
Cac mon hoc trong nganh {major_code}:
"""
            
            for syllabus in raw_data['syllabuses']:
                metadata = syllabus.get('metadata', {})
                subject_code = metadata.get('subject_code_on_page', 'UNKNOWN')
                course_name = metadata.get('title', 'N/A')
                credits = metadata.get('credits', 'N/A')
                semester = metadata.get('semester_from_curriculum', 'N/A')
                major_overview += f"- {subject_code}: {course_name} ({credits} tin chi, Ky {semester})\n"
            
            processed_data.append({
                'content': major_overview,
                'subject_code': 'MAJOR_OVERVIEW',
                'type': 'major_overview',
                'major_code': major_code,
                'metadata': {
                    'major_code': major_code,
                    'curriculum_title': curriculum_title,
                    'total_subjects': len(raw_data['syllabuses']),
                    'type': 'major_overview',
                    'search_keywords': f"nganh {major_code} chuong trinh hoc tat ca mon hoc liet ke danh sach"
                }
            })
            
            # Create combo specialization content
            combo_groups = {}
            for syllabus in raw_data['syllabuses']:
                metadata = syllabus.get('metadata', {})
                combo = metadata.get('combo_short_name_from_curriculum', '').strip()
                
                if combo:  # Only process subjects with actual combo names
                    if combo not in combo_groups:
                        combo_groups[combo] = []
                    combo_groups[combo].append({
                        'subject_code': metadata.get('subject_code_on_page', ''),
                        'title': metadata.get('title', ''),
                        'credits': metadata.get('credits', ''),
                        'semester': metadata.get('semester_from_curriculum', ''),
                        'description': metadata.get('description', '')[:200] + '...' if metadata.get('description') else ''
                    })
            
            # Create combo overview content
            for combo_name, subjects in combo_groups.items():
                combo_content = f"""
Combo chuyen nganh hep: {combo_name}
Nganh: {major_code}
So luong mon hoc: {len(subjects)} mon
Cac mon hoc trong combo:
"""
                for subject in subjects:
                    combo_content += f"- {subject['subject_code']}: {subject['title']} ({subject['credits']} tin chi, Ky {subject['semester']})\n"
                    if subject['description']:
                        combo_content += f"  Mo ta: {subject['description']}\n"
                
                # Add combo description based on name
                if 'COM1' in combo_name:
                    combo_content += "\nChuyen nganh: Data Science va Big Data Analytics"
                    combo_content += "\nMo ta: Tap trung vao khoa hoc du lieu, khai pha du lieu va phan tich du lieu lon"
                elif 'COM3' in combo_name:
                    combo_content += "\nChuyen nganh: AI for Healthcare va Research"
                    combo_content += "\nMo ta: Ung dung AI trong y te, nghien cuu khoa hoc va giao dich tai chinh"
                elif 'COM2' in combo_name:
                    combo_content += "\nChuyen nganh: Text Mining va Search Engineering"
                    combo_content += "\nMo ta: Khai thac van ban, xu ly ngon ngu tu nhien va cong cu tim kiem"
                
                processed_data.append({
                    'content': combo_content,
                    'subject_code': f'COMBO_{combo_name}',
                    'type': 'combo_specialization',
                    'major_code': major_code,
                    'combo_name': combo_name,
                    'subject_count': len(subjects),
                    'metadata': {
                        'major_code': major_code,
                        'combo_name': combo_name,
                        'type': 'combo_specialization',
                        'subject_count': len(subjects),
                        'search_keywords': f"combo chuyen nganh hep {combo_name} {major_code} specialization track"
                    }
                })
            
            for syllabus in raw_data['syllabuses']:
                metadata = syllabus.get('metadata', {})
                subject_code = metadata.get('subject_code_on_page', 'UNKNOWN')
                course_name = metadata.get('title', 'N/A')
                credits = metadata.get('credits', 'N/A')
                semester = metadata.get('semester_from_curriculum', 'N/A')
                
                # 1. COMPREHENSIVE GENERAL INFO
                general_content = f"""
Nganh: {major_code}
Mon hoc: {course_name}
Ma mon: {subject_code}
So tin chi: {credits}
Ky hoc: {semester}
Mon tien quyet: {metadata.get('prerequisites', 'Khong co')}
Mo ta: {metadata.get('description', 'N/A')}
Nhiem vu sinh vien: {metadata.get('student_tasks', 'N/A')}
"""
                
                processed_data.append({
                    'content': general_content,
                    'subject_code': subject_code,
                    'type': 'general_info',
                    'major_code': major_code,
                    'metadata': {
                        'major_code': major_code,
                        'subject_code': subject_code,
                        'course_name': course_name,
                        'credits': credits,
                        'semester': semester,
                        'semester_from_curriculum': metadata.get('semester_from_curriculum'),  # For filtering
                        'prerequisites': metadata.get('prerequisites', ''),
                        'course_type_guess': metadata.get('course_type_guess', ''),  # CRITICAL: For Coursera boost
                        'search_keywords': f"nganh {major_code} {subject_code} {course_name} {credits} tin chi ky {semester}"
                    }
                })
                
                # 2. DETAILED CLO PROCESSING
                if 'learning_outcomes' in syllabus:
                    clos = syllabus['learning_outcomes']
                    clo_count = len(clos)
                    
                    # Summary CLO info
                    clo_summary = f"""
Nganh {major_code} - Mon {subject_code} ({course_name}) co {clo_count} CLO (Course Learning Outcomes):
"""
                    for i, clo in enumerate(clos, 1):
                        clo_summary += f"{clo.get('id', f'CLO{i}')}: {clo.get('details', '')[:100]}...\n"
                    
                    processed_data.append({
                        'content': clo_summary,
                        'subject_code': subject_code,
                        'type': 'learning_outcomes_summary',
                        'major_code': major_code,
                        'clo_count': clo_count,
                        'metadata': {
                            'major_code': major_code,
                            'subject_code': subject_code,
                            'type': 'CLO_summary',
                            'count': clo_count,
                            'search_keywords': f"nganh {major_code} {subject_code} CLO learning outcomes {clo_count} CLO"
                        }
                    })
                    
                    # Individual CLO details
                    for clo in clos:
                        clo_detail = f"""
Nganh {major_code} - Mon {subject_code} - {clo.get('id', 'CLO')}: {clo.get('details', 'Khong co mo ta')}
"""
                        processed_data.append({
                            'content': clo_detail,
                            'subject_code': subject_code,
                            'type': 'learning_outcome_detail',
                            'major_code': major_code,
                            'clo_id': clo.get('id', ''),
                            'metadata': {
                                'major_code': major_code,
                                'subject_code': subject_code,
                                'clo_id': clo.get('id', ''),
                                'type': 'CLO_detail',
                                'search_keywords': f"nganh {major_code} {subject_code} {clo.get('id', '')} learning outcome"
                            }
                        })
                
                # 3. MATERIALS PROCESSING
                if 'materials' in syllabus:
                    materials = syllabus['materials']
                    main_materials = [m for m in materials if m.get('is_main_material', False)]
                    
                    materials_content = f"""
Tai lieu mon {subject_code} - Nganh {major_code} ({len(materials)} tai lieu):
Tai lieu chinh: {len(main_materials)} tai lieu
"""
                    for mat in materials:
                        mat_type = "Chinh" if mat.get('is_main_material') else "Phu"
                        materials_content += f"- [{mat_type}] {mat.get('description', '')}"
                        if mat.get('author'):
                            materials_content += f" - {mat.get('author', '')}"
                        materials_content += "\n"
                    
                    processed_data.append({
                        'content': materials_content,
                        'subject_code': subject_code,
                        'type': 'materials',
                        'major_code': major_code,
                        'material_count': len(materials),
                        'metadata': {
                            'major_code': major_code,
                            'subject_code': subject_code,
                            'type': 'materials',
                            'count': len(materials),
                            'main_count': len(main_materials),
                            'search_keywords': f"nganh {major_code} {subject_code} tai lieu materials textbook"
                        }
                    })
                
                # 4. ASSESSMENTS PROCESSING v·ªõi Special Notes Enhancement
                if 'assessments' in syllabus:
                    assessments = syllabus['assessments']
                    
                    assessment_content = f"""
Danh gia mon {subject_code} - Nganh {major_code} ({len(assessments)} loai):
"""
                    total_weight = 0
                    for assess in assessments:
                        weight = float(assess.get('weight', 0))
                        total_weight += weight
                        assessment_content += f"- {assess.get('category', '')}: {weight}% ({assess.get('type', '')})\n"
                        if assess.get('clos'):
                            assessment_content += f"  CLO lien quan: {', '.join(assess.get('clos', []))}\n"
                    
                    assessment_content += f"Tong trong so: {total_weight}%"
                    
                    processed_data.append({
                        'content': assessment_content,
                        'subject_code': subject_code,
                        'type': 'assessments',
                        'major_code': major_code,
                        'assessment_count': len(assessments),
                        'total_weight': total_weight,
                        'metadata': {
                            'major_code': major_code,
                            'subject_code': subject_code,
                            'type': 'assessments',
                            'count': len(assessments),
                            'total_weight': total_weight,
                            'search_keywords': f"nganh {major_code} {subject_code} danh gia assessment exam test weight"
                        }
                    })
                    
                    # SPECIAL NOTES PROCESSING - Extract bonus paper v√† c√°c note ƒë·∫∑c bi·ªát
                    # Check both syllabus-level v√† assessment-level completion criteria v√† notes
                    completion_criteria = syllabus.get('completion_criteria', '')
                    note_content = syllabus.get('note', '')
                    
                    # Also check individual assessments for completion_criteria (especially final exam)
                    assessment_criteria = ""
                    for assess in assessments:
                        assess_criteria = assess.get('completion_criteria', '')
                        if assess_criteria:
                            assessment_criteria += f" {assess_criteria}"
                    
                    # Combine all sources ƒë·ªÉ t√¨m special features
                    combined_special_text = f"{completion_criteria}\n{note_content}\n{assessment_criteria}".lower()
                    
                    special_features = []
                    special_content = ""
                    
                    # Detect Bonus Paper Scoring
                    if 'bonus' in combined_special_text and 'paper' in combined_special_text:
                        if 'scopus' in combined_special_text or 'isi' in combined_special_text:
                            special_features.append('bonus_paper_scoring')
                            
                            # Extract the bonus scoring section t·ª´ t·∫•t c·∫£ ngu·ªìn
                            bonus_section = ""
                            if 'bonus score for accepted paper' in combined_special_text:
                                # Check trong completion_criteria
                                all_text_sources = [completion_criteria, note_content, assessment_criteria]
                                for text_source in all_text_sources:
                                    if 'bonus score for accepted paper' in text_source.lower():
                                        lines = text_source.split('\n')
                                        in_bonus_section = False
                                        for line in lines:
                                            if 'bonus score for accepted paper' in line.lower():
                                                in_bonus_section = True
                                            if in_bonus_section:
                                                bonus_section += line + "\n"
                                                if 'source to check' in line.lower():
                                                    break
                                        if bonus_section.strip():
                                            break
                            
                            special_content += f"""
MON {subject_code} CO DIEM THUONG PAPER KHOA HOC:

{bonus_section}
"""
                    
                    # Detect Project-based Assessment
                    if 'capstone' in combined_special_text or 'project' in combined_special_text:
                        if 'oral presentation' in combined_special_text or 'final presentation' in combined_special_text:
                            special_features.append('capstone_project_assessment')
                            special_content += f"Mon {subject_code}: Co capstone project voi oral presentation\n"
                    
                    # Detect MOOC Requirements
                    if 'mooc' in combined_special_text or 'specialization' in combined_special_text:
                        special_features.append('mooc_required')
                        special_content += f"Mon {subject_code}: Yeu cau hoan thanh MOOC/certification\n"
                    
                    # Detect Special Language Requirements
                    if any(lang in combined_special_text for lang in ['korean', 'japanese', 'kor101', 'jpd']):
                        special_features.append('language_requirement')
                        special_content += f"Mon {subject_code}: Co yeu cau ngoai ngu dac biet\n"
                    
                    # Create special features chunk if any special features found
                    if special_features and special_content.strip():
                        processed_data.append({
                            'content': special_content,
                            'subject_code': subject_code,
                            'type': 'special_features',
                            'major_code': major_code,
                            'special_features': special_features,
                            'has_bonus_paper': 'bonus_paper_scoring' in special_features,
                            'has_capstone': 'capstone_project_assessment' in special_features,
                            'has_mooc': 'mooc_required' in special_features,
                            'has_language_req': 'language_requirement' in special_features,
                            'metadata': {
                                'major_code': major_code,
                                'subject_code': subject_code,
                                'type': 'special_features',
                                'special_features': special_features,
                                'has_bonus_paper': 'bonus_paper_scoring' in special_features,
                                'has_capstone': 'capstone_project_assessment' in special_features,
                                'has_mooc': 'mooc_required' in special_features,
                                'has_language_req': 'language_requirement' in special_features,
                                'search_keywords': f"nganh {major_code} {subject_code} diem thuong bonus paper scopus isi special features {' '.join(special_features)}"
                            }
                        })
                
                # 5. SCHEDULE SUMMARY (sample sessions)
                if 'schedule' in syllabus:
                    schedule = syllabus['schedule']
                    schedule_sample = schedule[:3]  # First 3 sessions as sample
                    
                    schedule_content = f"""
Lich hoc mon {subject_code} - Nganh {major_code} (Tong {len(schedule)} buoi hoc):
Cac buoi hoc dau:
"""
                    for i, session in enumerate(schedule_sample, 1):
                        schedule_content += f"Buoi {i}: {session.get('session', '')[:100]}...\n"
                        schedule_content += f"  CLO: {session.get('teaching_type', '')}\n"
                    
                    if len(schedule) > 3:
                        schedule_content += f"... va {len(schedule)-3} buoi hoc khac"
                    
                    processed_data.append({
                        'content': schedule_content,
                        'subject_code': subject_code,
                        'type': 'schedule',
                        'major_code': major_code,
                        'session_count': len(schedule),
                        'metadata': {
                            'major_code': major_code,
                            'subject_code': subject_code,
                            'type': 'schedule',
                            'session_count': len(schedule),
                            'search_keywords': f"nganh {major_code} {subject_code} lich hoc schedule session buoi hoc"
                        }
                    })
        
        # STUDENT DATA PROCESSING
        if 'students' in raw_data and raw_data['students']:
            students = raw_data['students']
            major_code = raw_data.get('major_code_input', 'UNKNOWN')
            
            # Create student overview
            student_overview = f"""
Danh sach sinh vien nganh {major_code}:
Tong so sinh vien: {len(students)} sinh vien
"""
            
            # Group students by major for overview
            major_groups = {}
            for student in students:
                student_major = student.get('Major', 'Unknown')
                if student_major not in major_groups:
                    major_groups[student_major] = []
                major_groups[student_major].append(student)
            
            # Add major distribution to overview
            for major, student_list in major_groups.items():
                student_overview += f"Nganh {major}: {len(student_list)} sinh vien\n"
            
            # Add sample student list
            student_overview += "\nDanh sach sinh vien:\n"
            for student in students[:10]:  # First 10 students as sample
                student_overview += f"- {student.get('RollNumber', '')}: {student.get('Fullname', '')} ({student.get('Major', '')})\n"
            
            if len(students) > 10:
                student_overview += f"... va {len(students)-10} sinh vien khac"
            
            processed_data.append({
                'content': student_overview,
                'subject_code': 'STUDENT_LIST',
                'type': 'student_overview',
                'major_code': major_code,
                'student_count': len(students),
                'metadata': {
                    'major_code': major_code,
                    'type': 'student_overview',
                    'student_count': len(students),
                    'majors': list(major_groups.keys()),
                    'search_keywords': f"sinh vien {major_code} danh sach student list roll number"
                }
            })
            
            # Individual student records
            for student in students:
                student_detail = f"""
Thong tin sinh vien:
Ma sinh vien: {student.get('RollNumber', '')}
Ho ten: {student.get('Fullname', '')}
Email: {student.get('Email', '')}
Nganh: {student.get('Major', '')}
Ho: {student.get('LastName', '')}
Ten dem: {student.get('MiddleName', '')}
Ten: {student.get('FirstName', '')}
"""
                
                processed_data.append({
                    'content': student_detail,
                    'subject_code': 'STUDENT_DETAIL',
                    'type': 'student_detail',
                    'major_code': student.get('Major', ''),
                    'roll_number': student.get('RollNumber', ''),
                    'student_name': student.get('Fullname', ''),
                    'metadata': {
                        'major_code': student.get('Major', ''),
                        'roll_number': student.get('RollNumber', ''),
                        'student_name': student.get('Fullname', ''),
                        'email': student.get('Email', ''),
                        'type': 'student_detail',
                        'search_keywords': f"sinh vien {student.get('RollNumber', '')} {student.get('Fullname', '')} {student.get('Major', '')} student"
                    }
                })
        
        return processed_data

    def _create_embeddings(self):
        logger.info("ƒêang t·∫°o embeddings...")
        contents = [item['content'] for item in self.data]
        self.embeddings = self.embedding_model.encode(contents)

    def _build_index(self):
        logger.info("ƒêang x√¢y d·ª±ng FAISS index...")
        embeddings_np = np.array(self.embeddings).astype('float32')
        self.index = faiss.IndexFlatIP(embeddings_np.shape[1])
        self.index.add(embeddings_np)

    def query(self, question: str, max_results: int = 10) -> Dict[str, Any]:
        start_time = time.time()
        logger.info(f"========== B·∫ÆT ƒê·∫¶U X·ª¨ L√ù TRUY V·∫§N ==========")
        logger.info(f"USER QUERY: '{question}'")
        
        if not self.is_initialized:
            logger.error("Engine ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            raise RuntimeError("Engine ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        # Check for quick response first
        logger.info("Ki·ªÉm tra quick response patterns...")
        quick_response = self.query_router.check_quick_response(question)
        if quick_response:
            logger.info("‚úì T√¨m th·∫•y quick response pattern")
            logger.info(f"QUICK RESPONSE: {quick_response[:100]}...")
            processing_time = time.time() - start_time
            logger.info(f"========== K·∫æT TH√öC X·ª¨ L√ù (Quick Response) - Th·ªùi gian: {processing_time:.2f}s ==========")
            return {
                'question': question,
                'answer': quick_response,
                'search_results': [],
                'is_quick_response': True
            }
        
        logger.info("Kh√¥ng t√¨m th·∫•y quick response, ti·∫øp t·ª•c x·ª≠ l√Ω...")
        
        # Expand query
        logger.info("M·ªü r·ªông truy v·∫•n...")
        expanded_query = self._expand_query(question)
        if expanded_query != question:
            logger.info(f"EXPANDED QUERY: '{expanded_query}'")
        
        # Analyze query intent
        logger.info("Ph√¢n t√≠ch √Ω ƒë·ªãnh truy v·∫•n...")
        intent = self.query_router.analyze_query(expanded_query)
        logger.info(f"QUERY INTENT:")
        logger.info(f"  - Type: {intent.query_type}")
        logger.info(f"  - Scope: {intent.subject_scope}")
        logger.info(f"  - Complexity: {intent.complexity}")
        logger.info(f"  - Requires summarization: {intent.requires_summarization}")
        if intent.target_subjects:
            logger.info(f"  - Target subjects: {intent.target_subjects}")
        else:
            logger.info(f"  - Target subjects: None")
        
        # Search strategy
        logger.info("Th·ª±c hi·ªán search strategy...")
        search_results = self._search_strategy(expanded_query, intent)
        logger.info(f"SEARCH RESULTS: T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£")
        
        # Log chi ti·∫øt t·ª´ng k·∫øt qu·∫£
        for i, result in enumerate(search_results[:5]):
            final_score = result.get('final_score', result.get('score', 0))
            search_method = result.get('search_method', 'unknown')
            content_preview = result.get('content', '')[:100].replace('\n', ' ')
            
            logger.info(f"  [{i+1}] {result.get('subject_code', 'N/A')}:")
            logger.info(f"      Type: {result.get('type', 'N/A')}")
            logger.info(f"      Score: {final_score:.4f}")
            logger.info(f"      Method: {search_method}")
            logger.info(f"      Content: {content_preview}...")
            
            # Log metadata n·∫øu c√≥
            metadata = result.get('metadata', {})
            if metadata:
                logger.info(f"      Metadata: {dict(list(metadata.items())[:3])}...")  # Ch·ªâ hi·ªÉn th·ªã 3 fields ƒë·∫ßu
        
        # Prepare context
        logger.info("Chu·∫©n b·ªã context...")
        context = self._prepare_context(search_results)
        context_length = len(context)
        logger.info(f"CONTEXT PREPARATION:")
        logger.info(f"  - Total length: {context_length} k√Ω t·ª±")
        
        # Count sections (cannot use backslash in f-string)
        context_lines = context.split('\n')
        section_count = len([line for line in context_lines if line.startswith('**')])
        logger.info(f"  - Number of sections: {section_count}")
        
        # Log preview c·ªßa context
        context_lines = context.split('\n')
        logger.info(f"CONTEXT PREVIEW (first 10 lines):")
        for i, line in enumerate(context_lines[:10]):
            if line.strip():
                logger.info(f"  {i+1}: {line[:80]}{'...' if len(line) > 80 else ''}")
        
        if len(context_lines) > 10:
            logger.info(f"  ... v√† {len(context_lines) - 10} d√≤ng kh√°c")
        
        # Generate response
        logger.info("G·ªçi Gemini ƒë·ªÉ t·∫°o ph·∫£n h·ªìi...")
        response = self._generate_response(question, context)
        response_length = len(response)
        logger.info(f"GEMINI RESPONSE: ƒê·ªô d√†i {response_length} k√Ω t·ª±")
        logger.info(f"RESPONSE PREVIEW: {response[:200]}...")
        
        processing_time = time.time() - start_time
        logger.info(f"========== K·∫æT TH√öC X·ª¨ L√ù - Th·ªùi gian: {processing_time:.2f}s ==========")
        
        return {
            'question': question,
            'answer': response,
            'search_results': search_results[:5],
            'is_quick_response': False
        }

    def query_with_multihop(self, question: str, enable_multihop: bool = True, max_results: int = 10) -> Dict[str, Any]:
        """
        Th·ª±c hi·ªán truy v·∫•n v·ªõi kh·∫£ nƒÉng multi-hop (truy v·∫•n k√©p)
        
        Args:
            question: C√¢u h·ªèi g·ªëc
            enable_multihop: B·∫≠t/t·∫Øt t√≠nh nƒÉng multi-hop
            max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ chi ti·∫øt c·ªßa chu·ªói truy v·∫•n
        """
        start_time = time.time()
        logger.info(f"========== B·∫ÆT ƒê·∫¶U X·ª¨ L√ù MULTI-HOP QUERY ==========")
        logger.info(f"USER QUERY: '{question}'")
        logger.info(f"MULTI-HOP ENABLED: {enable_multihop}")
        
        if not self.is_initialized:
            logger.error("Engine ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            raise RuntimeError("Engine ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        # Check for quick response first (no need for multi-hop)
        logger.info("Ki·ªÉm tra quick response patterns...")
        quick_response = self.query_router.check_quick_response(question)
        if quick_response:
            logger.info("‚úì T√¨m th·∫•y quick response pattern - b·ªè qua multi-hop")
            logger.info(f"QUICK RESPONSE: {quick_response[:100]}...")
            processing_time = time.time() - start_time
            logger.info(f"========== K·∫æT TH√öC X·ª¨ L√ù (Quick Response) - Th·ªùi gian: {processing_time:.2f}s ==========")
            return {
                'question': question,
                'original_answer': quick_response,
                'final_answer': quick_response,
                'followup_queries': [],
                'followup_results': [],
                'execution_path': ['Quick response - kh√¥ng c·∫ßn t√¨m ki·∫øm database'],
                'multihop_enabled': False,
                'has_followup': False,
                'is_quick_response': True
            }
        
        if not self.query_chain:
            logger.warning("QueryChain kh√¥ng kh·∫£ d·ª•ng - fallback to normal query")
            # Fallback to normal query if QueryChain not available
            normal_result = self.query(question, max_results)
            processing_time = time.time() - start_time
            logger.info(f"========== K·∫æT TH√öC X·ª¨ L√ù (Fallback) - Th·ªùi gian: {processing_time:.2f}s ==========")
            return {
                'question': question,
                'original_answer': normal_result['answer'],
                'final_answer': normal_result['answer'],
                'followup_queries': [],
                'followup_results': [],
                'execution_path': ['Fallback to normal query'],
                'multihop_enabled': False,
                'has_followup': False,
                'is_quick_response': normal_result.get('is_quick_response', False)
            }
        
        # Th·ª±c hi·ªán truy v·∫•n chu·ªói
        logger.info("B·∫Øt ƒë·∫ßu th·ª±c hi·ªán query chain...")
        chain_result = self.query_chain.execute_query_chain(question, enable_multihop)
        
        logger.info(f"FOLLOWUP QUERIES: {len(chain_result.followup_queries)} truy v·∫•n")
        for i, fq in enumerate(chain_result.followup_queries):
            logger.info(f"  Followup {i+1}: '{fq.query}' (confidence: {fq.confidence:.2f})")
        
        processing_time = time.time() - start_time
        logger.info(f"FINAL ANSWER LENGTH: {len(chain_result.final_integrated_answer)} k√Ω t·ª±")
        logger.info(f"FINAL ANSWER PREVIEW: {chain_result.final_integrated_answer[:200]}...")
        logger.info(f"========== K·∫æT TH√öC X·ª¨ L√ù MULTI-HOP - Th·ªùi gian: {processing_time:.2f}s ==========")
        
        return {
            'question': question,
            'original_answer': chain_result.original_answer,
            'final_answer': chain_result.final_integrated_answer,
            'followup_queries': [
                {
                    'query': fq.query,
                    'confidence': fq.confidence,
                    'type': fq.query_type,
                    'source': fq.source_info
                } for fq in chain_result.followup_queries
            ],
            'followup_results': chain_result.followup_results,
            'execution_path': chain_result.execution_path,
            'multihop_enabled': enable_multihop,
            'has_followup': len(chain_result.followup_queries) > 0,
            'is_quick_response': False
        }

    def _expand_query(self, query: str) -> str:
        """Expand query with synonyms and multilingual terms for better matching"""
        
        # Define common synonym mappings
        query_synonyms = {
            # Vietnamese-English mappings  
            't√≠n ch·ªâ': ['credit', 'credits'],
            'tin chi': ['credit', 'credits'], 
            'bao nhi√™u': ['how many', 's·ªë l∆∞·ª£ng'],
            'c√≥ bao nhi√™u': ['how many', 's·ªë l∆∞·ª£ng'],
            'm√¥n h·ªçc': ['subject', 'course'],
            'mon hoc': ['subject', 'course'],
            'ng√†nh': ['major', 'program'],
            'nganh': ['major', 'program'],
            'k·ª≥ h·ªçc': ['semester', 'term'],
            'ky hoc': ['semester', 'term'],
            'ƒë√°nh gi√°': ['assessment', 'evaluation'],
            'danh gia': ['assessment', 'evaluation'],
            't√†i li·ªáu': ['material', 'resource'],
            'tai lieu': ['material', 'resource'],
            'l·ªãch h·ªçc': ['schedule', 'timetable'],
            'lich hoc': ['schedule', 'timetable']
        }
        
        expanded_query = query
        query_lower = query.lower()
        
        # Add synonyms to query
        for vn_term, en_terms in query_synonyms.items():
            if vn_term in query_lower:
                # Add English equivalents
                for en_term in en_terms:
                    if en_term not in query_lower:
                        expanded_query += f" {en_term}"
        
        return expanded_query

    def _search_strategy(self, query: str, intent: QueryIntent) -> List[Dict[str, Any]]:
        """Enhanced search strategy v·ªõi intelligent routing"""
        search_start_time = time.time()
        logger.info(f"SEARCH STRATEGY: B·∫Øt ƒë·∫ßu t√¨m ki·∫øm")
        logger.info(f"  Query: '{query}'")
        logger.info(f"  Intent: {intent.query_type} / {intent.subject_scope} / {intent.complexity}")
        logger.info(f"  Target subjects: {intent.target_subjects}")
        
        # Log search configuration
        config = self._get_search_config(intent, query.lower())
        logger.info(f"SEARCH CONFIG:")
        logger.info(f"  - Max results: {config['max_results']}")
        logger.info(f"  - Content types: {config['content_types']}")
        logger.info(f"  - Boost factors: {config['boost_factors']}")
        
        # Log special configurations
        if config.get('force_all_combos'):
            logger.info(f"  - SPECIAL: Force all combos enabled")
        if config.get('coursera_boost'):
            logger.info(f"  - SPECIAL: Coursera boost enabled")
        if config.get('smart_filter_semester'):
            logger.info(f"  - SMART FILTER: semester={config['smart_filter_semester']}, suffix='{config.get('smart_filter_suffix', '')}'")
        if config.get('force_specific_student'):
            logger.info(f"  - STUDENT FILTER: targeting {config['force_specific_student']}")
        if config.get('force_student_overview'):
            logger.info(f"  - STUDENT FILTER: prioritizing overview")
        
        results = []
        
        # PRIORITY CHECK: For semester/major queries, ensure major_overview is found
        query_lower = query.lower()
        is_semester_query = any(term in query_lower for term in ['k·ª≥', 'ky', 'k√¨', 'ki', 'semester', 'h·ªçc k·ª≥', 'hoc ky'])
        is_major_query = any(term in query_lower for term in ['ng√†nh', 'nganh', 'major', 'ch∆∞∆°ng tr√¨nh'])
        
        # Get search configuration
        config = self._get_search_config(intent, query_lower)
        
        # STEP 0: FORCE ALL COMBOS for listing combo queries
        if config.get('force_all_combos', False):
            all_combo_items = [item for item in self.data if item.get('type') == 'combo_specialization']
            
            if all_combo_items:
                print(f"FORCED all combos: {len(all_combo_items)} items")
                query_embedding = self.embedding_model.encode([query])
                
                for item in all_combo_items:
                    # Find original index
                    original_idx = None
                    for i, orig_item in enumerate(self.data):
                        if orig_item is item:
                            original_idx = i
                            break
                    
                    if original_idx is not None:
                        item_embedding = self.embeddings[original_idx:original_idx+1]
                        
                        # Calculate inner product (to match FAISS)
                        score = float(np.dot(query_embedding[0], item_embedding[0]))
                        
                        # Force very high scores for all combos
                        score *= config['boost_factors'].get('combo_specialization', 15.0)
                        score += 50.0  # Base boost to ensure all combos appear
                        
                        results.append({
                            'content': item['content'],
                            'subject_code': item['subject_code'],
                            'type': item['type'],
                            'score': score,
                            'metadata': item.get('metadata', {}),
                            'search_method': 'forced_all_combos'
                        })
                        
                        print(f"FORCED combo {item['subject_code']} with score: {score:.4f}")
        
        # STEP 1: FORCE MAJOR_OVERVIEW for semester/major queries
        if (is_semester_query or is_major_query) and 'major_overview' in config['content_types']:
            major_overview_items = [item for item in self.data if item.get('type') == 'major_overview']
            
            if major_overview_items:
                # Calculate similarity for major_overview
                query_embedding = self.embedding_model.encode([query])
                
                for item in major_overview_items:
                    # Find original index
                    original_idx = None
                    for i, orig_item in enumerate(self.data):
                        if orig_item is item:
                            original_idx = i
                            break
                    
                    if original_idx is not None:
                        item_embedding = self.embeddings[original_idx:original_idx+1]
                        
                        # Calculate inner product (to match FAISS)
                        score = float(np.dot(query_embedding[0], item_embedding[0]))
                        
                        # Apply boost factor
                        if 'major_overview' in config['boost_factors']:
                            score *= config['boost_factors']['major_overview']
                        
                        # Force high priority for semester queries
                        if is_semester_query:
                            score *= 2.0  # Extra boost for semester queries
                        
                        results.append({
                            'content': item['content'],
                            'subject_code': item['subject_code'],
                            'type': item['type'],
                            'score': score,
                            'metadata': item.get('metadata', {}),
                            'search_method': 'forced_major_overview'
                        })
                        
                        print(f"FORCED major_overview with score: {score:.4f}")
        
        # STEP 2: Subject-specific search (if subjects detected)
        if intent.target_subjects:
            logger.info(f"STEP 2: Subject-specific search")
            logger.info(f"  Raw target subjects: {intent.target_subjects}")
            
            # Enhanced subject code resolution - handle partial codes
            resolved_subjects = []
            
            for subject_code in intent.target_subjects:
                # Check if it's a full subject code (has numbers)
                if re.match(r'[A-Z]{2,4}\d{3}[a-z]*', subject_code):
                    resolved_subjects.append(subject_code)
                # Check if it's a combo code (like AI17_COM2.1)
                elif re.match(r'[A-Z]{2,4}\d{2}_[A-Z]{3}\d+(?:\.\d+)?', subject_code):
                    # Convert to internal combo format
                    combo_subject = f"COMBO_{subject_code}"
                    resolved_subjects.append(combo_subject)
                    print(f"Resolved combo '{subject_code}' to: {combo_subject}")
                else:
                    # Partial subject code - find matches in data
                    partial_matches = []
                    for item in self.data:
                        item_subject = item.get('subject_code', '')
                        if subject_code.upper() in item_subject.upper():
                            partial_matches.append(item_subject)
                    
                    # Add unique matches
                    unique_matches = list(set(partial_matches))
                    resolved_subjects.extend(unique_matches)
                    
                    if unique_matches:
                        print(f"Resolved '{subject_code}' to: {unique_matches}")
            
            logger.info(f"  Resolved subjects: {resolved_subjects}")
            
            if resolved_subjects:
                subject_results = self._search_by_subject(resolved_subjects, config)
                logger.info(f"  Subject search returned: {len(subject_results)} results")
                results.extend(subject_results)
            else:
                logger.info(f"  No subjects could be resolved")
        
        # STEP 3: Content type search
        logger.info(f"STEP 3: Content type search")
        content_type_results = self._search_by_content_type(query, config)
        logger.info(f"  Content type search returned: {len(content_type_results)} results")
        results.extend(content_type_results)
        
        # STEP 4: General semantic search as fallback
        if len(results) < config['max_results']:
            logger.info(f"STEP 4: General semantic search (fallback)")
            remaining_slots = config['max_results'] - len(results)
            logger.info(f"  Need {remaining_slots} more results")
            fallback_config = config.copy()
            fallback_config['max_results'] = remaining_slots
            
            semantic_results = self._semantic_search(query, fallback_config)
            logger.info(f"  Semantic search returned: {len(semantic_results)} results")
            results.extend(semantic_results)
        else:
            logger.info(f"STEP 4: Skipped (already have {len(results)} results)")
        
        # STEP 5: Remove duplicates
        logger.info(f"STEP 5: Remove duplicates")
        original_count = len(results)
        results = self._deduplicate_results(results)
        logger.info(f"  Before: {original_count} results, After: {len(results)} results")
        logger.info(f"  Removed {original_count - len(results)} duplicates")
        
        # STEP 5.5: SMART PATTERN FILTERING - Add all matching courses if pattern detected
        if config.get('smart_filter_semester') and config.get('smart_filter_suffix'):
            target_semester = config['smart_filter_semester']
            target_suffix = config['smart_filter_suffix']
            print(f"STRATEGY PATTERN FILTER: semester={target_semester} + suffix='{target_suffix}'")
            
            # Find ALL courses matching the pattern in the data
            pattern_matches = []
            for item in self.data:
                if item.get('type') == 'general_info':  # Only general_info for courses
                    metadata = item.get('metadata', {})
                    subject_code = item.get('subject_code', '')
                    
                    is_target_semester = metadata.get('semester_from_curriculum') == target_semester
                    is_coursera = metadata.get('course_type_guess', '').startswith('coursera')
                    has_target_suffix = subject_code.endswith(target_suffix) and len(subject_code) > 1
                    
                    if is_target_semester and is_coursera and has_target_suffix:
                        # Check if already in results
                        already_included = any(r.get('subject_code') == subject_code for r in results)
                        if not already_included:
                            pattern_matches.append({
                                'content': item['content'],
                                'subject_code': item['subject_code'],
                                'type': item['type'],
                                'score': 30.0,  # High score for pattern match
                                'metadata': item.get('metadata', {}),
                                'search_method': 'strategy_pattern_matched'
                            })
                            print(f"STRATEGY PATTERN MATCHED & ADDED: {subject_code} (semester {target_semester} + coursera + ƒëu√¥i '{target_suffix}')")
            
            # Add pattern matches to results
            results.extend(pattern_matches)
        
        # STEP 6: Advanced ranking
        logger.info(f"STEP 6: Advanced ranking")
        pre_ranking_scores = [r.get('score', 0) for r in results[:5]]
        logger.info(f"  Pre-ranking top 5 scores: {[round(s, 2) for s in pre_ranking_scores]}")
        
        results = self._rank_results(results, query, intent)
        
        post_ranking_scores = [r.get('final_score', r.get('score', 0)) for r in results[:5]]
        logger.info(f"  Post-ranking top 5 scores: {[round(s, 2) for s in post_ranking_scores]}")
        
        search_time = time.time() - search_start_time
        logger.info(f"SEARCH STRATEGY: Ho√†n th√†nh trong {search_time:.2f}s")
        logger.info(f"  Final results: {len(results)} k·∫øt qu·∫£")
        logger.info(f"  Top 5 scores: {[round(r.get('final_score', r.get('score', 0)), 2) for r in results[:5]]}")
        
        # Log search method distribution
        method_counts = {}
        for r in results:
            method = r.get('search_method', 'unknown')
            method_counts[method] = method_counts.get(method, 0) + 1
        
        logger.info(f"  Search methods used: {method_counts}")
        
        # Log content type distribution
        type_counts = {}
        for r in results:
            content_type = r.get('type', 'unknown')
            type_counts[content_type] = type_counts.get(content_type, 0) + 1
        
        logger.info(f"  Content types found: {type_counts}")
        
        final_results = results[:config['max_results']]
        logger.info(f"  Returning top {len(final_results)} results")
        
        return final_results
    
    def hybrid_graph_query(self, question: str, max_results: int = 10) -> Dict[str, Any]:
        """
        Hybrid GraphRAG query: K·∫øt h·ª£p Vector Search v√† Graph Traversal
        Implements the GraphRAG architecture from the research paper
        """
        if not self.graph_enabled:
            logger.info("Graph kh√¥ng enabled - fallback to regular query")
            return self.query(question, max_results)
        
        start_time = time.time()
        logger.info(f"========== HYBRID GRAPHRAG QUERY ==========")
        logger.info(f"USER QUERY: '{question}'")
        
        try:
            # STEP 1: Regular vector search for semantic similarity
            logger.info("STEP 1: Vector Search for semantic similarity...")
            vector_results = self.query(question, max_results)
            vector_search_results = vector_results.get('search_results', [])
            
            # STEP 2: Extract entities from query for graph traversal
            logger.info("STEP 2: Entity extraction for graph traversal...")
            extracted_entities = self._extract_entities_from_query(question)
            
            # STEP 3: Graph traversal for relationship discovery
            graph_results = []
            if extracted_entities and hasattr(self, 'graph_entities'):
                logger.info("STEP 3: Graph traversal...")
                graph_results = self._perform_graph_traversal(extracted_entities, question)
            
            # STEP 4: Hybrid result integration
            logger.info("STEP 4: Integrating vector and graph results...")
            integrated_results = self._integrate_vector_graph_results(
                vector_search_results, graph_results, question
            )
            
            # STEP 5: Generate enhanced answer with both semantic and relational context
            logger.info("STEP 5: Generating enhanced answer...")
            enhanced_context = self._prepare_hybrid_context(integrated_results)
            enhanced_answer = self._generate_hybrid_response(question, enhanced_context, vector_results.get('answer', ''))
            
            processing_time = time.time() - start_time
            
            result = {
                'answer': enhanced_answer,
                'search_results': integrated_results,
                'metadata': {
                    'query_type': 'hybrid_graphrag',
                    'vector_results_count': len(vector_search_results),
                    'graph_results_count': len(graph_results),
                    'total_results': len(integrated_results),
                    'processing_time': processing_time,
                    'graph_enabled': True,
                    'entities_extracted': extracted_entities
                },
                'graph_info': {
                    'entities_found': extracted_entities,
                    'graph_traversal_performed': len(graph_results) > 0,
                    'relationship_paths': len(graph_results)
                }
            }
            
            logger.info(f"HYBRID GraphRAG SUMMARY:")
            logger.info(f"  - Vector results: {len(vector_search_results)}")
            logger.info(f"  - Graph results: {len(graph_results)}")
            logger.info(f"  - Integrated results: {len(integrated_results)}")
            logger.info(f"  - Entities extracted: {extracted_entities}")
            logger.info(f"  - Processing time: {processing_time:.2f}s")
            logger.info(f"========== HYBRID GRAPHRAG COMPLETED ==========")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói hybrid graph query: {e}")
            # Fallback to regular vector search
            logger.info("Fallback to regular vector search...")
            return self.query(question, max_results)
    
    def _extract_entities_from_query(self, query: str) -> List[str]:
        """Extract course codes v√† entities t·ª´ user query"""
        entities = []
        
        # Extract course codes (e.g., CSI106, MAD101, etc.)
        course_pattern = r'\b([A-Z]{2,4}\d{3}[a-z]*)\b'
        course_matches = re.findall(course_pattern, query, re.IGNORECASE)
        entities.extend([code.upper() for code in course_matches])
        
        # Extract semester numbers
        semester_pattern = r'k√¨\s*(\d+)|ky\s*(\d+)|semester\s*(\d+)'
        semester_matches = re.findall(semester_pattern, query, re.IGNORECASE)
        for match in semester_matches:
            semester_num = next((num for num in match if num), None)
            if semester_num:
                entities.append(f"Semester_{semester_num}")
        
        # Extract combo patterns
        if 'combo' in query.lower() or 'chuy√™n ng√†nh' in query.lower():
            entities.append("COMBO_ENTITY")
        
        logger.info(f"Extracted entities from query: {entities}")
        return entities
    
    def _perform_graph_traversal(self, entities: List[str], query: str) -> List[Dict[str, Any]]:
        """Perform graph traversal to find relationships"""
        if not hasattr(self, 'graph_entities'):
            return []
        
        graph_results = []
        nodes = self.graph_entities.get('nodes', [])
        relationships = self.graph_entities.get('relationships', [])
        
        logger.info(f"Graph traversal v·ªõi {len(entities)} entities...")
        
        # Simple graph traversal - find related nodes
        for entity in entities:
            # Find direct matches in nodes
            matching_nodes = [node for node in nodes if node.id == entity]
            
            # Find relationships involving this entity
            related_relationships = [
                rel for rel in relationships 
                if rel.source_id == entity or rel.target_id == entity
            ]
            
            # Build graph results
            for rel in related_relationships:
                # Find the related course/entity
                related_entity_id = rel.target_id if rel.source_id == entity else rel.source_id
                related_node = next((node for node in nodes if node.id == related_entity_id), None)
                
                if related_node:
                    graph_result = {
                        'content': f"Graph relationship: {entity} {rel.type} {related_entity_id}",
                        'subject_code': related_entity_id,
                        'type': 'graph_relationship',
                        'score': 5.0,  # High score for direct relationships
                        'metadata': {
                            'relationship_type': rel.type,
                            'source_entity': entity,
                            'target_entity': related_entity_id,
                            'graph_traversal': True,
                            'node_properties': related_node.properties
                        },
                        'search_method': 'graph_traversal'
                    }
                    graph_results.append(graph_result)
        
        logger.info(f"Graph traversal found {len(graph_results)} relationship results")
        return graph_results
    
    def _integrate_vector_graph_results(self, vector_results: List[Dict], graph_results: List[Dict], query: str) -> List[Dict]:
        """Integrate vector search results with graph traversal results"""
        integrated = []
        
        # Add vector results with marking
        for result in vector_results:
            result['result_source'] = 'vector_search'
            integrated.append(result)
        
        # Add graph results with marking and boost scores
        for result in graph_results:
            result['result_source'] = 'graph_traversal'
            # Boost graph result scores since they represent explicit relationships
            if 'score' in result:
                result['score'] = result['score'] * 1.5  # Boost graph results
            integrated.append(result)
        
        # Remove duplicates based on subject_code and content similarity
        unique_results = []
        seen_combinations = set()
        
        for result in integrated:
            key = (result.get('subject_code', ''), result.get('type', ''))
            if key not in seen_combinations:
                seen_combinations.add(key)
                unique_results.append(result)
        
        # Sort by score (descending)
        unique_results.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        logger.info(f"Integrated results: {len(vector_results)} vector + {len(graph_results)} graph = {len(unique_results)} unique")
        return unique_results
    
    def _prepare_hybrid_context(self, integrated_results: List[Dict]) -> str:
        """Prepare context t·ª´ integrated vector + graph results"""
        if not integrated_results:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."
        
        context = "=== TH√îNG TIN T√åM KI·∫æM ===\n\n"
        
        # Group by result source
        vector_results = [r for r in integrated_results if r.get('result_source') == 'vector_search']
        graph_results = [r for r in integrated_results if r.get('result_source') == 'graph_traversal']
        
        # Add vector search results
        if vector_results:
            context += "üìö TH√îNG TIN N·ªòI DUNG:\n"
            for i, result in enumerate(vector_results[:5], 1):  # Top 5 vector results
                context += f"{i}. {result.get('content', '')}\n\n"
        
        # Add graph relationship results
        if graph_results:
            context += "üîó M·ªêI QUAN H·ªÜ LI√äN K·∫æT:\n"
            for i, result in enumerate(graph_results[:3], 1):  # Top 3 graph results
                rel_type = result.get('metadata', {}).get('relationship_type', 'RELATED')
                source = result.get('metadata', {}).get('source_entity', '')
                target = result.get('metadata', {}).get('target_entity', '')
                context += f"{i}. {source} --[{rel_type}]--> {target}\n"
                context += f"   Chi ti·∫øt: {result.get('content', '')}\n\n"
        
        return context
    
    def _generate_hybrid_response(self, question: str, hybrid_context: str, original_answer: str) -> str:
        """Generate enhanced answer s·ª≠ d·ª•ng c·∫£ vector v√† graph context"""
        try:
            prompt = f"""B·∫°n l√† AI Assistant th√¥ng minh c·ªßa FPTU, chuy√™n ph√¢n t√≠ch th√¥ng tin gi√°o d·ª•c v·ªõi kh·∫£ nƒÉng hi·ªÉu m·ªëi quan h·ªá ph·ª©c t·∫°p.

NG·ªÆ C·∫¢NH TH√îNG TIN (Vector Search + Graph Relationships):
{hybrid_context}

C√ÇU H·ªéI: {question}

C√ÇU TR·∫¢ L·ªúI G·ªêC (Vector-only): {original_answer}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI ENHANCED:
1. K·∫øt h·ª£p th√¥ng tin t·ª´ c·∫£ n·ªôi dung (vector search) v√† m·ªëi quan h·ªá (graph traversal)
2. ∆Øu ti√™n th√¥ng tin c√≥ m·ªëi quan h·ªá r√µ r√†ng t·ª´ graph analysis
3. Gi·∫£i th√≠ch c√°c m·ªëi li√™n k·∫øt v√† ph·ª• thu·ªôc n·∫øu c√≥
4. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán v√† c√≥ c·∫•u tr√∫c
5. Kh√¥ng s·ª≠ d·ª•ng bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch t·ª± nhi√™n v√† chuy√™n nghi·ªáp:"""

            enhanced_answer = self._call_gemini_with_rotation(prompt)
            return enhanced_answer
            
        except Exception as e:
            logger.error(f"L·ªói generate hybrid response: {e}")
            return original_answer  # Fallback to original answer
    
    def _get_search_config(self, intent: QueryIntent, query_lower: str) -> Dict[str, Any]:
        """Get search configuration based on query intent and content"""
        
        # Base configuration
        config = {
            'max_results': 5,
            'content_types': ['general_info', 'learning_outcomes_summary'],
            'boost_factors': {
                'general_info': 2.0,
                'learning_outcomes_summary': 1.5
            }
        }
        
        # Adjust based on query type
        if intent.query_type == 'listing':
            config['max_results'] = 15
            config['content_types'] = ['major_overview', 'combo_specialization', 'general_info', 'learning_outcomes_summary']
            config['boost_factors'] = {
                'major_overview': 10.0,  # Highest priority for listing queries
                'combo_specialization': 8.0,  # High priority for combo queries
                'general_info': 3.0,
                'learning_outcomes_summary': 2.0
            }
        
        elif intent.query_type == 'factual':
            config['content_types'] = ['combo_specialization', 'general_info', 'learning_outcomes_summary', 'materials', 'assessments']
            config['boost_factors'] = {
                'combo_specialization': 10.0,  # High priority for combo queries
                'general_info': 3.0,
                'learning_outcomes_summary': 2.0,
                'materials': 1.8,
                'assessments': 1.5
            }
        
        elif intent.query_type == 'comparative':
            config['max_results'] = 20
            config['content_types'] = ['major_overview', 'general_info', 'learning_outcomes_summary']
            config['boost_factors'] = {
                'major_overview': 5.0,
                'general_info': 3.0,
                'learning_outcomes_summary': 2.0
            }
        
        elif intent.query_type == 'analytical':
            config['max_results'] = 25
            config['content_types'] = ['major_overview', 'general_info', 'learning_outcomes_summary', 'materials', 'assessments', 'schedule']
            config['boost_factors'] = {
                'major_overview': 6.0,
                'general_info': 4.0,
                'learning_outcomes_summary': 3.0,
                'materials': 2.0,
                'assessments': 2.0,
                'schedule': 1.5
            }
        
        # Special cases based on query content
        
        # Special Features Detection - Bonus Paper, Special Assessment, etc.
        special_features_keywords = [
            # Bonus paper keywords
            'ƒëi·ªÉm th∆∞·ªüng', 'diem thuong', 'bonus', 'paper', 'b√†i b√°o', 'bai bao', 'scopus', 'isi',
            'nghi√™n c·ª©u', 'nghien cuu', 'research', 'publication', 'xu·∫•t b·∫£n', 'xuat ban',
            # Project-based keywords
            'capstone', 'd·ª± √°n', 'du an', 'project', 'thuy·∫øt tr√¨nh', 'thuyet trinh', 'presentation',
            # MOOC keywords
            'mooc', 'coursera', 'certification', 'ch·ª©ng ch·ªâ', 'chung chi', 'online',
            # Language requirements
            'ti·∫øng nh·∫≠t', 'tieng nhat', 'ti·∫øng h√†n', 'tieng han', 'japanese', 'korean',
            # Special assessment
            'ƒë·∫∑c bi·ªát', 'dac biet', 'special', 'unique', 'ri√™ng', 'rieng'
        ]
        
        if any(keyword in query_lower for keyword in special_features_keywords):
            # Add special_features to content types with high priority
            if 'special_features' not in config['content_types']:
                config['content_types'].insert(0, 'special_features')
            config['boost_factors']['special_features'] = 20.0  # Very high priority
            config['max_results'] = max(config['max_results'], 10)
            config['include_special_features'] = True
        
        # Combo/specialization queries get highest priority
        if any(keyword in query_lower for keyword in ['combo', 'chuy√™n ng√†nh', 'chuyen nganh', 'specialization', 'track', 'h·∫πp', 'hep']):
            config['content_types'].insert(0, 'combo_specialization')
            config['boost_factors']['combo_specialization'] = 15.0
            config['max_results'] = max(config['max_results'], 10)
            
            # Special handling for listing all combos
            if any(list_term in query_lower for list_term in ['c√°c', 'cac', 't·∫•t c·∫£', 'tat ca', 'all', 'list']):
                config['force_all_combos'] = True
        
        if 'ng√†nh' in query_lower:
            config['content_types'].insert(0, 'major_overview')
            config['boost_factors']['major_overview'] = 8.0
            config['max_results'] = max(config['max_results'], 20)
            
        # Semester/term specific queries - IMPORTANT FIX
        if any(term in query_lower for term in ['k·ª≥', 'ky', 'k√¨', 'ki', 'semester', 'h·ªçc k·ª≥', 'hoc ky']):
            config['content_types'].insert(0, 'major_overview')
            config['boost_factors']['major_overview'] = 15.0  # Very high priority for semester queries
            config['max_results'] = max(config['max_results'], 15)
            
        if any(term in query_lower for term in ['li·ªát k√™', 'danh s√°ch', 'list', 't·∫•t c·∫£', 'tat ca']):
            config['content_types'].insert(0, 'major_overview')
            config['boost_factors']['major_overview'] = 12.0
            config['max_results'] = max(config['max_results'], 15)
        
        if any(term in query_lower for term in ['t√†i li·ªáu', 'materials', 's√°ch', 'book']):
            config['content_types'].extend(['materials'])
            config['boost_factors']['materials'] = 4.0
        
        if any(term in query_lower for term in ['ƒë√°nh gi√°', 'assessment', 'ki·ªÉm tra', 'b√†i t·∫≠p']):
            config['content_types'].extend(['assessments'])
            config['boost_factors']['assessments'] = 4.0
        
        if any(term in query_lower for term in ['l·ªãch h·ªçc', 'schedule', 'tu·∫ßn', 'week']):
            config['content_types'].extend(['schedule'])
            config['boost_factors']['schedule'] = 4.0
        
        # Coursera-specific queries - CRITICAL FIX for missing DWP301c
        if any(term in query_lower for term in ['coursera', 'mooc', 'online course']):
            # Add special handling to prioritize Coursera courses in FAISS search
            config['coursera_boost'] = True  # Flag to boost coursera courses in semantic search
            config['boost_factors']['general_info'] = 5.0  # Higher boost for coursera course info
            config['max_results'] = max(config['max_results'], 15)
            
            # PATTERN-BASED FILTERING: If both coursera + semester mentioned
            semester_match = None
            for i in range(1, 9):
                if any(sem_term in query_lower for sem_term in [f'k·ª≥ {i}', f'ky {i}', f'k√¨ {i}', f'ki {i}', f'semester {i}']):
                    semester_match = i
                    break
            
            if semester_match:
                config['force_semester_coursera_filter'] = semester_match
                print(f"SMART FILTER: Forcing semester {semester_match} + Coursera courses (ƒëu√¥i 'c')")
                config['max_results'] = max(config['max_results'], 20)
                
                # SMART FILTERING: No hardcode, just filter by pattern
                config['smart_filter_semester'] = semester_match
                config['smart_filter_suffix'] = 'c'  # Filter courses ending with 'c'
                print(f"SMART PATTERN: semester={semester_match} + suffix='c' + coursera_type")
        
        # Student queries - SMART PATTERN RECOGNITION
        if any(term in query_lower for term in ['sinh vi√™n', 'sinh vien', 'student', 'h·ªçc sinh', 'hoc sinh', 'danh s√°ch sinh vi√™n', 'm√£ sinh vi√™n', 'ma sinh vien']):
            config['content_types'].extend(['student_overview', 'student_detail'])
            config['boost_factors']['student_overview'] = 20.0  # Higher than major_overview boost
            config['boost_factors']['student_detail'] = 15.0   # High priority for student details
            config['max_results'] = max(config['max_results'], 15)
            
            # PATTERN DETECTION: Student ID format (DE + digits)
            import re
            student_id_pattern = re.search(r'[Dd][Ee]\d{6}', query_lower)
            if student_id_pattern:
                config['force_specific_student'] = student_id_pattern.group().upper()
                print(f"SMART FILTER: Targeting specific student {config['force_specific_student']}")
                config['boost_factors']['student_detail'] = 50.0  # MASSIVE boost for exact student match
            
            # For general student queries, check if asking for list/overview
            elif any(list_term in query_lower for list_term in ['danh s√°ch', 'list', 't·∫•t c·∫£', 'tat ca']):
                config['force_student_overview'] = True
                print(f"SMART FILTER: Prioritizing student overview/listing")
                config['boost_factors']['student_overview'] = 30.0
            
            # Legacy: For specific student queries (with roll numbers), prioritize detail
            elif any(code in query_lower for code in ['de', 'DE']) or any(char.isdigit() for char in query_lower):
                config['boost_factors']['student_detail'] = 25.0  # Highest priority for specific students
                config['boost_factors']['student_overview'] = 10.0
        
        # Remove duplicates while preserving order
        config['content_types'] = list(dict.fromkeys(config['content_types']))
        
        return config
    
    def _search_by_subject(self, subject_codes: List[str], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Search specifically by subject codes with enhanced fallback"""
        results = []
        
        for subject_code in subject_codes:
            # First, try exact match
            subject_items = [item for item in self.data if item['subject_code'] == subject_code]
            print(f"Found {len(subject_items)} items for exact match '{subject_code}'")
            
            # If no exact match and this looks like a full subject code, try partial match
            if len(subject_items) == 0 and len(subject_code) > 4:
                # Extract prefix for fallback search (e.g., "DPL" from "DPL302m")
                prefix_match = re.match(r'([A-Z]{2,4})', subject_code)
                if prefix_match:
                    prefix = prefix_match.group(1)
                    print(f"No exact match for '{subject_code}', trying partial match with prefix '{prefix}'")
                    
                    # Search for items containing the prefix or mentioning the full code
                    partial_items = []
                    for item in self.data:
                        item_content = item.get('content', '').upper()
                        item_subject = item.get('subject_code', '').upper()
                        
                        # Check if item mentions the subject code in content or has similar prefix
                        if (subject_code.upper() in item_content or 
                            (item_subject.startswith(prefix) and item_subject != subject_code.upper()) or
                            (prefix in item_content and 'TI√äN QUY·∫æT' in item_content.upper()) or
                            (prefix in item_content and 'PREREQUISITE' in item_content.upper())):
                            partial_items.append(item)
                    
                    subject_items = partial_items
                    print(f"Found {len(subject_items)} items for partial/content match")
            
            # For subject-specific queries, we want comprehensive info
            # Override config to include all relevant content types
            if len(subject_items) > 0:
                # Check if this is a combo item
                is_combo = subject_items[0].get('type') == 'combo_specialization'
                
                if is_combo:
                    # For combo queries, prioritize combo content
                    relevant_content_types = ['combo_specialization']
                else:
                    relevant_content_types = ['general_info', 'learning_outcomes_summary', 'materials', 'assessments', 'schedule']
                
                # Filter by available content types in config, but prioritize comprehensive coverage
                for content_type in relevant_content_types:
                    filtered_items = [
                        item for item in subject_items 
                        if item['type'] == content_type
                    ]
                    
                    for item in filtered_items:
                        score = 2.0  # Base high score for exact subject match
                        
                        # Lower score if this was a fallback search
                        if item['subject_code'] != subject_code:
                            score *= 0.8  # Slightly lower score for partial matches
                        
                        # Apply content type priorities
                        if content_type == 'combo_specialization':
                            score *= 5.0  # Highest priority for combo content
                        elif content_type == 'general_info':
                            score *= 3.0  # High priority for general info
                        elif content_type == 'learning_outcomes_summary':
                            score *= 2.0
                        elif content_type in ['materials', 'assessments']:
                            score *= 1.5
                        
                        # Apply boost factors from config if available
                        if content_type in config.get('boost_factors', {}):
                            score *= config['boost_factors'][content_type]
                        
                        search_method = 'subject_specific' if item['subject_code'] == subject_code else 'subject_partial_match'
                        
                        result_item = {
                            'content': item['content'],
                            'subject_code': item['subject_code'],
                            'type': item['type'],
                            'score': score,
                            'metadata': item.get('metadata', {}),
                            'search_method': search_method
                        }
                        
                        # Preserve special fields for special_features
                        if item['type'] == 'special_features':
                            result_item['has_bonus_paper'] = item.get('has_bonus_paper', False)
                            result_item['has_mooc'] = item.get('has_mooc', False)
                            result_item['has_special_assessment'] = item.get('has_special_assessment', False)
                        
                        results.append(result_item)
        
        # Sort by score and return top results
        results.sort(key=lambda x: x['score'], reverse=True)
        print(f"Subject search returning {len(results)} results (top types: {[r['type'] for r in results[:5]]})")
        return results
    
    def _search_by_content_type(self, query: str, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Search by content type with semantic similarity"""
        results = []
        query_embedding = self.embedding_model.encode([query])
        
        # Filter data by content types
        filtered_data = [
            item for item in self.data 
            if item['type'] in config['content_types']
        ]
        
        if not filtered_data:
            return results
        
        # Get embeddings for filtered data
        filtered_indices = [i for i, item in enumerate(self.data) if item['type'] in config['content_types']]
        filtered_embeddings = self.embeddings[filtered_indices]
        
        # Build temporary index
        temp_index = faiss.IndexFlatIP(filtered_embeddings.shape[1])
        temp_index.add(filtered_embeddings.astype('float32'))
        
        # Search
        distances, indices = temp_index.search(query_embedding.astype('float32'), min(20, len(filtered_data)))
        
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(filtered_data):
                original_idx = filtered_indices[idx]
                item = self.data[original_idx]
                
                score = float(dist)
                # Apply boost factors
                if item['type'] in config['boost_factors']:
                    score *= config['boost_factors'][item['type']]
                
                result_item = {
                    'content': item['content'],
                    'subject_code': item['subject_code'],
                    'type': item['type'],
                    'score': score,
                    'metadata': item.get('metadata', {}),
                    'search_method': 'content_type_semantic'
                }
                
                # Preserve special fields for special_features
                if item['type'] == 'special_features':
                    result_item['has_bonus_paper'] = item.get('has_bonus_paper', False)
                    result_item['has_mooc'] = item.get('has_mooc', False)
                    result_item['has_special_assessment'] = item.get('has_special_assessment', False)
                
                results.append(result_item)
        
        return results
    
    def _semantic_search(self, query: str, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """General semantic search as fallback"""
        results = []
        query_embedding = self.embedding_model.encode([query])
        
        # Search in main index with more results if coursera query
        search_k = config['max_results'] * 10 if config.get('coursera_boost') else config['max_results']
        distances, indices = self.index.search(query_embedding.astype('float32'), search_k)
        
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.data):
                item = self.data[idx]
                
                # Apply content type filter
                if item['type'] not in config['content_types']:
                    continue
                
                score = float(dist)
                
                # SMART SEMESTER + COURSERA FILTER: Force specific patterns
                if config.get('force_semester_coursera_filter'):
                    target_semester = config['force_semester_coursera_filter']
                    metadata = item.get('metadata', {})
                    subject_code = item.get('subject_code', '')
                    
                    # Check if this matches semester + coursera pattern
                    is_target_semester = metadata.get('semester_from_curriculum') == target_semester
                    is_coursera = metadata.get('course_type_guess', '').startswith('coursera')
                    has_c_suffix = subject_code.endswith('c') and len(subject_code) > 1
                    
                    if is_target_semester and is_coursera and has_c_suffix:
                        score *= 20.0  # MASSIVE boost for perfect pattern match
                        print(f"PERFECT PATTERN MATCH: {subject_code} (semester {target_semester} + coursera + ƒëu√¥i 'c')")
                    elif is_target_semester and is_coursera:
                        score *= 10.0  # High boost for semester + coursera
                        print(f"SEMESTER + COURSERA MATCH: {subject_code}")
                    elif is_coursera and has_c_suffix:
                        score *= 8.0  # Good boost for coursera + suffix
                        print(f"COURSERA + SUFFIX MATCH: {subject_code}")
                
                # SMART STUDENT PATTERN FILTER
                elif config.get('force_specific_student'):
                    target_student = config['force_specific_student']
                    content = item.get('content', '')
                    if target_student in content:
                        score *= 100.0  # MASSIVE boost for exact student match
                        print(f"EXACT STUDENT MATCH: {target_student} found in content")
                
                elif config.get('force_student_overview'):
                    if item.get('type') == 'student_overview':
                        score *= 15.0  # High boost for student overview
                        print(f"STUDENT OVERVIEW BOOST applied")
                
                # COURSERA BOOST: Check if this is a Coursera course
                elif config.get('coursera_boost') and item.get('metadata', {}).get('course_type_guess', '').startswith('coursera'):
                    score *= 5.0  # 5x boost for Coursera courses when coursera query detected
                    print(f"COURSERA BOOST applied to {item.get('subject_code', 'unknown')}: {item.get('metadata', {}).get('course_type_guess', '')}")
                    
                    # Extra boost if also semester 5
                    if item.get('metadata', {}).get('semester_from_curriculum') == 5:
                        score *= 2.0  # Additional 2x boost for semester 5 Coursera courses
                        print(f"SEMESTER 5 + COURSERA BOOST applied to {item.get('subject_code', 'unknown')}")
                
                result_item = {
                    'content': item['content'],
                    'subject_code': item['subject_code'],
                    'type': item['type'],
                    'score': score,
                    'metadata': item.get('metadata', {}),
                    'search_method': 'general_semantic'
                }
                
                # Preserve special fields for special_features
                if item['type'] == 'special_features':
                    result_item['has_bonus_paper'] = item.get('has_bonus_paper', False)
                    result_item['has_mooc'] = item.get('has_mooc', False)
                    result_item['has_special_assessment'] = item.get('has_special_assessment', False)
                
                results.append(result_item)
        
        # Sort by score and return top results
        results.sort(key=lambda x: x['score'], reverse=True)
        
        # SMART PATTERN FILTERING: Add all matching courses if pattern detected
        if config.get('smart_filter_semester') and config.get('smart_filter_suffix'):
            target_semester = config['smart_filter_semester']
            target_suffix = config['smart_filter_suffix']
            
            # Find ALL courses matching the pattern in the data
            pattern_matches = []
            for item in self.data:
                if item.get('type') == 'general_info':  # Only general_info for courses
                    metadata = item.get('metadata', {})
                    subject_code = item.get('subject_code', '')
                    
                    is_target_semester = metadata.get('semester_from_curriculum') == target_semester
                    is_coursera = metadata.get('course_type_guess', '').startswith('coursera')
                    has_target_suffix = subject_code.endswith(target_suffix) and len(subject_code) > 1
                    
                    if is_target_semester and is_coursera and has_target_suffix:
                        # Check if already in results
                        already_included = any(r.get('subject_code') == subject_code for r in results)
                        if not already_included:
                            pattern_matches.append({
                                'content': item['content'],
                                'subject_code': item['subject_code'],
                                'type': item['type'],
                                'score': 25.0,  # High score for pattern match
                                'metadata': item.get('metadata', {}),
                                'search_method': 'pattern_matched'
                            })
                            print(f"PATTERN MATCHED & ADDED: {subject_code} (semester {target_semester} + coursera + ƒëu√¥i '{target_suffix}')")
            
            # Add pattern matches to results
            results.extend(pattern_matches)
            results.sort(key=lambda x: x['score'], reverse=True)
        
        return results[:config['max_results']]
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate results based on content similarity"""
        if not results:
            return results
        
        unique_results = []
        seen_contents = set()
        
        for result in results:
            # Create content hash for deduplication
            content_hash = hash(result['content'][:200])  # Hash first 200 chars
            
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(result)
        
        return unique_results
    
    def _rank_results(self, results: List[Dict[str, Any]], query: str, intent: QueryIntent) -> List[Dict[str, Any]]:
        """Advanced ranking based on multiple factors"""
        
        for result in results:
            final_score = result['score']
            
            # 1. Search method boost
            if result['search_method'] == 'subject_specific':
                final_score *= 1.5
            elif result['search_method'] == 'content_type_semantic':
                final_score *= 1.2
            
            # 2. Keyword matching boost
            query_words = set(query.lower().split())
            content_words = set(result['content'].lower().split())
            keyword_overlap = len(query_words.intersection(content_words)) / len(query_words)
            final_score *= (1 + keyword_overlap * 0.3)
            
            # 3. Metadata relevance boost
            metadata = result.get('metadata', {})
            if 'search_keywords' in metadata:
                keyword_text = metadata['search_keywords'].lower()
                if any(word in keyword_text for word in query.lower().split()):
                    final_score *= 1.3
            
            # 4. Intent-specific boosts
            if intent.query_type == 'listing' and result['type'] == 'general_info':
                final_score *= 1.4
            elif intent.requires_summarization and result['type'].endswith('_summary'):
                final_score *= 1.3
            
            result['final_score'] = final_score
        
        # Sort by final score
        return sorted(results, key=lambda x: x['final_score'], reverse=True)

    def _prepare_context(self, results: List[Dict]) -> str:
        """Intelligent context preparation with compression and summarization"""
        if not results:
            return ""
        
        # 1. GROUP RESULTS BY TYPE AND SUBJECT
        grouped_results = self._group_results(results)
        
        # 2. APPLY CONTEXT COMPRESSION
        compressed_context = self._compress_context(grouped_results)
        
        # 3. ENSURE CONTEXT LENGTH LIMIT
        final_context = self._limit_context_length(compressed_context)
        
        return final_context
    
    def _group_results(self, results: List[Dict]) -> Dict[str, Dict[str, List[Dict]]]:
        """Group results by subject code and content type"""
        grouped = {}
        
        for result in results:
            subject = result.get('subject_code', 'UNKNOWN')
            content_type = result.get('type', 'general')
            
            if subject not in grouped:
                grouped[subject] = {}
            if content_type not in grouped[subject]:
                grouped[subject][content_type] = []
            
            grouped[subject][content_type].append(result)
        
        return grouped
    
    def _compress_context(self, grouped_results: Dict[str, Dict[str, List[Dict]]]) -> str:
        """Compress context based on content type and importance"""
        context_parts = []
        
        # PRIORITY 1: Special Features with Bonus Paper
        special_features_results = []
        bonus_paper_results = []
        for subject_code, content_types in grouped_results.items():
            if 'special_features' in content_types:
                for result in content_types['special_features']:
                    special_features_results.append(result)
                    if result.get('has_bonus_paper', False):
                        bonus_paper_results.append(result)
        
        # If we have bonus paper results, show them prominently
        if bonus_paper_results:
            context_parts.append(f"\n** MON CO DIEM THUONG PAPER KHOA HOC **:")
            for bonus_result in bonus_paper_results:
                subject_code = bonus_result.get('subject_code', '')
                content = bonus_result.get('content', '')
                context_parts.append(f"\n** {subject_code} **: {content}")
            context_parts.append("")  # Empty line
        
        # PRIORITY 2: Check if we have combo specialization (highest priority for combo queries)
        combo_results = []
        for subject_code, content_types in grouped_results.items():
            if 'combo_specialization' in content_types:
                combo_results.extend(content_types['combo_specialization'])
        
        # If we have combo results, prioritize them
        if combo_results:
            context_parts.append(f"\n** COMBO CHUYEN NGANH HEP **:")
            for combo_result in combo_results:
                context_parts.append(f"\n{combo_result['content']}")
        
        # Check if we have major overview
        major_overview_results = []
        for subject_code, content_types in grouped_results.items():
            if 'major_overview' in content_types:
                major_overview_results.extend(content_types['major_overview'])
        
        # If we have major overview, prioritize it
        if major_overview_results:
            best_overview = max(major_overview_results, key=lambda x: x.get('final_score', x.get('score', 0)))
            context_parts.append(f"\n** MAJOR_OVERVIEW **:\n{best_overview['content']}")
            
            # For major queries, we might want to add some supporting general info
            general_info_count = 0
            for subject_code, content_types in grouped_results.items():
                if subject_code != 'MAJOR_OVERVIEW' and 'general_info' in content_types and general_info_count < 3:
                    results = content_types['general_info']
                    best_result = max(results, key=lambda x: x.get('final_score', x.get('score', 0)))
                    context_parts.append(f"\n** {subject_code} **:\n{self._format_general_info(best_result)}")
                    general_info_count += 1
        
        else:
            # Regular processing for non-major queries
            for subject_code, content_types in grouped_results.items():
                if subject_code == 'MAJOR_OVERVIEW':
                    continue
                    
                subject_context = f"\n** {subject_code} **:\n"
                
                # Prioritize content types
                priority_order = [
                    'general_info',
                    'learning_outcomes_summary', 
                    'learning_outcome_detail',
                    'assessments',
                    'materials',
                    'schedule'
                ]
                
                for content_type in priority_order:
                    if content_type in content_types:
                        results = content_types[content_type]
                        
                        if content_type == 'general_info':
                            # For general info, take the best one
                            best_result = max(results, key=lambda x: x.get('final_score', x.get('score', 0)))
                            subject_context += self._format_general_info(best_result)
                            
                        elif content_type == 'learning_outcomes_summary':
                            # Combine CLO summaries
                            best_result = max(results, key=lambda x: x.get('final_score', x.get('score', 0)))
                            subject_context += f"\n** CLO tom tat **: {best_result['content'].strip()}\n"
                            
                        elif content_type == 'learning_outcome_detail':
                            # Summarize individual CLOs
                            clo_details = [r['content'].strip() for r in results[:5]]  # Top 5 CLOs
                            if clo_details:
                                subject_context += f"\n** CLO chi tiet **:\n" + "\n".join(clo_details[:3]) + "\n"
                            
                        elif content_type == 'assessments':
                            best_result = max(results, key=lambda x: x.get('final_score', x.get('score', 0)))
                            subject_context += f"\n** Danh gia **: {best_result['content'].strip()}\n"
                            
                        elif content_type == 'materials':
                            best_result = max(results, key=lambda x: x.get('final_score', x.get('score', 0)))
                            subject_context += f"\n** Tai lieu **: {best_result['content'].strip()}\n"
                            
                        elif content_type == 'schedule':
                            best_result = max(results, key=lambda x: x.get('final_score', x.get('score', 0)))
                            subject_context += f"\n** Lich hoc **: {best_result['content'].strip()}\n"
                
                context_parts.append(subject_context)
        
        return "\n".join(context_parts)
    
    def _format_general_info(self, result: Dict) -> str:
        """Format general info in a structured way"""
        content = result['content'].strip()
        metadata = result.get('metadata', {})
        
        formatted = f"\n** Thong tin chung **:\n{content}"
        
        # Add metadata if available
        if metadata.get('credits'):
            formatted += f"\nTin chi: {metadata['credits']}"
        if metadata.get('semester'):
            formatted += f"\nKy hoc: {metadata['semester']}"
        if metadata.get('prerequisites'):
            formatted += f"\nTien quyet: {metadata['prerequisites']}"
        
        return formatted + "\n"
    
    def _limit_context_length(self, context: str, max_length: int = 8000) -> str:
        """Limit context length to prevent overwhelming the LLM"""
        if len(context) <= max_length:
            return context
        
        # Intelligent truncation - keep the most important parts
        lines = context.split('\n')
        important_lines = []
        current_length = 0
        
        # For major overview, prioritize keeping the full list
        if '** MAJOR_OVERVIEW **' in context:
            # Keep major overview section fully
            for line in lines:
                if current_length + len(line) > max_length:
                    break
                important_lines.append(line)
                current_length += len(line)
        else:
            # Priority: general_info > assessments > learning_outcomes > materials > schedule
            for line in lines:
                if current_length + len(line) > max_length:
                    break
                
                # Always keep subject headers and general info
                if ('** ' in line and ' **' in line):
                    important_lines.append(line)
                    current_length += len(line)
                elif current_length < max_length * 0.8:  # Use 80% for detailed content
                    important_lines.append(line)
                    current_length += len(line)
        
        if current_length >= max_length:
            important_lines.append("\n... (da rut gon de toi uu phan hoi)")
        
        return '\n'.join(important_lines)

    def _generate_response(self, question: str, context: str) -> str:
        """T·∫°o ph·∫£n h·ªìi s·ª≠ d·ª•ng Gemini v·ªõi context ƒë∆∞·ª£c cung c·∫•p"""
        try:
            # ENHANCED: Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ t·∫°o prompt ph√π h·ª£p
            question_lower = question.lower()
            
            # Detect question type for better prompt
            is_semester_question = any(term in question_lower for term in ['k·ª≥', 'ky', 'k√¨', 'ki', 'semester'])
            is_listing_question = any(term in question_lower for term in ['li·ªát k√™', 'danh s√°ch', 'list', 't·∫•t c·∫£', 'c√°c m√¥n'])
            is_specific_subject = bool(re.search(r'[A-Za-z]{2,4}\d{3}[a-zA-Z]*', question))
            is_followup_question = any(term in question_lower for term in ['th√¨ sao', 'ra sao', 'nh∆∞ th·∫ø n√†o', 'c√≤n', 'con'])
            
            # Detect special features question
            is_special_features_question = any(keyword in question_lower for keyword in [
                'ƒëi·ªÉm th∆∞·ªüng', 'diem thuong', 'bonus', 'paper', 'b√†i b√°o', 'bai bao', 'scopus', 'isi',
                'ƒë·∫∑c bi·ªát', 'dac biet', 'special', 'mooc', 'coursera'
            ])
            has_bonus_paper_in_context = '** MON CO DIEM THUONG PAPER KHOA HOC **' in context
            
            # Build enhanced prompt based on question type
            if is_special_features_question and has_bonus_paper_in_context:
                # Special features questions (bonus paper, MOOC, etc.)
                prompt = f"""B·∫°n l√† AI Assistant chuy√™n m√¥n v·ªÅ th√¥ng tin h·ªçc t·∫≠p t·∫°i FPT University. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n CH√çNH X√ÅC th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC QUAN TR·ªåNG CHO SPECIAL FEATURES:
- Tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p v·ªÅ c√°c m√¥n h·ªçc c√≥ ƒë·∫∑c ƒëi·ªÉm ƒë·∫∑c bi·ªát
- Khi c√≥ th√¥ng tin v·ªÅ ƒëi·ªÉm th∆∞·ªüng paper, h√£y li·ªát k√™ r√µ r√†ng c√°c m√¥n h·ªçc v√† m·ª©c ƒëi·ªÉm th∆∞·ªüng
- T·∫°o b·∫£ng th√¥ng tin v·ªõi c√°c c·ªôt: M√£ m√¥n, ƒêi·ªÉm th∆∞·ªüng ISI/Scopus, ƒêi·ªÅu ki·ªán
- CH√ö √ù: Th√¥ng tin v·ªÅ bonus paper n·∫±m trong ph·∫ßn "** MON CO DIEM THUONG PAPER KHOA HOC **"

D·ªÆ LI·ªÜU:
{context}

T√çNH NƒÇNG QUAN TR·ªåNG:
- N·∫øu c√¢u h·ªèi h·ªèi v·ªÅ "m√¥n c√≥ ƒëi·ªÉm th∆∞·ªüng paper", h√£y t·∫≠p trung v√†o ph·∫ßn "** MON CO DIEM THUONG PAPER KHOA HOC **"
- Tr·∫£ l·ªùi b·∫±ng format b·∫£ng r√µ r√†ng v·ªÅ m·ª©c ƒëi·ªÉm th∆∞·ªüng cho t·ª´ng lo·∫°i journal
- Bao g·ªìm ƒëi·ªÅu ki·ªán v√† y√™u c·∫ßu cho vi·ªác nh·∫≠n ƒëi·ªÉm th∆∞·ªüng

C√ÇU H·ªéI: {question}"""
            
            elif is_semester_question and (is_listing_question or is_followup_question):
                # Semester-focused questions
                prompt = f"""B·∫°n l√† AI Assistant chuy√™n m√¥n v·ªÅ th√¥ng tin h·ªçc t·∫≠p t·∫°i FPT University. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n CH√çNH X√ÅC th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC QUAN TR·ªåNG:
- S·ª≠ d·ª•ng CH√çNH X√ÅC th√¥ng tin t·ª´ d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p
- Khi tr·∫£ l·ªùi v·ªÅ m√¥n h·ªçc trong m·ªôt k·ª≥ c·ª• th·ªÉ, h√£y li·ªát k√™ T·∫§T C·∫¢ c√°c m√¥n c·ªßa k·ª≥ ƒë√≥
- T·∫°o b·∫£ng th√¥ng tin r√µ r√†ng v·ªõi c√°c c·ªôt: M√£ m√¥n, T√™n m√¥n, S·ªë t√≠n ch·ªâ
- ƒê·∫£m b·∫£o th√¥ng tin v·ªÅ t√≠n ch·ªâ, k·ª≥ h·ªçc ƒë∆∞·ª£c hi·ªÉn th·ªã ch√≠nh x√°c
- T√≠nh t·ªïng s·ªë t√≠n ch·ªâ c·ªßa k·ª≥ h·ªçc
- Kh√¥ng t·ª± t·∫°o ra th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu

ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI CHO C√ÇUH·ªéI V·ªÄ K·ª≤ H·ªåC:

**K·ª≥ [s·ªë] ng√†nh AI t·∫°i FPT University g·ªìm [s·ªë] m√¥n h·ªçc:**

| M√£ m√¥n | T√™n m√¥n h·ªçc | S·ªë t√≠n ch·ªâ |
|--------|-------------|------------|
| [m√£]   | [t√™n]       | [t√≠n ch·ªâ]  |

**T·ªïng s·ªë t√≠n ch·ªâ:** [t·ªïng] t√≠n ch·ªâ

**L∆∞u √Ω b·ªï sung:** [n·∫øu c√≥ th√¥ng tin ƒë·∫∑c bi·ªát v·ªÅ m√¥n n√†o]

D·ªÆ LI·ªÜU:
{context}

C√ÇUH·ªéI: {question}

TR·∫¢ L·ªúI:"""

            elif is_specific_subject:
                # Subject-specific questions
                prompt = f"""B·∫°n l√† AI Assistant chuy√™n m√¥n v·ªÅ th√¥ng tin h·ªçc t·∫≠p t·∫°i FPT University. H√£y tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ m√¥n h·ªçc c·ª• th·ªÉ d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC:
- Cung c·∫•p th√¥ng tin CHI TI·∫æT v√† CH√çNH X√ÅC v·ªÅ m√¥n h·ªçc
- Bao g·ªìm: m√£ m√¥n, t√™n ƒë·∫ßy ƒë·ªß, s·ªë t√≠n ch·ªâ, k·ª≥ h·ªçc, m√¥ t·∫£, CLO n·∫øu c√≥
- ƒê·ªãnh d·∫°ng th√¥ng tin r√µ r√†ng, d·ªÖ ƒë·ªçc
- N·∫øu c√≥ nhi·ªÅu m√¥n t∆∞∆°ng t·ª±, so s√°nh v√† ph√¢n bi·ªát

D·ªÆ LI·ªÜU:
{context}

C√ÇUH·ªéI: {question}

TR·∫¢ L·ªúI:"""

            else:
                # General questions
                prompt = f"""B·∫°n l√† AI Assistant chuy√™n m√¥n v·ªÅ th√¥ng tin h·ªçc t·∫≠p t·∫°i FPT University (FPTU). H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. S·ª≠ d·ª•ng th√¥ng tin ch√≠nh x√°c t·ª´ d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p
2. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† chuy√™n nghi·ªáp  
3. C·∫•u tr√∫c th√¥ng tin logic, d·ªÖ hi·ªÉu
4. N·∫øu c·∫ßn li·ªát k√™ nhi·ªÅu m√¥n h·ªçc, s·ª≠ d·ª•ng b·∫£ng markdown
5. T√≠nh to√°n t·ªïng s·ªë t√≠n ch·ªâ khi c·∫ßn thi·∫øt
6. Th√™m l∆∞u √Ω h·ªØu √≠ch cho sinh vi√™n

QUAN TR·ªåNG:
- KH√îNG s·ª≠ d·ª•ng bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c hay icon
- KH√îNG t·ª± t·∫°o th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu
- N·∫øu thi·∫øu th√¥ng tin, n√™u r√µ v√† g·ª£i √Ω c√°ch t√¨m hi·ªÉu th√™m

D·ªÆ LI·ªÜU:
{context}

C√ÇUH·ªéI: {question}

TR·∫¢ L·ªúI:"""
            
            # Generate response with enhanced context awareness and API rotation
            answer = self._call_gemini_with_rotation(prompt)
            
            # ENHANCED: Post-process to ensure quality
            # Ensure no emojis or icons (as per user rules)
            answer = re.sub(r'[üéØüöÄ‚úÖ‚ùåüìöüìùüí°‚≠êüîçüìäüåü‚ú®üé™üé®üé≠üé™üî•üí•üéâüéäüéàüéÅüéÄüéÉüéÑüéÜüéá‚ú®üåà‚ö°üíéüåü]', '', answer)
            
            # Ensure proper formatting for tables if detected
            if '|' in answer and 'M√£ m√¥n' in answer:
                # This looks like a table - ensure proper markdown formatting
                lines = answer.split('\n')
                formatted_lines = []
                in_table = False
                
                for line in lines:
                    if '|' in line and ('M√£ m√¥n' in line or 'T√™n m√¥n' in line):
                        in_table = True
                        formatted_lines.append(line)
                        # Add separator line if not already present
                        if not any('---' in next_line for next_line in lines[lines.index(line)+1:lines.index(line)+3]):
                            formatted_lines.append('|--------|-------------|------------|')
                    elif '|' in line and in_table:
                        formatted_lines.append(line)
                    elif in_table and line.strip() == '':
                        in_table = False
                        formatted_lines.append(line)
                    else:
                        formatted_lines.append(line)
                
                answer = '\n'.join(formatted_lines)
            
            return answer
            
        except Exception as e:
            logger.error(f"L·ªói t·∫°o ph·∫£n h·ªìi: {e}")
            return self._fallback_response(question, context)
    
    def _fallback_response(self, question: str, context: str) -> str:
        """Enhanced fallback response khi kh√¥ng th·ªÉ g·ªçi Gemini"""
        
        # ENHANCED: Try to extract useful information from context even without LLM
        question_lower = question.lower()
        
        # 1. COMBO/SPECIALIZATION QUERIES
        if any(term in question_lower for term in ['combo', 'chuy√™n ng√†nh']):
            if any(term in question_lower for term in ['bao nhi√™u', 'c√≥ m·∫•y', 's·ªë l∆∞·ª£ng']):
                # Extract combo information from context
                combo_count = context.count('COMBO_')
                combo_names = []
                if 'AI17_COM1' in context:
                    combo_names.append('AI17_COM1 (Data Science v√† Big Data Analytics)')
                if 'AI17_COM3' in context:
                    combo_names.append('AI17_COM3 (AI for Healthcare v√† Research)')
                if 'AI17_COM2.1' in context:
                    combo_names.append('AI17_COM2.1 (Text Mining v√† Search Engineering)')
                
                if combo_names:
                    response = f"Ng√†nh AI t·∫°i FPT University c√≥ **{len(combo_names)} combo chuy√™n ng√†nh h·∫πp**:\n\n"
                    for i, combo in enumerate(combo_names, 1):
                        response += f"{i}. {combo}\n"
                    response += "\nM·ªói combo bao g·ªìm c√°c m√¥n h·ªçc chuy√™n s√¢u trong lƒ©nh v·ª±c ƒë√≥."
                    return response
            
            elif any(term in question_lower for term in ['l√† g√¨', 'ƒë·ªãnh nghƒ©a']):
                return """**Combo chuy√™n ng√†nh h·∫πp** l√† c√°c nh√≥m m√¥n h·ªçc chuy√™n s√¢u trong ng√†nh AI t·∫°i FPT University.

Ng√†nh AI c√≥ **3 combo chuy√™n ng√†nh h·∫πp**:

1. **AI17_COM1**: Data Science v√† Big Data Analytics
   - T·∫≠p trung v√†o khoa h·ªçc d·ªØ li·ªáu v√† ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn

2. **AI17_COM2.1**: Text Mining v√† Search Engineering  
   - Khai th√°c vƒÉn b·∫£n v√† c√¥ng c·ª• t√¨m ki·∫øm

3. **AI17_COM3**: AI for Healthcare v√† Research
   - ·ª®ng d·ª•ng AI trong y t·∫ø v√† nghi√™n c·ª©u khoa h·ªçc

M·ªói combo gi√∫p sinh vi√™n chuy√™n s√¢u v√†o m·ªôt lƒ©nh v·ª±c c·ª• th·ªÉ c·ªßa AI."""
        
        # 2. SEMESTER QUERIES
        if any(term in question_lower for term in ['k√¨', 'ky', 'k·ª≥']):
            # Extract semester information from context
            semester_info = {}
            lines = context.split('\n')
            for line in lines:
                if 'Ky ' in line or 'k√Ω ' in line:
                    # Try to extract semester info
                    if ' - ' in line and ':' in line:
                        parts = line.split(' - ')
                        if len(parts) >= 2:
                            subject_part = parts[0].strip()
                            desc_part = parts[1].split('(')[0].strip()
                            # Extract semester number
                            ky_match = re.search(r'Ky (\d+)', line)
                            if ky_match:
                                semester = ky_match.group(1)
                                if semester not in semester_info:
                                    semester_info[semester] = []
                                semester_info[semester].append(f"- {subject_part}: {desc_part}")
            
            if semester_info:
                response = "**Th√¥ng tin m√¥n h·ªçc theo k√¨:**\n\n"
                for semester in sorted(semester_info.keys()):
                    if len(semester_info[semester]) > 0:
                        response += f"**K√¨ {semester}:**\n"
                        response += '\n'.join(semester_info[semester][:5])  # Limit to 5 subjects
                        if len(semester_info[semester]) > 5:
                            response += f"\n... v√† {len(semester_info[semester]) - 5} m√¥n kh√°c"
                        response += '\n\n'
                return response
        
        # 3. RELATIONSHIP QUERIES
        if 'li√™n quan' in question_lower and ('k√¨' in question_lower or 'ky' in question_lower):
            return """ƒê·ªÉ ph√¢n t√≠ch m·ªëi li√™n h·ªá gi·ªØa c√°c m√¥n h·ªçc ·ªü c√°c k√¨ kh√°c nhau, c·∫ßn xem x√©t:

**1. M√¥n ti√™n quy·∫øt (Prerequisites)**:
- M·ªôt s·ªë m√¥n ·ªü k√¨ sau y√™u c·∫ßu ho√†n th√†nh m√¥n ·ªü k√¨ tr∆∞·ªõc
- V√≠ d·ª•: To√°n cao c·∫•p (k√¨ 1) ‚Üí X√°c su·∫•t th·ªëng k√™ (k√¨ sau)

**2. Chu·ªói ki·∫øn th·ª©c**:
- M√¥n c∆° b·∫£n ‚Üí M√¥n n√¢ng cao ‚Üí M√¥n chuy√™n s√¢u
- V√≠ d·ª•: L·∫≠p tr√¨nh c∆° b·∫£n ‚Üí C·∫•u tr√∫c d·ªØ li·ªáu ‚Üí Thu·∫≠t to√°n AI

**3. Nh√≥m m√¥n c√πng lƒ©nh v·ª±c**:
- To√°n h·ªçc: MAD101, MAE101, PRO192...
- L·∫≠p tr√¨nh: PFP191, PRO192, OOP...
- AI chuy√™n s√¢u: AIG, AIE, AIH...

ƒê·ªÉ bi·∫øt chi ti·∫øt m·ªëi li√™n h·ªá c·ª• th·ªÉ, b·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ t·ª´ng c·∫∑p m√¥n h·ªçc."""
        
        # 4. EXTRACT SUBJECT CODES FROM CONTEXT
        subjects = set()
        for line in context.split('\n'):
            # Extract from various patterns
            if '** ' in line:
                match = re.search(r'\*\*([A-Z]{2,4}\d{3}[a-z]*)\*\*', line)
                if match:
                    subjects.add(match.group(1))
            # Extract from listing patterns
            subject_codes = re.findall(r'([A-Z]{2,4}\d{3}[a-z]*)', line)
            subjects.update(subject_codes)
        
        if subjects:
            subjects_list = sorted(list(subjects))
            return f"""D·ª±a tr√™n d·ªØ li·ªáu t√¨m ƒë∆∞·ª£c, t√¥i c√≥ th√¥ng tin v·ªÅ **{len(subjects_list)} m√¥n h·ªçc**: {', '.join(subjects_list[:10])}{', ...' if len(subjects_list) > 10 else ''}.

**C√¢u h·ªèi c·ªßa b·∫°n**: {question}

Tuy nhi√™n, h·ªá th·ªëng g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t khi t·∫°o c√¢u tr·∫£ l·ªùi chi ti·∫øt. B·∫°n c√≥ th·ªÉ:
- H·ªèi c·ª• th·ªÉ v·ªÅ m·ªôt m√¥n h·ªçc: "CSI106 l√† m√¥n g√¨?"
- H·ªèi v·ªÅ k√¨ h·ªçc: "K√¨ 1 c√≥ nh·ªØng m√¥n g√¨?"
- Th·ª≠ l·∫°i sau khi h·ªá th·ªëng ph·ª•c h·ªìi"""
        
        # 5. DEFAULT FALLBACK
        return f"""Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t khi x·ª≠ l√Ω c√¢u h·ªèi.

**C√¢u h·ªèi c·ªßa b·∫°n**: {question}

**G·ª£i √Ω c√°c c√¢u h·ªèi b·∫°n c√≥ th·ªÉ th·ª≠**:
- "Li·ªát k√™ c√°c m√¥n h·ªçc k√¨ 1"
- "CSI106 l√† m√¥n g√¨?"
- "C√≥ bao nhi√™u combo chuy√™n ng√†nh h·∫πp?"
- "Chuy√™n ng√†nh h·∫πp l√† g√¨?"
- "C√°c m√¥n c√≥ 3 t√≠n ch·ªâ"

**Ho·∫∑c th·ª≠ l·∫°i sau khi h·ªá th·ªëng ph·ª•c h·ªìi.**"""

    def get_subject_overview(self, subject_code: str = None) -> Dict[str, Any]:
        """L·∫•y t·ªïng quan v·ªÅ m√¥n h·ªçc"""
        if subject_code:
            # Find subject in data
            subject_items = [item for item in self.data if item.get('subject_code') == subject_code]
            if subject_items:
                general_info = next((item for item in subject_items if item.get('type') == 'general_info'), None)
                if general_info:
                    return {
                        'subject_code': subject_code,
                        'summary': general_info['content'],
                        'metadata': general_info.get('metadata', {})
                    }
            return {'error': f'Kh√¥ng t√¨m th·∫•y m√¥n {subject_code}'}
        else:
            # Tr·∫£ v·ªÅ t·∫•t c·∫£ m√¥n
            subjects = []
            seen_subjects = set()
            
            for item in self.data:
                if item.get('type') == 'general_info' and item.get('subject_code') not in seen_subjects:
                    subject_code = item.get('subject_code')
                    seen_subjects.add(subject_code)
                    metadata = item.get('metadata', {})
                    
                    subjects.append({
                        'subject_code': subject_code,
                        'name': metadata.get('course_name', metadata.get('title', subject_code)),
                        'credits': metadata.get('credits', 'N/A'),
                        'summary': item['content'][:200] + '...'
                    })
            
            return {'subjects': subjects, 'total': len(subjects)}

class QueryChain:
    """X·ª≠ l√Ω chu·ªói truy v·∫•n ƒëa c·∫•p (multi-hop query)"""
    
    def __init__(self, rag_engine):
        self.rag_engine = rag_engine
        self.max_hops = 3  # Gi·ªõi h·∫°n s·ªë l·∫ßn truy v·∫•n k√©p ƒë·ªÉ tr√°nh v√≤ng l·∫∑p
        
        # Patterns ƒë·ªÉ nh·∫≠n di·ªán th√¥ng tin c√≥ th·ªÉ truy v·∫•n ti·∫øp
        self.prerequisite_patterns = [
            r'm√¥n ti√™n quy·∫øt.*?([A-Z]{2,4}\d{3}[a-z]*)',
            r'prerequisite.*?([A-Z]{2,4}\d{3}[a-z]*)',
            r'c·∫ßn h·ªçc tr∆∞·ªõc.*?([A-Z]{2,4}\d{3}[a-z]*)',
            r'ph·∫£i ho√†n th√†nh.*?([A-Z]{2,4}\d{3}[a-z]*)',
        ]
        
        self.related_subject_patterns = [
            r'li√™n quan ƒë·∫øn.*?([A-Z]{2,4}\d{3}[a-z]*)',
            r'k·∫øt h·ª£p v·ªõi.*?([A-Z]{2,4}\d{3}[a-z]*)',
            r'ti·∫øp theo.*?([A-Z]{2,4}\d{3}[a-z]*)',
            r'n√¢ng cao.*?([A-Z]{2,4}\d{3}[a-z]*)',
        ]
        
        self.detail_expansion_keywords = [
            'chi ti·∫øt h∆°n', 'th√¥ng tin ƒë·∫ßy ƒë·ªß', 'm√¥ t·∫£ c·ª• th·ªÉ', 't√†i li·ªáu',
            'gi√°o tr√¨nh', 'syllabus', 'CLO', 'learning outcomes'
        ]

    def detect_followup_queries(self, answer: str, original_query: str) -> List[FollowupQuery]:
        """Ph√°t hi·ªán c√°c truy v·∫•n ti·∫øp theo t·ª´ c√¢u tr·∫£ l·ªùi - Optimized to be less aggressive"""
        followup_queries = []
        answer_lower = answer.lower()
        original_query_lower = original_query.lower()
        
        # Early exit for simple queries that don't need followup
        simple_query_indicators = [
            'l√† g√¨', 'l√† m√¥n g√¨', 'bao nhi√™u t√≠n ch·ªâ', 'k·ª≥ n√†o', 'k·ª≥ m·∫•y',
            'gi·∫£ng vi√™n l√† ai', 'h·ªçc ph√≠', 'ƒë√°nh gi√° nh∆∞ th·∫ø n√†o'
        ]
        
        if any(indicator in original_query_lower for indicator in simple_query_indicators):
            # Only proceed if explicitly asking for prerequisites
            if not any(keyword in original_query_lower for keyword in ['ti√™n quy·∫øt', 'li√™n quan', 'chi ti·∫øt']):
                return []
        
        # Only detect prerequisites if explicitly mentioned in original query OR answer is very detailed
        if ('ti√™n quy·∫øt' in original_query_lower or 'v√† c√°c m√¥n' in original_query_lower or 
            len(answer.split()) > 200):  # Only for detailed answers
            
            # 1. Ph√°t hi·ªán m√¥n ti√™n quy·∫øt (more selective)
            for pattern in self.prerequisite_patterns:
                matches = re.findall(pattern, answer, re.IGNORECASE)
                for subject_code in matches:
                    if (subject_code not in original_query and 
                        subject_code not in [m[0] for m in followup_queries if hasattr(m, 'query')]):
                        query = f"Th√¥ng tin chi ti·∫øt v·ªÅ m√¥n {subject_code}"
                        followup_queries.append(FollowupQuery(
                            query=query,
                            confidence=0.9,
                            query_type='prerequisite',
                            source_info=f"M√¥n ti√™n quy·∫øt ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong c√¢u tr·∫£ l·ªùi"
                        ))
                        # Limit to 1 prerequisite query per answer
                        break
        
        # 2. Only detect related subjects if explicitly requested
        if any(keyword in original_query_lower for keyword in ['li√™n quan', 'k·∫øt h·ª£p', 't∆∞∆°ng t·ª±']):
            for pattern in self.related_subject_patterns:
                matches = re.findall(pattern, answer, re.IGNORECASE)
                for subject_code in matches[:1]:  # Limit to 1
                    if subject_code not in original_query:
                        query = f"Th√¥ng tin v·ªÅ m√¥n {subject_code}"
                        followup_queries.append(FollowupQuery(
                            query=query,
                            confidence=0.7,
                            query_type='related_subject',
                            source_info=f"M√¥n li√™n quan ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn"
                        ))
                        break
        
        # 3. Only expand details if explicitly requested
        if any(keyword in original_query_lower for keyword in ['chi ti·∫øt', 'ƒë·∫ßy ƒë·ªß', 'syllabus', 'm·ªü r·ªông']):
            for keyword in self.detail_expansion_keywords:
                if keyword in answer_lower:
                    # Extract subject codes t·ª´ c√¢u tr·∫£ l·ªùi
                    subject_codes = re.findall(r'([A-Z]{2,4}\d{3}[a-z]*)', answer)
                    for subject_code in subject_codes[:1]:  # Only 1 expansion
                        if subject_code not in original_query:
                            query = f"Th√¥ng tin ƒë·∫ßy ƒë·ªß v·ªÅ {subject_code} bao g·ªìm syllabus v√† CLO"
                            followup_queries.append(FollowupQuery(
                                query=query,
                                confidence=0.6,
                                query_type='detail_expansion',
                                source_info=f"C·∫ßn th√¥ng tin chi ti·∫øt h∆°n v·ªÅ {subject_code}"
                            ))
                            break
        
        # 4. Skip incomplete information detection for basic queries
        # Only proceed for complex/detailed queries
        if (len(answer.split()) > 150 and 
            any(phrase in answer_lower for phrase in [
                'kh√¥ng c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß', 'c·∫ßn t√¨m th√™m th√¥ng tin', 
                'th√¥ng tin chi ti·∫øt c·∫ßn ƒë∆∞·ª£c t√¨m hi·ªÉu th√™m'
            ])):
            # Extract subject codes t·ª´ c√¢u tr·∫£ l·ªùi
            subject_codes = re.findall(r'([A-Z]{2,4}\d{3}[a-z]*)', answer)
            for subject_code in subject_codes[:1]:  # Limit to 1
                if subject_code not in original_query:
                    query = f"Th√¥ng tin chi ti·∫øt v·ªÅ {subject_code}"
                    followup_queries.append(FollowupQuery(
                        query=query,
                        confidence=0.8,
                        query_type='detail_expansion',
                        source_info=f"C√¢u tr·∫£ l·ªùi thi·∫øu th√¥ng tin v·ªÅ {subject_code}"
                    ))
                    break
        
        # S·∫Øp x·∫øp theo confidence v√† lo·∫°i b·ªè tr√πng l·∫∑p
        unique_queries = {}
        for fq in followup_queries:
            if fq.query not in unique_queries or unique_queries[fq.query].confidence < fq.confidence:
                unique_queries[fq.query] = fq
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng followup queries - more restrictive
        sorted_queries = sorted(unique_queries.values(), key=lambda x: x.confidence, reverse=True)
        return sorted_queries[:2]  # T·ªëi ƒëa 2 followup queries (gi·∫£m t·ª´ 3)

    def execute_query_chain(self, original_query: str, enable_multihop: bool = True) -> QueryChainResult:
        """Th·ª±c hi·ªán chu·ªói truy v·∫•n ƒëa c·∫•p"""
        execution_path = [f"Truy v·∫•n g·ªëc: {original_query}"]
        
        # B∆∞·ªõc 1: Th·ª±c hi·ªán truy v·∫•n g·ªëc
        original_result = self.rag_engine.query(original_query)
        original_answer = original_result['answer']
        
        if not enable_multihop:
            return QueryChainResult(
                original_query=original_query,
                original_answer=original_answer,
                followup_queries=[],
                followup_results=[],
                final_integrated_answer=original_answer,
                execution_path=execution_path
            )
        
        # B∆∞·ªõc 2: Ph√°t hi·ªán truy v·∫•n ti·∫øp theo
        followup_queries = self.detect_followup_queries(original_answer, original_query)
        
        if not followup_queries:
            execution_path.append("Kh√¥ng ph√°t hi·ªán truy v·∫•n ti·∫øp theo")
            return QueryChainResult(
                original_query=original_query,
                original_answer=original_answer,
                followup_queries=[],
                followup_results=[],
                final_integrated_answer=original_answer,
                execution_path=execution_path
            )
        
        # B∆∞·ªõc 3: Th·ª±c hi·ªán c√°c truy v·∫•n ti·∫øp theo
        followup_results = []
        for i, fq in enumerate(followup_queries):
            if i >= self.max_hops:
                break
                
            execution_path.append(f"Truy v·∫•n ti·∫øp theo {i+1}: {fq.query} (confidence: {fq.confidence:.2f})")
            
            try:
                result = self.rag_engine.query(fq.query)
                result['followup_query'] = fq
                followup_results.append(result)
                execution_path.append(f"  -> Ho√†n th√†nh truy v·∫•n {i+1}")
            except Exception as e:
                execution_path.append(f"  -> L·ªói truy v·∫•n {i+1}: {str(e)}")
                continue
        
        # B∆∞·ªõc 4: T√≠ch h·ª£p k·∫øt qu·∫£
        final_answer = self._integrate_results(original_result, followup_results, original_query)
        execution_path.append("T√≠ch h·ª£p k·∫øt qu·∫£ ho√†n t·∫•t")
        
        return QueryChainResult(
            original_query=original_query,
            original_answer=original_answer,
            followup_queries=followup_queries,
            followup_results=followup_results,
            final_integrated_answer=final_answer,
            execution_path=execution_path
        )

    def _integrate_results(self, original_result: Dict, followup_results: List[Dict], original_query: str) -> str:
        """T√≠ch h·ª£p k·∫øt qu·∫£ t·ª´ truy v·∫•n g·ªëc v√† c√°c truy v·∫•n ti·∫øp theo"""
        if not followup_results:
            return original_result['answer']
        
        # Chu·∫©n b·ªã context t√≠ch h·ª£p
        integration_context = f"""
TRUY V·∫§N G·ªêC: {original_query}
TH√îNG TIN S·ªê 1 (C√¢u tr·∫£ l·ªùi ch√≠nh):
{original_result['answer']}

"""
        
        # Th√™m th√¥ng tin t·ª´ c√°c truy v·∫•n ti·∫øp theo
        for i, result in enumerate(followup_results):
            fq = result.get('followup_query')
            integration_context += f"""TH√îNG TIN B·ªî SUNG {i+2} (T·ª´ truy v·∫•n: {fq.query if fq else 'N/A'}):
{result['answer']}

"""
        
        # Prompt t√≠ch h·ª£p th√¥ng minh
        integration_prompt = f"""
D·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p, h√£y t·∫°o ra m·ªôt c√¢u tr·∫£ l·ªùi t√≠ch h·ª£p v√† ho√†n ch·ªânh cho truy v·∫•n g·ªëc.

QUY T·∫ÆC T√çCH H·ª¢P:
1. B·∫Øt ƒë·∫ßu v·ªõi c√¢u tr·∫£ l·ªùi ch√≠nh cho truy v·∫•n g·ªëc
2. B·ªï sung th√¥ng tin chi ti·∫øt t·ª´ c√°c truy v·∫•n ti·∫øp theo m·ªôt c√°ch logic
3. Tr√°nh l·∫∑p l·∫°i th√¥ng tin
4. S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± quan tr·ªçng v√† logic
5. Gi·ªØ nguy√™n c√°c th√¥ng tin quan tr·ªçng nh∆∞ m√£ m√¥n h·ªçc, t√≠n ch·ªâ, k·ª≥ h·ªçc
6. N·∫øu c√≥ th√¥ng tin m√¢u thu·∫´n, ∆∞u ti√™n th√¥ng tin t·ª´ c√¢u tr·∫£ l·ªùi ch√≠nh

TH√îNG TIN ƒê·∫¶U V√ÄO:
{integration_context}

H√£y t·∫°o ra m·ªôt c√¢u tr·∫£ l·ªùi t√≠ch h·ª£p, ƒë·∫ßy ƒë·ªß v√† c√≥ c·∫•u tr√∫c r√µ r√†ng:
"""
        
        try:
            # S·ª≠ d·ª•ng Gemini ƒë·ªÉ t√≠ch h·ª£p v·ªõi rotation
            if hasattr(self.rag_engine, '_call_gemini_with_rotation'):
                result_text = self.rag_engine._call_gemini_with_rotation(integration_prompt)
                return result_text
            else:
                # Fallback: direct call
                model = genai.GenerativeModel('gemini-1.5-flash')
                response = model.generate_content(integration_prompt)
                if response.text:
                    return response.text.strip()
                else:
                    return self._simple_integration(original_result['answer'], followup_results)
                
        except Exception as e:
            logger.error(f"L·ªói t√≠ch h·ª£p k·∫øt qu·∫£ v·ªõi Gemini: {e}")
            return self._simple_integration(original_result['answer'], followup_results)

    def _simple_integration(self, original_answer: str, followup_results: List[Dict]) -> str:
        """T√≠ch h·ª£p ƒë∆°n gi·∫£n khi Gemini kh√¥ng kh·∫£ d·ª•ng"""
        integrated = original_answer + "\n\n"
        
        for i, result in enumerate(followup_results):
            fq = result.get('followup_query')
            if fq:
                integrated += f"TH√îNG TIN B·ªî SUNG V·ªÄ {fq.query.upper()}:\n"
                integrated += result['answer'] + "\n\n"
        
        return integrated.strip() 