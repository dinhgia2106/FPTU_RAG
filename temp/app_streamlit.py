import streamlit as st
import json
import os
import numpy as np
import faiss
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from dotenv import load_dotenv
import asyncio
import time
import re

# X·ª≠ l√Ω event loop cho asyncio (ƒë·∫∑t ·ªü ƒë·∫ßu)
try:
    loop = asyncio.get_event_loop()
    if loop.is_closed():  # Ki·ªÉm tra n·∫øu loop ƒë√£ b·ªã ƒë√≥ng
        raise RuntimeError("Event loop is closed")
except RuntimeError:
    new_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(new_loop)

# --- Constants ---
# S·ª≠ d·ª•ng c√°c ƒë∆∞·ªùng d·∫´n file t·ª´ pipeline c·ªßa ch√∫ng ta
FAISS_INDEX_FILE = "all_data.faiss"
FAISS_MAPPING_FILE = "all_data_faiss_mapping.json"
# File n√†y ch·ª©a n·ªôi dung ƒë·∫ßy ƒë·ªß v√† metadata c·ªßa c√°c chunk
ALL_CHUNKS_DATA_FILE = "embedded_all_chunks_with_students.json"

CHROMA_PERSIST_DIRECTORY = "all_data_chroma_db_store"
CHROMA_COLLECTION_NAME = "all_syllabus_and_students_collection"

# Model embedding ƒë∆∞·ª£c s·ª≠ d·ª•ng (ph·∫£i kh·ªõp v·ªõi embedder.py v√† c√°c vector stores)
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'

# Gemini API Key and Model
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh trong file .env. Vui l√≤ng ki·ªÉm tra.")
    st.stop()

try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh Gemini API: {e}")
    st.stop()

GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest'  # Ho·∫∑c gemini-1.5-pro-latest

# --- Helper Functions for Translation ---


def is_vietnamese(text):
    """Ki·ªÉm tra s∆° b·ªô xem vƒÉn b·∫£n c√≥ ch·ª©a k√Ω t·ª± ti·∫øng Vi·ªát kh√¥ng."""
    vietnamese_chars = r"[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê]"
    return bool(re.search(vietnamese_chars, text))


def translate_to_english_if_vietnamese(text_to_translate, gemini_model_instance, original_user_question_for_log=""):
    """D·ªãch vƒÉn b·∫£n sang ti·∫øng Anh n·∫øu ph√°t hi·ªán l√† ti·∫øng Vi·ªát, s·ª≠ d·ª•ng Gemini."""
    if is_vietnamese(text_to_translate):
        log_query_display = original_user_question_for_log if original_user_question_for_log else text_to_translate
        print(
            f"App: Ph√°t hi·ªán c√¢u h·ªèi ti·∫øng Vi·ªát: '{log_query_display}'. ƒêang d·ªãch sang ti·∫øng Anh...")
        st.info(
            f"ƒêang d·ªãch c√¢u h·ªèi '{log_query_display[:50]}...' sang ti·∫øng Anh...")
        try:
            prompt = f'''Translate the following Vietnamese text to English. Provide only the English translation, without any introductory phrases, explanations, or quotation marks. Vietnamese text: "{text_to_translate}"'''
            response = gemini_model_instance.generate_content(prompt)
            translated_text = response.text.strip()
            # X√≥a d·∫•u ngo·∫∑c k√©p bao quanh n·∫øu c√≥
            if translated_text.startswith('"') and translated_text.endswith('"'):
                translated_text = translated_text[1:-1]
            if translated_text.startswith("'") and translated_text.endswith("'"):
                translated_text = translated_text[1:-1]

            print(f'''App: ƒê√£ d·ªãch sang ti·∫øng Anh: "{translated_text}"''')
            st.success(
                f'''ƒê√£ d·ªãch c√¢u h·ªèi sang ti·∫øng Anh: "{translated_text[:100]}..."''')
            return translated_text
        except Exception as e:
            st.warning(
                f"L·ªói khi d·ªãch sang ti·∫øng Anh: {e}. S·ª≠ d·ª•ng c√¢u h·ªèi g·ªëc.")
            print(
                f"App: D·ªãch thu·∫≠t sang ti·∫øng Anh th·∫•t b·∫°i cho: '{text_to_translate}'. L·ªói: {e}")
            return text_to_translate  # Tr·∫£ v·ªÅ vƒÉn b·∫£n g·ªëc n·∫øu l·ªói
    return text_to_translate  # Tr·∫£ v·ªÅ vƒÉn b·∫£n g·ªëc n·∫øu kh√¥ng ph·∫£i ti·∫øng Vi·ªát

# --- Functions ---


@st.cache_resource
def get_gemini_model():
    """T·∫£i v√† cache m√¥ h√¨nh Gemini."""
    print("App: ƒêang kh·ªüi t·∫°o m√¥ h√¨nh Gemini...")
    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        print("App: Kh·ªüi t·∫°o m√¥ h√¨nh Gemini th√†nh c√¥ng.")
        # Th√™m tr·∫°ng th√°i cho Gemini LLM
        st.sidebar.success("‚úÖ M√¥ h√¨nh Gemini (LLM)")
        return model
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói t·∫£i Gemini LLM: {e}")
        st.error(
            f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh Gemini: {e}. ·ª®ng d·ª•ng kh√¥ng th·ªÉ ti·∫øp t·ª•c.")
        st.stop()
        return None


@st.cache_resource
def load_all_resources():
    """T·∫£i t·∫•t c·∫£ c√°c t√†i nguy√™n c·∫ßn thi·∫øt: m√¥ h√¨nh embedding, FAISS, ChromaDB, d·ªØ li·ªáu chunks v√† m√¥ h√¨nh Gemini LLM."""
    resources = {
        "embedding_model": None,
        "faiss_index": None,
        "faiss_mapping": None,
        "all_chunks_data": None,
        "chroma_collection": None,
        "gemini_llm_model": None  # Th√™m key cho m√¥ h√¨nh Gemini
    }

    st.sidebar.write("--- Tr·∫°ng Th√°i T·∫£i T√†i Nguy√™n ---")

    # 0. T·∫£i m√¥ h√¨nh Gemini LLM (cho d·ªãch v√† sinh c√¢u tr·∫£ l·ªùi)
    resources["gemini_llm_model"] = get_gemini_model()
    if not resources["gemini_llm_model"]:  # get_gemini_model ƒë√£ st.stop() n·∫øu l·ªói
        return resources  # S·∫Ω kh√¥ng ƒë·∫øn ƒë√¢y n·∫øu get_gemini_model b·ªã l·ªói v√† stop

    # 1. T·∫£i m√¥ h√¨nh Embedding
    try:
        print("App: ƒêang t·∫£i m√¥ h√¨nh Sentence Transformer...")
        resources["embedding_model"] = SentenceTransformer(
            EMBEDDING_MODEL_NAME)
        print("App: T·∫£i m√¥ h√¨nh Sentence Transformer th√†nh c√¥ng.")
        st.sidebar.success("‚úÖ M√¥ h√¨nh Embedding")
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói t·∫£i Embedding Model: {e}")
        st.error(
            f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh embedding: {e}. ·ª®ng d·ª•ng kh√¥ng th·ªÉ ti·∫øp t·ª•c.")
        st.stop()

    # 2. T·∫£i d·ªØ li·ªáu chunks ƒë·∫ßy ƒë·ªß (c·∫ßn cho c·∫£ FAISS ƒë·ªÉ l·∫•y content)
    if not os.path.exists(ALL_CHUNKS_DATA_FILE):
        st.sidebar.error(
            f"‚ùå File chunks ch√≠nh '{ALL_CHUNKS_DATA_FILE}' kh√¥ng t√¨m th·∫•y.")
        st.error(
            f"File d·ªØ li·ªáu chunks '{ALL_CHUNKS_DATA_FILE}' kh√¥ng t·ªìn t·∫°i. H√£y ƒë·∫£m b·∫£o n√≥ ƒë∆∞·ª£c t·∫°o b·ªüi script embedder.py.")
    else:
        try:
            with open(ALL_CHUNKS_DATA_FILE, 'r', encoding='utf-8') as f:
                resources["all_chunks_data"] = json.load(f)
            st.sidebar.success(
                f"‚úÖ D·ªØ li·ªáu Chunks ({len(resources['all_chunks_data'])} items)")
        except Exception as e:
            st.sidebar.error(f"‚ùå L·ªói t·∫£i file '{ALL_CHUNKS_DATA_FILE}': {e}")

    # 3. T·∫£i FAISS index v√† mapping
    if not os.path.exists(FAISS_INDEX_FILE):
        st.sidebar.warning(
            f"‚ÑπÔ∏è File FAISS index '{FAISS_INDEX_FILE}' kh√¥ng t√¨m th·∫•y.")
    elif not os.path.exists(FAISS_MAPPING_FILE):
        st.sidebar.warning(
            f"‚ÑπÔ∏è File FAISS mapping '{FAISS_MAPPING_FILE}' kh√¥ng t√¨m th·∫•y.")
    elif resources["all_chunks_data"] is None:  # FAISS c·∫ßn all_chunks_data ƒë·ªÉ ho·∫°t ƒë·ªông
        st.sidebar.error(
            "‚ùå Kh√¥ng th·ªÉ t·∫£i FAISS do thi·∫øu d·ªØ li·ªáu chunks ch√≠nh.")
    else:
        try:
            resources["faiss_index"] = faiss.read_index(FAISS_INDEX_FILE)
            with open(FAISS_MAPPING_FILE, 'r', encoding='utf-8') as f:
                faiss_mapping_raw = json.load(f)
            # Chuy·ªÉn key c·ªßa id_to_chunk_info t·ª´ string sang int
            resources["faiss_mapping"] = {
                int(k): v for k, v in faiss_mapping_raw.items()}
            st.sidebar.success(
                f"‚úÖ FAISS Index ({resources['faiss_index'].ntotal} vectors)")
        except Exception as e:
            st.sidebar.error(f"‚ùå L·ªói t·∫£i FAISS: {e}")

    # 4. K·∫øt n·ªëi ChromaDB
    if not os.path.exists(CHROMA_PERSIST_DIRECTORY):
        st.sidebar.warning(
            f"‚ÑπÔ∏è Th∆∞ m·ª•c ChromaDB '{CHROMA_PERSIST_DIRECTORY}' kh√¥ng t√¨m th·∫•y.")
    else:
        try:
            client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY)
            resources["chroma_collection"] = client.get_collection(
                name=CHROMA_COLLECTION_NAME)
            st.sidebar.success(
                f"‚úÖ ChromaDB Collection ({resources['chroma_collection'].count()} items)")
        except Exception as e:
            # C√≥ th·ªÉ collection ch∆∞a t·ªìn t·∫°i, kh√¥ng n√™n coi l√† l·ªói nghi√™m tr·ªçng ·ªü b∆∞·ªõc t·∫£i
            st.sidebar.warning(f"‚ÑπÔ∏è ChromaDB: {e}")
            # N·∫øu l·ªói kh√¥ng ph·∫£i do collection kh√¥ng t·ªìn t·∫°i, c√≥ th·ªÉ l√† l·ªói k·∫øt n·ªëi
            # st.error(f"L·ªói nghi√™m tr·ªçng khi k·∫øt n·ªëi ChromaDB: {e}")

    st.sidebar.write("--------------------------------")
    return resources


def search_faiss(index, id_to_chunk_info, all_chunks_data, query_embedding, k=5):
    if not index or not id_to_chunk_info or not all_chunks_data:
        st.warning("FAISS ch∆∞a s·∫µn s√†ng ho·∫∑c thi·∫øu d·ªØ li·ªáu.")
        return []

    print(f"FAISS: ƒêang t√¨m ki·∫øm v·ªõi k={k}...")
    distances, indices = index.search(
        np.array([query_embedding]).astype('float32'), k)
    results = []
    if indices.size > 0:
        for i in range(len(indices[0])):
            faiss_id = indices[0][i]
            if faiss_id < 0:  # Invalid index from FAISS
                continue

            mapped_info = id_to_chunk_info.get(faiss_id)
            if mapped_info:
                original_chunk_index = mapped_info.get('original_chunk_index')
                if original_chunk_index is not None and 0 <= original_chunk_index < len(all_chunks_data):
                    chunk_data = all_chunks_data[original_chunk_index]
                    # S·ª≠ d·ª•ng ID g·ªëc t·ª´ all_chunks_data n·∫øu c√≥, n·∫øu kh√¥ng th√¨ t·∫°o ID FAISS
                    result_id = chunk_data.get(
                        'id', f"faiss_idx_{faiss_id}_orig_idx_{original_chunk_index}")
                    results.append({
                        'id': result_id,
                        'content': chunk_data.get('content', 'N/A'),
                        'score': float(distances[0][i]),
                        'metadata': chunk_data.get('metadata', {}),
                        # ƒê·∫£m b·∫£o type, syllabus_id, course_id ƒë∆∞·ª£c l·∫•y t·ª´ chunk_data.metadata n·∫øu c√≥
                        'type': chunk_data.get('metadata', {}).get('type', chunk_data.get('type', 'N/A')),
                        'syllabus_id': chunk_data.get('metadata', {}).get('syllabus_id', mapped_info.get('syllabus_id', 'N/A')),
                        'course_id': chunk_data.get('metadata', {}).get('course_id', mapped_info.get('course_id', 'N/A'))
                    })
                else:
                    print(
                        f"FAISS: original_chunk_index kh√¥ng h·ª£p l·ªá ({original_chunk_index}) cho FAISS ID {faiss_id}.")
            else:
                print(
                    f"FAISS: Kh√¥ng t√¨m th·∫•y mapping cho FAISS ID {faiss_id}.")
    print(f"FAISS: T√¨m th·∫•y {len(results)} k·∫øt qu·∫£.")
    return results


def search_chroma(collection, query_embedding, k=5):
    if not collection:
        st.warning("ChromaDB collection ch∆∞a s·∫µn s√†ng.")
        return []

    print(f"ChromaDB: ƒêang t√¨m ki·∫øm v·ªõi k={k}...")
    try:
        query_results = collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=k,
            include=['documents', 'metadatas', 'distances']
        )
    except Exception as e:
        st.error(f"L·ªói khi query ChromaDB: {e}")
        return []

    results = []
    if query_results and query_results['ids'] and query_results['ids'][0]:
        for i in range(len(query_results['ids'][0])):
            results.append({
                'id': query_results['ids'][0][i],
                'content': query_results['documents'][0][i] if query_results['documents'] else 'N/A',
                'score': float(query_results['distances'][0][i]) if query_results['distances'] else float('inf'),
                'metadata': query_results['metadatas'][0][i] if query_results['metadatas'] else {}
            })
    print(f"ChromaDB: T√¨m th·∫•y {len(results)} k·∫øt qu·∫£.")
    return results


def get_answer_from_gemini(question, context_chunks, gemini_model_instance, temperature=0.2, queried_student_name=None):
    if not GEMINI_API_KEY:
        st.error("GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
        return "L·ªói: API key c·ªßa Gemini ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh."
    if not gemini_model_instance:
        st.error("M√¥ h√¨nh Gemini (LLM) ch∆∞a ƒë∆∞·ª£c t·∫£i.")
        return "L·ªói: M√¥ h√¨nh Gemini (LLM) ch∆∞a ƒë∆∞·ª£c t·∫£i."

    context_str = "Kh√¥ng c√≥ th√¥ng tin tham kh·∫£o n√†o ƒë∆∞·ª£c t√¨m th·∫•y."  # Default if no chunks
    if context_chunks:
        context_str = "\n\n---\n\n".join(
            [f"Ngu·ªìn {i+1} (ID: {chunk.get('id', 'N/A')}, Lo·∫°i: {chunk.get('type', chunk.get('metadata', {}).get('type', 'N/A'))}, Course: {chunk.get('course_id', chunk.get('metadata', {}).get('course_id', 'N/A'))}, Score: {chunk.get('score', -1):.4f})\nN·ªôi dung: {chunk['content']}"
             for i, chunk in enumerate(context_chunks)]
        )

    specific_student_instruction = ""
    if queried_student_name and any(chunk.get('type') == 'student_list_by_major' for chunk in context_chunks):
        specific_student_instruction = f"""
ƒê·∫∑c bi·ªát quan tr·ªçng: C√¢u h·ªèi n√†y li√™n quan ƒë·∫øn sinh vi√™n c·ª• th·ªÉ '{queried_student_name}'. 
D·ª±a v√†o 'Th√¥ng tin tham kh·∫£o', h√£y ƒë·∫£m b·∫£o b·∫°n tr√≠ch xu·∫•t v√† li·ªát k√™ ƒë·∫ßy ƒë·ªß c√°c th√¥ng tin sau c·ªßa sinh vi√™n n√†y n·∫øu c√≥:
- M√£ s·ªë sinh vi√™n (MSSV)
- H·ªç v√† t√™n ƒë·∫ßy ƒë·ªß
- ƒê·ªãa ch·ªâ email
N·∫øu c√°c th√¥ng tin n√†y c√≥ trong m·ªôt d√≤ng ho·∫∑c m·ª•c danh s√°ch, h√£y tr√¨nh b√†y r√µ r√†ng t·ª´ng th√¥ng tin."""

    prompt = f"""D·ª±a v√†o c√°c th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y t·ª´ t√†i li·ªáu syllabus v√† danh s√°ch sinh vi√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√¢y m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c.

C√¢u h·ªèi: {question}

Th√¥ng tin tham kh·∫£o:
{context_str}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
- Ch·ªâ d·ª±a v√†o "Th√¥ng tin tham kh·∫£o" ƒë∆∞·ª£c cung c·∫•p. Kh√¥ng s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu r√µ r√†ng.
- N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i r√µ l√† "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu tham kh·∫£o."
- N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn nhi·ªÅu th·ª±c th·ªÉ (v√≠ d·ª•: nhi·ªÅu m√¥n h·ªçc), h√£y c·ªë g·∫Øng t√¨m v√† gi·∫£i th√≠ch m·ªëi quan h·ªá gi·ªØa ch√∫ng (v√≠ d·ª•: m√¥n ti√™n quy·∫øt, n·ªôi dung li√™n quan, so s√°nh) d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. N·∫øu kh√¥ng c√≥ m·ªëi quan h·ªá n√†o ƒë∆∞·ª£c t√¨m th·∫•y trong t√†i li·ªáu, h√£y n√™u r√µ ƒëi·ªÅu ƒë√≥.
- Khi li·ªát k√™ Learning Outcomes (CLO), h√£y ƒë·∫£m b·∫£o li·ªát k√™ ƒë·∫ßy ƒë·ªß n·∫øu c√≥ th√¥ng tin.
- ∆Øu ti√™n tr√≠ch xu·∫•t th√¥ng tin tr·ª±c ti·∫øp t·ª´ n·ªôi dung chunk n·∫øu c√≥ th·ªÉ v√† ch·ªâ r√µ ngu·ªìn (v√≠ d·ª•: "Theo Ngu·ªìn X...").
{specific_student_instruction}

C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n:"""

    try:
        print("App: G·ª≠i prompt t·ªõi Gemini...")
        response = gemini_model_instance.generate_content(prompt)
        print("App: Nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini.")
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi Gemini API: {e}")
        return f"ƒê√£ x·∫£y ra l·ªói khi c·ªë g·∫Øng t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ Gemini: {str(e)}"

# --- Streamlit UI ---


def main():
    st.set_page_config(
        layout="wide", page_title="H·ªèi ƒê√°p Syllabus FPTU (Gemini)")
    st.title("üìö H·ªá Th·ªëng H·ªèi ƒê√°p Th√¥ng Tin Syllabus FPTU")
    st.markdown(
        "S·ª≠ d·ª•ng AI (Gemini) v√† Vector Search (FAISS/ChromaDB) ƒë·ªÉ t√¨m ki·∫øm v√† gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ ch∆∞∆°ng tr√¨nh h·ªçc.")

    loaded_resources = load_all_resources()

    embedding_model = loaded_resources["embedding_model"]
    faiss_index = loaded_resources["faiss_index"]
    faiss_mapping = loaded_resources["faiss_mapping"]
    all_chunks_data = loaded_resources["all_chunks_data"]
    chroma_collection = loaded_resources["chroma_collection"]
    # L·∫•y m√¥ h√¨nh Gemini LLM
    gemini_llm_model = loaded_resources["gemini_llm_model"]

    if not embedding_model or not all_chunks_data or not gemini_llm_model:
        st.error(
            "Kh√¥ng th·ªÉ t·∫£i m·ªôt ho·∫∑c nhi·ªÅu t√†i nguy√™n c·∫ßn thi·∫øt (Embedding model, Chunks data, Gemini LLM). Vui l√≤ng ki·ªÉm tra l·∫°i.")
        return

    st.sidebar.title("‚öôÔ∏è C·∫•u H√¨nh T√¨m Ki·∫øm")
    db_option = st.sidebar.radio(
        "Ch·ªçn Vector Database:",
        ('FAISS', 'ChromaDB'),
        help="FAISS: Nhanh, t√¨m ki·∫øm tr√™n vector. ChromaDB: Persistent, c√≥ th·ªÉ query metadata (ch∆∞a implement trong UI n√†y)."
    )

    k_results = st.sidebar.slider(
        "S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t√¨m ki·∫øm (chunks) m·ªói truy v·∫•n:", 1, 1000, 100)
    gemini_temp = st.sidebar.slider(
        "Nhi·ªát ƒë·ªô Gemini (s√°ng t·∫°o):", 0.0, 1.0, 0.2, 0.05)

    st.sidebar.markdown("---")

    # Input c√¢u h·ªèi
    user_question = st.text_area(
        "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y:",
        height=100,
        placeholder="V√≠ d·ª•: Session 1 c·ªßa m√¥n XYZ c√≥ chu·∫©n ƒë·∫ßu ra n√†o? M√¥n ABC l√† ti√™n quy·∫øt c·ªßa m√¥n n√†o? Danh s√°ch sinh vi√™n chuy√™n ng√†nh SE? Th√¥ng tin c·ªßa Nguy·ªÖn VƒÉn A?"
    )

    submit_button = st.button("üîç G·ª≠i C√¢u H·ªèi")

    if submit_button and user_question:
        st.markdown("---")
        st.subheader(f'üí¨ C√¢u h·ªèi c·ªßa b·∫°n: "{user_question}"')

        original_user_question = user_question  # Gi·ªØ l·∫°i c√¢u h·ªèi g·ªëc
        query_for_search_embedding = translate_to_english_if_vietnamese(
            user_question, gemini_llm_model, original_user_question_for_log=user_question)

        retrieved_chunks = []
        search_method_message = ""
        extracted_name_for_gemini_prompt = None  # Kh·ªüi t·∫°o ƒë·ªÉ truy·ªÅn cho Gemini
        # search_logic_flow_debug = [] # Optional: for detailed debug path

        # --- ƒê·ªãnh nghƒ©a c√°c pattern Regex ---
        # Pattern cho t√™n sinh vi√™n (2-4 t·ª´ ti·∫øng Vi·ªát c√≥ vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu)
        specific_name_pattern = re.compile(
            r"\b([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+(?:\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+){1,3})\b")
        # Pattern cho t·ª´ kh√≥a danh s√°ch sinh vi√™n theo ng√†nh
        student_list_major_pattern = re.compile(
            r"(danh s√°ch sinh vi√™n|sinh vi√™n ng√†nh|list of students|students in major|student list for)\s*([A-Z]{2,4})\b", re.IGNORECASE)
        # Pattern cho m√£ m√¥n h·ªçc
        course_code_pattern = re.compile(
            r"\b([A-Z]{3,4}[0-9]{3}[a-z]?)\b", re.IGNORECASE)
        # T·ª´ kh√≥a th∆∞·ªùng ƒëi v·ªõi c√¢u h·ªèi v·ªÅ m√¥n h·ªçc (ƒë·ªÉ ph√¢n bi·ªát v·ªõi t√™n ng∆∞·ªùi)
        course_related_keywords = re.compile(
            r"(m√¥n h·ªçc|h·ªçc ph·∫ßn|ti√™n quy·∫øt|syllabus|course code|m√£ m√¥n|CLO|h·ªçc g√¨|n·ªôi dung|ƒë·ªÅ c∆∞∆°ng|th√¥ng tin m√¥n)", re.IGNORECASE)

        # --- Logic t√¨m ki·∫øm ∆∞u ti√™n ---
        with st.spinner(f"ƒêang t√¨m ki·∫øm th√¥ng tin trong {db_option}..."):
            # 1. ∆Øu ti√™n t√¨m ki·∫øm th√¥ng tin sinh vi√™n c·ª• th·ªÉ b·∫±ng t√™n (scan tr·ª±c ti·∫øp)
            potential_name_match_in_query = specific_name_pattern.search(
                original_user_question)
            if potential_name_match_in_query and not student_list_major_pattern.search(original_user_question) and not course_related_keywords.search(original_user_question) and not course_code_pattern.search(original_user_question):
                extracted_name = potential_name_match_in_query.group(1).strip()
                print(
                    f"App: Ph√°t hi·ªán truy v·∫•n c√≥ th·ªÉ l√† t√™n SV c·ª• th·ªÉ: '{extracted_name}'. Th·ª≠ t√¨m tr·ª±c ti·∫øp.")
                # search_logic_flow_debug.append(f"Trying direct student name search for: {extracted_name}")
                if all_chunks_data:
                    for idx, chunk_item in enumerate(all_chunks_data):
                        if chunk_item.get('type') == 'student_list_by_major' and \
                           extracted_name.lower() in chunk_item.get('content', '').lower():
                            retrieved_chunks = [{
                                'id': f"direct_student_name_idx_{idx}",
                                'content': chunk_item.get('content', 'N/A'),
                                'score': 0.0,
                                'metadata': chunk_item.get('metadata', {}),
                                'type': chunk_item.get('type', 'N/A'),
                                'course_id': chunk_item.get('metadata', {}).get('course_id'),
                                'syllabus_id': chunk_item.get('metadata', {}).get('syllabus_id')
                            }]
                            major_name_for_msg = retrieved_chunks[0]['metadata'].get(
                                'major_name', 'N/A')
                            search_method_message = f"T√¨m th·∫•y th√¥ng tin tr·ª±c ti·∫øp cho sinh vi√™n '{extracted_name}' trong danh s√°ch c·ªßa chuy√™n ng√†nh {major_name_for_msg}."
                            # L∆∞u t√™n ƒë·ªÉ d√πng cho prompt Gemini
                            extracted_name_for_gemini_prompt = extracted_name
                            # search_logic_flow_debug.append(f"SUCCESS: Found '{extracted_name}' in student list chunk.")
                            print(search_method_message)
                            break
                    if not retrieved_chunks:
                        # search_logic_flow_debug.append(f"FAILED: Direct student name search for '{extracted_name}' found no chunk.")
                        print(
                            f"App: Kh√¥ng t√¨m th·∫•y '{extracted_name}' tr·ª±c ti·∫øp trong chunk danh s√°ch sinh vi√™n n√†o.")

            # 2. N·∫øu kh√¥ng ph·∫£i truy v·∫•n SV c·ª• th·ªÉ (ho·∫∑c t√¨m kh√¥ng th·∫•y), ki·ªÉm tra truy v·∫•n danh s√°ch SV theo ng√†nh
            if not retrieved_chunks:
                student_major_match_result = student_list_major_pattern.search(
                    original_user_question)
                if student_major_match_result:
                    # C·∫ßn flag n√†y ƒë·ªÉ kh√¥ng b·ªã nh·∫ßm v·ªõi multi-course/general
                    is_student_list_query_flag = True
                    target_major_code = student_major_match_result.group(
                        2).upper()
                    print(
                        f"App: Ph√°t hi·ªán c√¢u h·ªèi danh s√°ch sinh vi√™n cho ng√†nh: {target_major_code}. ∆Øu ti√™n t√¨m tr·ª±c ti·∫øp.")
                    # search_logic_flow_debug.append(f"Trying student list by major (direct scan first): {target_major_code}")

                    # Th·ª≠ t√¨m tr·ª±c ti·∫øp tr∆∞·ªõc
                    if all_chunks_data:
                        for idx, chunk_item in enumerate(all_chunks_data):
                            meta = chunk_item.get('metadata', {})
                            if chunk_item.get('type') == 'student_list_by_major' and meta.get('major_code') == target_major_code:
                                retrieved_chunks = [{
                                    'id': f"direct_major_list_idx_{idx}",
                                    'content': chunk_item.get('content', 'N/A'),
                                    'score': 0.0,
                                    'metadata': meta,
                                    'type': chunk_item.get('type', 'N/A'),
                                    'course_id': meta.get('course_id'),
                                    'syllabus_id': meta.get('syllabus_id')
                                }]
                                search_method_message = f"ƒê√£ t√¨m th·∫•y tr·ª±c ti·∫øp chunk danh s√°ch sinh vi√™n cho ng√†nh {target_major_code}."
                                # search_logic_flow_debug.append("SUCCESS: Found student list by major directly.")
                                print(search_method_message)
                                break

                    if not retrieved_chunks:  # N·∫øu t√¨m tr·ª±c ti·∫øp kh√¥ng th·∫•y, d√πng vector search l√†m fallback
                        print(
                            f"App: Kh√¥ng t√¨m th·∫•y DS SV cho ng√†nh {target_major_code} tr·ª±c ti·∫øp. D√πng vector search...")
                        # search_logic_flow_debug.append(f"FALLBACK: Vector search for student list by major: {target_major_code}")
                        query_embedding_for_list = embedding_model.encode(
                            query_for_search_embedding)  # Encode c√¢u h·ªèi ƒë√£ d·ªãch
                        candidate_chunks_vector = []
                        # ... (logic vector search l·∫•y k_results*5 chunks nh∆∞ ƒë√£ code ·ªü b∆∞·ªõc tr∆∞·ªõc)
                        # L·∫•y nhi·ªÅu ·ª©ng vi√™n h∆°n ƒë·ªÉ l·ªçc sau
                        num_candidates_to_fetch = k_results * 5
                        if db_option == 'FAISS':
                            if faiss_index and faiss_mapping and all_chunks_data:
                                if num_candidates_to_fetch > faiss_index.ntotal:
                                    num_candidates_to_fetch = faiss_index.ntotal
                                distances, indices = faiss_index.search(np.array(
                                    [query_embedding_for_list]).astype('float32'), num_candidates_to_fetch)
                                # ... (x·ª≠ l√Ω k·∫øt qu·∫£ faiss ƒë·ªÉ ƒëi·ªÅn v√†o candidate_chunks_vector) ...
                                if indices.size > 0:
                                    for i in range(len(indices[0])):
                                        faiss_id = indices[0][i]
                                        mapped_info = faiss_mapping.get(
                                            faiss_id)
                                        if faiss_id < 0 or not mapped_info:
                                            continue
                                        original_chunk_index = mapped_info.get(
                                            'original_chunk_index')
                                        if original_chunk_index is not None and 0 <= original_chunk_index < len(all_chunks_data):
                                            chunk_data = all_chunks_data[original_chunk_index]
                                            candidate_chunks_vector.append({'id': f"faiss_id_{faiss_id}_orig_idx_{original_chunk_index}", 'content': chunk_data.get('content', 'N/A'), 'score': float(distances[0][i]), 'metadata': chunk_data.get(
                                                'metadata', {}), 'type': chunk_data.get('metadata', {}).get('type', chunk_data.get('type', 'N/A')), 'course_id': chunk_data.get('metadata', {}).get('course_id', mapped_info.get('course_id', 'N/A'))})
                        elif db_option == 'ChromaDB':
                            if chroma_collection:
                                if num_candidates_to_fetch > chroma_collection.count():
                                    num_candidates_to_fetch = chroma_collection.count()
                                query_res_chroma = chroma_collection.query(query_embeddings=[query_embedding_for_list.tolist(
                                )], n_results=num_candidates_to_fetch, include=['documents', 'metadatas', 'distances'])
                                # ... (x·ª≠ l√Ω k·∫øt qu·∫£ chroma ƒë·ªÉ ƒëi·ªÅn v√†o candidate_chunks_vector) ...
                                if query_res_chroma and query_res_chroma['ids'] and query_res_chroma['ids'][0]:
                                    for i in range(len(query_res_chroma['ids'][0])):
                                        candidate_chunks_vector.append({'id': query_res_chroma['ids'][0][i], 'content': query_res_chroma['documents'][0][i] if query_res_chroma['documents'] else 'N/A', 'score': float(
                                            query_res_chroma['distances'][0][i]) if query_res_chroma['distances'] else float('inf'), 'metadata': query_res_chroma['metadatas'][0][i] if query_res_chroma['metadatas'] else {}})

                        # L·ªçc l·∫°i t·ª´ candidate_chunks_vector
                        student_list_chunks_from_vector = []
                        other_vector_chunks = []
                        for chunk in candidate_chunks_vector:
                            meta_vec = chunk.get('metadata', {})
                            if chunk.get('type') == 'student_list_by_major' and meta_vec.get('major_code') == target_major_code:
                                student_list_chunks_from_vector.append(chunk)
                            else:
                                other_vector_chunks.append(chunk)
                        student_list_chunks_from_vector.sort(
                            key=lambda x: x.get('score', float('inf')))
                        other_vector_chunks.sort(
                            key=lambda x: x.get('score', float('inf')))
                        retrieved_chunks = (
                            student_list_chunks_from_vector + other_vector_chunks)[:k_results]
                        search_method_message = f"T√¨m DS SV cho ng√†nh {target_major_code} b·∫±ng vector search. {len(student_list_chunks_from_vector)} chunk DS SV kh·ªõp. T·ªïng {len(retrieved_chunks)} chunks."
                        if not student_list_chunks_from_vector:
                            search_method_message += " Kh√¥ng t√¨m th·∫•y chunk DS SV kh·ªõp tr·ª±c ti·∫øp t·ª´ vector search."
                        # search_logic_flow_debug.append(f"Vector search for student list yielded {len(student_list_chunks_from_vector)} specific chunks.")
                else:
                    is_student_list_query_flag = False  # Reset flag if not this type of query

            # 3. N·∫øu kh√¥ng ph·∫£i c√°c lo·∫°i tr√™n, ki·ªÉm tra truy v·∫•n ƒëa m√¥n ho·∫∑c ƒë∆°n m√¥n (d√πng vector search)
            if not retrieved_chunks:
                query_embedding_for_courses = embedding_model.encode(
                    query_for_search_embedding)  # ƒê·∫£m b·∫£o ƒë√£ encode c√¢u h·ªèi ƒë√£ d·ªãch

                course_codes_found_in_query = list(
                    # T√¨m m√£ m√¥n tr√™n c√¢u h·ªèi ƒë√£ d·ªãch
                    set([match.upper() for match in course_code_pattern.findall(query_for_search_embedding)]))

                # search_logic_flow_debug.append(f"Course codes found in translated query: {course_codes_found_in_query}")

                # is_multi_course_query_flag ch·ªâ True n·∫øu >1 m√£ m√¥n V√Ä KH√îNG PH·∫¢I l√† truy v·∫•n DS Sinh vi√™n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
                is_multi_course_query_flag = len(
                    course_codes_found_in_query) > 1 and not is_student_list_query_flag

                if is_multi_course_query_flag:
                    # search_logic_flow_debug.append("Handling as multi-course query.")
                    search_method_message = f"T√¨m ki·∫øm th√¥ng tin cho c√°c m√¥n: {', '.join(course_codes_found_in_query)}.\n"
                    sub_k_fetch = max(k_results * 2, 15)

                    all_sub_query_chunks_collected = []
                    # ... (To√†n b·ªô logic c·ªßa is_multi_course_query nh∆∞ c≈©, nh∆∞ng d√πng course_codes_found_in_query v√† query_embedding_for_courses n·∫øu c·∫ßn, ho·∫∑c sub_query_text_en cho sub-embeddings)
                    # ƒê·∫£m b·∫£o r·∫±ng sub_query_text_en ƒë∆∞·ª£c encode v√† t√¨m ki·∫øm
                    # V√† k·∫øt qu·∫£ cu·ªëi c√πng ƒë∆∞·ª£c g√°n cho retrieved_chunks
                    # ... (Copy v√† ƒëi·ªÅu ch·ªânh logic is_multi_course_query t·ª´ phi√™n b·∫£n tr∆∞·ªõc v√†o ƒë√¢y) ...
                    # V√≠ d·ª•:
                    processed_sub_ids = set()
                    for course_code_item in course_codes_found_in_query:
                        sub_query_text_en = f"Detailed information about the course {course_code_item}, including its overview, learning objectives, CLOs, reference materials, and most importantly, its prerequisites or any directly related courses to {course_code_item}."
                        sub_query_emb = embedding_model.encode(
                            sub_query_text_en)
                        current_sub_res = []
                        if db_option == 'FAISS':
                            if faiss_index and faiss_mapping and all_chunks_data:
                                current_sub_res = search_faiss(
                                    faiss_index, faiss_mapping, all_chunks_data, sub_query_emb, k=sub_k_fetch)
                        elif db_option == 'ChromaDB':
                            if chroma_collection:
                                current_sub_res = search_chroma(
                                    chroma_collection, sub_query_emb, k=sub_k_fetch)
                        all_sub_query_chunks_collected.extend(current_sub_res)

                    unique_chunks_by_id_dict = {
                        chunk['id']: chunk for chunk in all_sub_query_chunks_collected}
                    prioritized_for_gemini = []
                    temp_processed_ids_multi = set()

                    for code_mc in course_codes_found_in_query:
                        ov_chunk = next((c for c_id, c in unique_chunks_by_id_dict.items() if c.get('metadata', {}).get(
                            'course_id') == code_mc and c.get('type') == 'overview' and c_id not in temp_processed_ids_multi), None)
                        if ov_chunk:
                            prioritized_for_gemini.append(ov_chunk)
                            temp_processed_ids_multi.add(ov_chunk['id'])
                        pr_chunk = next((c for c_id, c in unique_chunks_by_id_dict.items() if c.get('metadata', {}).get(
                            'course_id') == code_mc and c.get('type') == 'prerequisites' and c_id not in temp_processed_ids_multi), None)
                        if pr_chunk:
                            prioritized_for_gemini.append(pr_chunk)
                            temp_processed_ids_multi.add(pr_chunk['id'])

                    remaining_sorted = sorted([c for c_id, c in unique_chunks_by_id_dict.items(
                    ) if c_id not in temp_processed_ids_multi], key=lambda x: x.get('score', float('inf')))
                    for chunk_rem in remaining_sorted:
                        if len(prioritized_for_gemini) >= k_results:
                            break
                        if chunk_rem['id'] not in temp_processed_ids_multi:
                            prioritized_for_gemini.append(chunk_rem)
                            temp_processed_ids_multi.add(chunk_rem['id'])

                    retrieved_chunks = prioritized_for_gemini[:k_results]
                    retrieved_chunks.sort(
                        key=lambda x: x.get('score', float('inf')))
                    search_method_message += f"ƒê√£ x·ª≠ l√Ω {len(course_codes_found_in_query)} sub-queries. {len(retrieved_chunks)} chunks ƒë∆∞·ª£c ch·ªçn (gi·ªõi h·∫°n k={k_results})."

                elif not is_student_list_query_flag:  # Truy v·∫•n ƒë∆°n l·∫ª, kh√¥ng ph·∫£i SV, kh√¥ng ph·∫£i ƒëa m√¥n
                    # search_logic_flow_debug.append("Handling as general single vector search.")
                    print(
                        f"App: Th·ª±c hi·ªán general vector search cho: {query_for_search_embedding}")

                    # Ki·ªÉm tra xem c√≥ ph·∫£i l√† truy v·∫•n cho m·ªôt m√¥n h·ªçc duy nh·∫•t kh√¥ng
                    if len(course_codes_found_in_query) == 1 and not is_student_list_query_flag and not is_multi_course_query_flag:
                        single_course_code = course_codes_found_in_query[0]
                        # search_logic_flow_debug.append(f"Identified as single course query for: {single_course_code}")
                        search_method_message = f"T√¨m ki·∫øm th√¥ng tin chi ti·∫øt cho m√¥n h·ªçc: {single_course_code}.\n"
                        print(
                            f"App: ∆Øu ti√™n l·∫•y tr·ª±c ti·∫øp chunks overview, prerequisites, learning_outcomes cho {single_course_code}")

                        directly_fetched_single_course_chunks = []
                        processed_ids_single_course = set()

                        if all_chunks_data:
                            # 1. L·∫•y overview
                            overview_chunk = next((c for c in all_chunks_data if c.get('metadata', {}).get(
                                'course_id') == single_course_code and c.get('type') == 'overview'), None)
                            if overview_chunk and overview_chunk.get('id') not in processed_ids_single_course:
                                directly_fetched_single_course_chunks.append(
                                    # ∆Øu ti√™n cao
                                    {**overview_chunk, 'score': 0.01})
                                processed_ids_single_course.add(
                                    overview_chunk.get('id'))

                            # 2. L·∫•y prerequisites
                            prereq_chunk = next((c for c in all_chunks_data if c.get('metadata', {}).get(
                                'course_id') == single_course_code and c.get('type') == 'prerequisites'), None)
                            if prereq_chunk and prereq_chunk.get('id') not in processed_ids_single_course:
                                directly_fetched_single_course_chunks.append(
                                    # ∆Øu ti√™n cao
                                    {**prereq_chunk, 'score': 0.02})
                                processed_ids_single_course.add(
                                    prereq_chunk.get('id'))

                            # 3. L·∫•y t·∫•t c·∫£ learning_outcomes
                            for clo_chunk in all_chunks_data:
                                if clo_chunk.get('metadata', {}).get('course_id') == single_course_code and clo_chunk.get('type') == 'learning_outcome':
                                    if clo_chunk.get('id') not in processed_ids_single_course:
                                        directly_fetched_single_course_chunks.append(
                                            # ∆Øu ti√™n cao
                                            {**clo_chunk, 'score': 0.03})
                                        processed_ids_single_course.add(
                                            clo_chunk.get('id'))

                        search_method_message += f"ƒê√£ l·∫•y tr·ª±c ti·∫øp {len(directly_fetched_single_course_chunks)} chunks (overview, prereqs, CLOs). "

                        # 4. B·ªï sung b·∫±ng vector search n·∫øu c·∫ßn th√™m chunks
                        remaining_k = k_results - \
                            len(directly_fetched_single_course_chunks)
                        if remaining_k > 0:
                            search_method_message += f"T√¨m th√™m {remaining_k} chunks b·∫±ng vector search. "
                            vector_search_results_single_course = []
                            if db_option == 'FAISS':
                                if faiss_index and faiss_mapping and all_chunks_data:
                                    vector_search_results_single_course = search_faiss(
                                        # L·∫•y k_results ·ª©ng vi√™n
                                        faiss_index, faiss_mapping, all_chunks_data, query_embedding_for_courses, k=k_results)
                            elif db_option == 'ChromaDB':
                                if chroma_collection:
                                    vector_search_results_single_course = search_chroma(
                                        chroma_collection, query_embedding_for_courses, k=k_results)  # L·∫•y k_results ·ª©ng vi√™n

                            # Th√™m c√°c chunks t·ª´ vector search m√† ch∆∞a c√≥, cho ƒë·∫øn khi ƒë·ªß k_results
                            for v_chunk in vector_search_results_single_course:
                                if len(directly_fetched_single_course_chunks) >= k_results:
                                    break
                                if v_chunk.get('id') not in processed_ids_single_course:
                                    directly_fetched_single_course_chunks.append(
                                        v_chunk)
                                    processed_ids_single_course.add(
                                        v_chunk.get('id'))

                        retrieved_chunks = directly_fetched_single_course_chunks[:k_results]
                        # S·∫Øp x·∫øp l·∫°i l·∫ßn cu·ªëi, ∆∞u ti√™n c√°c chunk l·∫•y tr·ª±c ti·∫øp, sau ƒë√≥ theo score t·ª´ vector search
                        retrieved_chunks.sort(
                            key=lambda x: x.get('score', float('inf')))
                        search_method_message += f"T·ªïng c·ªông {len(retrieved_chunks)} chunks ƒë∆∞·ª£c ch·ªçn cho Gemini."

                    # General vector search (kh√¥ng ph·∫£i specific single course, kh√¥ng ph·∫£i student list, kh√¥ng ph·∫£i multi-course)
                    else:
                        # search_logic_flow_debug.append("Fallback to general vector search as no specific logic matched.")
                        print(
                            f"App: Th·ª±c hi·ªán general vector search (fallback) cho: {query_for_search_embedding}")
                        if db_option == 'FAISS':
                            if faiss_index and faiss_mapping and all_chunks_data:
                                retrieved_chunks = search_faiss(
                                    faiss_index, faiss_mapping, all_chunks_data, query_embedding_for_courses, k=k_results)
                                search_method_message = f"ƒê√£ t√¨m th·∫•y {len(retrieved_chunks)} chunks li√™n quan b·∫±ng FAISS (general search)."
                            else:
                                st.error(
                                    "FAISS ch∆∞a ƒë∆∞·ª£c t·∫£i ho·∫∑c c·∫•u h√¨nh ƒë√∫ng.")
                        elif db_option == 'ChromaDB':
                            if chroma_collection:
                                retrieved_chunks = search_chroma(
                                    chroma_collection, query_embedding_for_courses, k=k_results)
                                search_method_message = f"ƒê√£ t√¨m th·∫•y {len(retrieved_chunks)} chunks li√™n quan b·∫±ng ChromaDB (general search)."
                            else:
                                st.error(
                                    "ChromaDB collection ch∆∞a ƒë∆∞·ª£c t·∫£i ƒë√∫ng.")

            # print(f"App: Search Logic Flow: {' -> '.join(search_logic_flow_debug)}") # Optional debug
            # N·∫øu kh√¥ng c√≥ th√¥ng b√°o n√†o ƒë∆∞·ª£c ƒë·∫∑t v√† kh√¥ng c√≥ chunk
            if not search_method_message and not retrieved_chunks:
                search_method_message = "Kh√¥ng c√≥ logic t√¨m ki·∫øm n√†o ƒë∆∞·ª£c k√≠ch ho·∫°t ho·∫∑c kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£."
            # N·∫øu c√≥ chunk nh∆∞ng kh√¥ng c√≥ msg (tr∆∞·ªùng h·ª£p hi·∫øm)
            elif not search_method_message and retrieved_chunks:
                search_method_message = f"ƒê√£ t√¨m th·∫•y {len(retrieved_chunks)} chunks."

        st.info(search_method_message)

        if retrieved_chunks:
            with st.expander("Xem c√°c chunks th√¥ng tin ƒë∆∞·ª£c t√¨m th·∫•y", expanded=False):
                for i, chunk in enumerate(retrieved_chunks):
                    st.markdown(
                        f"**Chunk {i+1} (Score: {chunk.get('score', 'N/A'):.4f})** - ID: `{chunk.get('id', 'N/A')}`")

                    # L·∫•y th√¥ng tin type v√† course_id m·ªôt c√°ch nh·∫•t qu√°n t·ª´ metadata n·∫øu c√≥
                    chunk_type = chunk.get('type', 'N/A')
                    chunk_course_id = 'N/A'
                    if 'metadata' in chunk and isinstance(chunk['metadata'], dict):
                        # ∆Øu ti√™n type t·ª´ metadata n·∫øu c√≥, n·∫øu kh√¥ng th√¨ d√πng type ·ªü ngo√†i
                        chunk_type = chunk['metadata'].get(
                            'type', chunk_type)
                        chunk_course_id = chunk['metadata'].get(
                            'course_id', 'N/A')

                    st.markdown(
                        f"*Lo·∫°i:* `{chunk_type}` - *Course ID:* `{chunk_course_id}`")
                    st.text_area(
                        f"N·ªôi dung chunk {i+1}", chunk['content'], height=150, key=f"chunk_exp_{i}")
                    st.markdown("---")

        st.markdown("---")
        st.subheader("ü§ñ C√¢u Tr·∫£ L·ªùi t·ª´ Gemini:")
        with st.spinner("Gemini ƒëang x·ª≠ l√Ω v√† t·∫°o c√¢u tr·∫£ l·ªùi..."):
            start_time = time.time()
            gemini_answer = get_answer_from_gemini(
                # Truy·ªÅn gemini_llm_model
                user_question,
                retrieved_chunks,
                gemini_llm_model,
                temperature=gemini_temp,
                queried_student_name=extracted_name_for_gemini_prompt  # Truy·ªÅn t√™n SV ƒë√£ tr√≠ch xu·∫•t
            )
            processing_time = time.time() - start_time
            st.markdown(gemini_answer)
            st.caption(
                f"Th·ªùi gian Gemini x·ª≠ l√Ω: {processing_time:.2f} gi√¢y")

    elif submit_button and not user_question:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n.")

    st.sidebar.markdown("---")
    st.sidebar.info(
        f"ƒêang s·ª≠ d·ª•ng file chunks: {ALL_CHUNKS_DATA_FILE}. Model: {EMBEDDING_MODEL_NAME}. Gemini: {GEMINI_MODEL_NAME}.")


if __name__ == "__main__":
    main()
